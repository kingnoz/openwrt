diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/autoeee/autoeee.c linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/autoeee/autoeee.c
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/autoeee/autoeee.c	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/autoeee/autoeee.c	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,125 @@
+#include <linux/phy.h>
+#include <linux/micrel_phy.h>
+#include "../higmac.h"
+#include "autoeee.h"
+
+void init_autoeee(struct higmac_netdev_local *ld)
+{
+	int phy_id = ld->phy->phy_id;
+	struct phy_info *phy_info;
+
+	if (ld->eee_init)
+		goto eee_init;
+
+	phy_info = phy_search_ids(phy_id);
+	if (phy_info) {
+		int eee_available, lp_eee_capable, v;
+		u32 link_stat = 0;
+
+		eee_available = phy_info->eee_available;
+		if (netif_msg_wol(ld))
+			pr_info("fit phy_id:0x%x, phy_name:%s, eee:%d\n",
+				phy_info->phy_id, phy_info->name,
+				eee_available);
+
+		if (!eee_available)
+			goto not_support;
+
+		if (eee_available == PHY_EEE) {
+			if (netif_msg_wol(ld))
+				pr_info("enter phy-EEE mode\n");
+
+			v = readl(ld->gmac_iobase + EEE_ENABLE);
+			v &= ~BIT_EEE_ENABLE;	/* disable auto-EEE */
+			writel(v, ld->gmac_iobase + EEE_ENABLE);
+			return;
+		}
+
+		ld->eee_init = phy_info->eee_init;
+eee_init:
+		switch (ld->phy->speed) {
+		case SPEED_10:
+			link_stat |= HIGMAC_SPD_10M;
+			break;
+		case SPEED_100:
+			link_stat |= HIGMAC_SPD_100M;
+			break;
+		case SPEED_1000:
+			link_stat |= HIGMAC_SPD_1000M;
+			break;
+		default:
+			break;
+		}
+
+		lp_eee_capable = ld->eee_init(ld->phy);
+		if (lp_eee_capable < 0)
+			return;
+
+		if (ld->phy->link) {
+			if (((u32)lp_eee_capable) & link_stat) {
+				if ((phy_id & REALTEK_PHY_MASK) ==
+				    REALTEK_PHY_ID_8211E) {
+					v = readl(ld->gmac_iobase + EEE_CLK);
+					v &= ~MASK_EEE_CLK;
+					v |= BIT_DISABLE_TX_CLK;
+					writel(v, ld->gmac_iobase + EEE_CLK);
+				} else if ((phy_id & MICREL_PHY_ID_MASK) ==
+					   PHY_ID_KSZ9031) {
+					v = readl(ld->gmac_iobase + EEE_CLK);
+					v &= ~MASK_EEE_CLK;
+					v |= (BIT_DISABLE_TX_CLK |
+						BIT_PHY_KSZ9031);
+					writel(v, ld->gmac_iobase + EEE_CLK);
+				}
+
+				/* EEE_1us: 0x7c for 125M */
+				writel(0x7c, ld->gmac_iobase +
+				       EEE_TIME_CLK_CNT);
+				writel(0x1e0400, ld->gmac_iobase +
+				       EEE_TIMER);/* FIXME */
+
+				v = readl(ld->gmac_iobase + EEE_LINK_STATUS);
+				v |= 0x3 << 1;	/* auto EEE and ... */
+				v |= BIT_PHY_LINK_STATUS;	/* phy linkup */
+				writel(v, ld->gmac_iobase + EEE_LINK_STATUS);
+
+				v = readl(ld->gmac_iobase + EEE_ENABLE);
+				v |= BIT_EEE_ENABLE;	/* enable EEE */
+				writel(v, ld->gmac_iobase + EEE_ENABLE);
+
+				if (netif_msg_wol(ld))
+					pr_info("enter auto-EEE mode\n");
+			} else {
+				if (netif_msg_wol(ld))
+					pr_info("link partner not support EEE\n");
+			}
+		} else {
+			v = readl(ld->gmac_iobase + EEE_LINK_STATUS);
+			v &= ~(BIT_PHY_LINK_STATUS);	/* phy linkdown */
+			writel(v, ld->gmac_iobase + EEE_LINK_STATUS);
+		}
+
+		return;
+	}
+
+not_support:
+	ld->eee_init = NULL;
+	if (netif_msg_wol(ld))
+		pr_info("non-EEE mode\n");
+}
+
+void eee_phy_linkdown(struct higmac_netdev_local *ld)
+{
+	int v = readl(ld->gmac_iobase + EEE_LINK_STATUS);
+	/* update phy link state */
+	v &= ~BIT_PHY_LINK_STATUS;
+	writel(v, ld->gmac_iobase + EEE_LINK_STATUS);
+}
+
+void eee_phy_linkup(struct higmac_netdev_local *ld)
+{
+	int v = readl(ld->gmac_iobase + EEE_LINK_STATUS);
+	/* update phy link state */
+	v |= BIT_PHY_LINK_STATUS;
+	writel(v, ld->gmac_iobase + EEE_LINK_STATUS);
+}
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/autoeee/autoeee.h linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/autoeee/autoeee.h
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/autoeee/autoeee.h	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/autoeee/autoeee.h	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,42 @@
+#ifndef	_AUTO_EEE_H
+
+#define NO_EEE          0
+#define MAC_EEE         1
+#define PHY_EEE         2
+#define PARTNER_EEE     2
+
+struct phy_info {
+	char *name;
+	int phy_id;
+	char eee_available;	/* eee support by this phy */
+	int (*eee_init)(struct phy_device *phy_dev);
+};
+
+/* GMAC register definition */
+#define EEE_CLK			0x800
+#define MASK_EEE_CLK		(0x3 << 20)
+#define BIT_DISABLE_TX_CLK	BIT(21)
+#define BIT_PHY_KSZ9031		BIT(20)
+#define EEE_ENABLE		0x808
+#define BIT_EEE_ENABLE		BIT(0)
+#define EEE_TIMER		0x80C
+#define EEE_LINK_STATUS		0x810
+#define BIT_PHY_LINK_STATUS	BIT(0)
+#define EEE_TIME_CLK_CNT	0x814
+
+/* ----------------------------phy register-------------------------------*/
+/* MMD: MDIO Manageable Device */
+#define MACR		0x0D
+#define MAADR		0x0E
+#define EEE_DEV		0x3
+#define EEE_CAPABILITY	0x14
+#define	EEELPAR_DEV	0x7
+#define EEELPAR		0x3D	/* EEE link partner ability register */
+#define EEE_ADVERTISE	0x3c
+#define LP_1000BASE_EEE	BIT(2)
+#define LP_100BASE_EEE	BIT(1)
+
+struct phy_info *phy_search_ids(int phy_id);
+void init_autoeee(struct higmac_netdev_local *ld);
+
+#endif
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/autoeee/phy_id_table.c linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/autoeee/phy_id_table.c
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/autoeee/phy_id_table.c	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/autoeee/phy_id_table.c	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,177 @@
+#include <linux/delay.h>
+#include <linux/kernel.h>
+#include <linux/phy.h>
+#include "../higmac.h"
+#include "autoeee.h"
+
+struct phy_info phy_info_table[];
+
+struct phy_info *phy_search_ids(int phy_id)
+{
+	int i;
+	struct phy_info *fit_info = NULL;
+
+	for (i = 0; phy_info_table[i].name; i++) {
+		if (phy_id == phy_info_table[i].phy_id)
+			fit_info = &phy_info_table[i];
+	}
+
+	return fit_info;
+}
+
+static inline int phy_mmd_read(struct phy_device *phy_dev,
+			       u32 mmd_device, u32 regnum)
+{
+	phy_write(phy_dev, MACR, mmd_device);	/* function = 00 address */
+	phy_write(phy_dev, MAADR, regnum);
+	phy_write(phy_dev, MACR, 0x4000 | mmd_device);	/* function = 01 data */
+
+	return phy_read(phy_dev, MAADR);
+}
+
+static inline int phy_mmd_write(struct phy_device *phy_dev, u32 mmd_device,
+				u32 regnum, u16 val)
+{
+	phy_write(phy_dev, MACR, mmd_device);	/* function = 00 address */
+	phy_write(phy_dev, MAADR, regnum);
+	phy_write(phy_dev, MACR, 0x4000 | mmd_device);	/* function = 01 data */
+
+	return phy_write(phy_dev, MAADR, val);
+}
+
+static int smsc_lan8740_init(struct phy_device *phy_dev)
+{
+	static int first_time;
+	int v, eee_type = 0;
+
+	if (!first_time) {
+		/* Realtek LAN 8740 start to enable eee */
+		int eee_lan;
+
+		eee_lan = phy_read(phy_dev, 0x10);
+		if (eee_lan < 0)
+			return eee_lan;
+		eee_lan |= 0x4;
+		phy_write(phy_dev, 0x10, eee_lan);
+		eee_lan = phy_read(phy_dev, 0x10);
+		if (eee_lan < 0)
+			return eee_lan;
+		/* auto negotiate after enable eee */
+		eee_lan = phy_read(phy_dev, 0x0);
+		if (eee_lan < 0)
+			return eee_lan;
+		eee_lan |= 0x200;
+		phy_write(phy_dev, 0x0, eee_lan);
+		first_time = 1;
+	}
+
+	v = phy_mmd_read(phy_dev, EEELPAR_DEV, EEELPAR);
+
+	if (v & LP_1000BASE_EEE)
+		eee_type |= HIGMAC_SPD_1000M;
+	if (v & LP_100BASE_EEE)
+		eee_type |= HIGMAC_SPD_100M;
+
+	return eee_type;
+}
+
+#define RTL8211EG_MAC	0
+#if RTL8211EG_MAC
+static int rtl8211EG_mac_init(struct phy_device *phy_dev)
+{
+	static int first_time;
+	/* Realtek 8211EG start reset to change eee to mac */
+	int v, eee_type = 0;
+
+	if (!first_time) {
+		int tmp = 0;
+
+		phy_write(phy_dev, 0x1f, 0x0);
+		phy_write(phy_dev, MII_BMCR, BMCR_RESET);	/* reset phy */
+		do {		/* wait phy restart over */
+			udelay(1);
+			tmp = phy_read(phy_dev, MII_BMSR);
+			/* no need to wait AN finished */
+			tmp &= (BMSR_ANEGCOMPLETE | BMSR_ANEGCAPABLE);
+		} while (!tmp);
+
+		phy_write(phy_dev, 0x1f, 0x7);
+		phy_write(phy_dev, 0x1e, 0x20);
+		phy_write(phy_dev, 0x1b, 0xa03a);
+		phy_write(phy_dev, 0x1f, 0x0);
+
+		first_time = 1;
+	}
+
+	v = phy_mmd_read(phy_dev, EEELPAR_DEV, EEELPAR);
+
+	if (v & LP_1000BASE_EEE)
+		eee_type |= HIGMAC_SPD_1000M;
+	if (v & LP_100BASE_EEE)
+		eee_type |= HIGMAC_SPD_100M;
+
+	return eee_type;
+}
+#else
+static int rtl8211EG_init(struct phy_device *phy_dev)
+{
+	int eee_type = 0, v;
+
+	v = phy_mmd_read(phy_dev, EEELPAR_DEV, EEELPAR);
+
+	if (v & LP_1000BASE_EEE)
+		eee_type |= HIGMAC_SPD_1000M;
+	if (v & LP_100BASE_EEE)
+		eee_type |= HIGMAC_SPD_100M;
+
+	return eee_type;
+}
+#endif
+
+static int festa_v200_init(struct phy_device *phy_dev)
+{
+	static int first_time_init;
+	int v, eee_type = 0;
+
+	if (!first_time_init) {
+		/* EEE_CAPABILITY register: support 100M-BaseT */
+		v = phy_mmd_read(phy_dev, EEE_DEV, EEE_CAPABILITY);
+		phy_mmd_write(phy_dev, EEE_DEV, EEE_CAPABILITY,
+			      ((u32)v) | BIT(1));
+
+		/* EEE_ADVERTISEMENT register: advertising 100M-BaseT */
+		v = phy_mmd_read(phy_dev, EEELPAR_DEV, EEE_ADVERTISE);
+		phy_mmd_write(phy_dev, EEELPAR_DEV, EEE_ADVERTISE,
+			      ((u32)v) | BIT(1));
+
+		v = phy_read(phy_dev, MII_BMCR);
+		if (v < 0)
+			return v;
+		v |= (BMCR_ANENABLE | BMCR_ANRESTART);
+		phy_write(phy_dev, MII_BMCR, v);	/* auto-neg restart */
+
+		first_time_init = 1;
+	}
+
+	v = phy_mmd_read(phy_dev, EEELPAR_DEV, EEELPAR);
+
+	if (v & LP_1000BASE_EEE)
+		eee_type |= HIGMAC_SPD_1000M;
+	if (v & LP_100BASE_EEE)
+		eee_type |= HIGMAC_SPD_100M;
+
+	return eee_type;
+}
+
+struct phy_info phy_info_table[] = {
+	/* phy_name             phy_id  eee_available   phy_driver */
+/* SMSC */
+	{"SMSC LAN8740", 0x0007c110, MAC_EEE, &smsc_lan8740_init},
+/* Realtek */
+#if RTL8211EG_MAC
+	{"Realtek 8211EG", 0x001cc915, MAC_EEE, &rtl8211EG_mac_init},
+#else
+	{"Realtek 8211EG", 0x001cc915, PHY_EEE, &rtl8211EG_init},
+#endif
+	{"Festa V200", HISILICON_PHY_ID_FESTAV200, MAC_EEE, &festa_v200_init},
+};
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/board.c linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/board.c
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/board.c	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/board.c	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,96 @@
+#include <linux/clk.h>
+#include <linux/kernel.h>
+#include <linux/reset.h>
+#include "higmac.h"
+
+void higmac_mac_core_reset(struct higmac_netdev_local *priv)
+{
+	/* undo reset */
+	reset_control_deassert(priv->port_rst);
+	usleep_range(50, 60);
+
+	/* soft reset mac port */
+	reset_control_assert(priv->port_rst);
+	usleep_range(50, 60);
+	/* undo reset */
+	reset_control_deassert(priv->port_rst);
+}
+
+void higmac_hw_internal_phy_reset(struct higmac_netdev_local *priv)
+{
+}
+
+void higmac_hw_phy_reset(struct higmac_netdev_local *priv)
+{
+	if (priv->internal_phy)
+		higmac_hw_internal_phy_reset(priv);
+	else
+		higmac_hw_external_phy_reset(priv);
+}
+
+void higmac_hw_external_phy_reset(struct higmac_netdev_local *priv)
+{
+	if (priv->phy_rst) {
+		/* write 0 to cancel reset */
+		reset_control_deassert(priv->phy_rst);
+		msleep(50);
+
+		/* HIFONE or 98cv200 use CRG register to reset phy */
+		/* RST_BIT, write 0 to reset phy, write 1 to cancel reset */
+		reset_control_assert(priv->phy_rst);
+
+		/* delay some time to ensure reset ok,
+		 * this depends on PHY hardware feature
+		 */
+		msleep(50);
+
+		/* write 0 to cancel reset */
+		reset_control_deassert(priv->phy_rst);
+		/* delay some time to ensure later MDIO access */
+		msleep(50);
+	}
+}
+
+void higmac_internal_phy_clk_disable(struct higmac_netdev_local *priv)
+{
+}
+
+void higmac_internal_phy_clk_enable(struct higmac_netdev_local *priv)
+{
+}
+
+void higmac_hw_all_clk_disable(struct higmac_netdev_local *priv)
+{
+	/* If macif clock is enabled when suspend, we should
+	 * disable it here.
+	 * Because when resume, PHY will link up again and
+	 * macif clock will be enabled too. If we don't disable
+	 * macif clock in suspend, macif clock will be enabled twice.
+	 */
+	if (priv->netdev->flags & IFF_UP)
+		clk_disable_unprepare(priv->macif_clk);
+
+	/* This is called in suspend, when net device is down,
+	 * MAC clk is disabled.
+	 * So we need to judge whether MAC clk is enabled,
+	 * otherwise kernel will WARNING if clk disable twice.
+	 */
+	if (priv->netdev->flags & IFF_UP)
+		clk_disable_unprepare(priv->clk);
+
+	if (priv->internal_phy)
+		higmac_internal_phy_clk_disable(priv);
+}
+
+void higmac_hw_all_clk_enable(struct higmac_netdev_local *priv)
+{
+	if (priv->internal_phy)
+		higmac_internal_phy_clk_enable(priv);
+
+	if (priv->netdev->flags & IFF_UP)
+		clk_prepare_enable(priv->macif_clk);
+
+	/* If net device is down when suspend, we should not enable MAC clk. */
+	if (priv->netdev->flags & IFF_UP)
+		clk_prepare_enable(priv->clk);
+}
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/higmac.c linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/higmac.c
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/higmac.c	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/higmac.c	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,2997 @@
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/unistd.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/spinlock.h>
+#include <linux/mm.h>
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include <linux/phy.h>
+#include <linux/dma-mapping.h>
+#include <linux/workqueue.h>
+#include <linux/device.h>
+#include <linux/atomic.h>
+#include <linux/platform_device.h>
+#include <linux/capability.h>
+#include <linux/time.h>
+#include <asm/setup.h>
+#include <linux/proc_fs.h>
+#include <linux/module.h>
+
+#include <linux/circ_buf.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+#include <net/ipv6.h>
+
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
+#include <linux/clk.h>
+#include <linux/reset.h>
+
+#include "util.h"
+#include "higmac.h"
+#include "autoeee/autoeee.h"
+#include "sockioctl.h"
+
+#define HAS_TSO_CAP(hw_cap)		((((hw_cap) >> 28) & 0x3) == VER_TSO)
+#define HAS_RXHASH_CAP(hw_cap)		((hw_cap) & BIT(30))
+#define HAS_RSS_CAP(hw_cap)		((hw_cap) & BIT(31))
+
+#define RGMII_SPEED_1000		0x2c
+#define RGMII_SPEED_100			0x2f
+#define RGMII_SPEED_10			0x2d
+#define MII_SPEED_100			0x0f
+#define MII_SPEED_10			0x0d
+#define RMII_SPEED_100			0x8f
+#define RMII_SPEED_10			0x8d
+#define GMAC_FULL_DUPLEX		BIT(4)
+
+static unsigned int flow_ctrl_en = FLOW_OFF;
+static int tx_flow_ctrl_pause_time = CONFIG_TX_FLOW_CTRL_PAUSE_TIME;
+static int tx_flow_ctrl_pause_interval = CONFIG_TX_FLOW_CTRL_PAUSE_INTERVAL;
+static int tx_flow_ctrl_active_threshold = CONFIG_TX_FLOW_CTRL_ACTIVE_THRESHOLD;
+static int tx_flow_ctrl_deactive_threshold =
+				CONFIG_TX_FLOW_CTRL_DEACTIVE_THRESHOLD;
+
+#define DEFAULT_MSG_ENABLE (NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_LINK)
+static int debug = -1;
+module_param(debug, int, 0000);
+MODULE_PARM_DESC(debug, "Debug level (0=none,...,16=all)");
+
+static void higmac_config_port(struct net_device *dev, u32 speed, u32 duplex)
+{
+	struct higmac_netdev_local *priv = netdev_priv(dev);
+	u32 val;
+
+	switch (priv->phy_mode) {
+	case PHY_INTERFACE_MODE_RGMII:
+		if (speed == SPEED_1000)
+			val = RGMII_SPEED_1000;
+		else if (speed == SPEED_100)
+			val = RGMII_SPEED_100;
+		else
+			val = RGMII_SPEED_10;
+		break;
+	case PHY_INTERFACE_MODE_MII:
+		if (speed == SPEED_100)
+			val = MII_SPEED_100;
+		else
+			val = MII_SPEED_10;
+		break;
+	case PHY_INTERFACE_MODE_RMII:
+		if (speed == SPEED_100)
+			val = RMII_SPEED_100;
+		else
+			val = RMII_SPEED_10;
+		break;
+	default:
+		netdev_warn(dev, "not supported mode\n");
+		val = MII_SPEED_10;
+		break;
+	}
+
+	if (duplex)
+		val |= GMAC_FULL_DUPLEX;
+
+	reset_control_assert(priv->macif_rst);
+	writel_relaxed(val, priv->macif_base);
+	reset_control_deassert(priv->macif_rst);
+
+	writel_relaxed(BIT_MODE_CHANGE_EN, priv->gmac_iobase + MODE_CHANGE_EN);
+	if (speed == SPEED_1000)
+		val = GMAC_SPEED_1000;
+	else if (speed == SPEED_100)
+		val = GMAC_SPEED_100;
+	else
+		val = GMAC_SPEED_10;
+	writel_relaxed(val, priv->gmac_iobase + PORT_MODE);
+	writel_relaxed(0, priv->gmac_iobase + MODE_CHANGE_EN);
+	writel_relaxed(duplex, priv->gmac_iobase + MAC_DUPLEX_HALF_CTRL);
+}
+
+static void higmac_set_desc_depth(struct higmac_netdev_local *priv,
+				  u32 rx, u32 tx)
+{
+	u32 reg;
+	int i;
+
+	writel(BITS_RX_FQ_DEPTH_EN, priv->gmac_iobase + RX_FQ_REG_EN);
+	writel(rx << DESC_WORD_SHIFT, priv->gmac_iobase + RX_FQ_DEPTH);
+	writel(0, priv->gmac_iobase + RX_FQ_REG_EN);
+
+	writel(BITS_RX_BQ_DEPTH_EN, priv->gmac_iobase + RX_BQ_REG_EN);
+	writel(rx << DESC_WORD_SHIFT, priv->gmac_iobase + RX_BQ_DEPTH);
+	for (i = 1; i < priv->num_rxqs; i++) {
+		reg = RX_BQ_DEPTH_QUEUE(i);
+		writel(rx << DESC_WORD_SHIFT, priv->gmac_iobase + reg);
+	}
+	writel(0, priv->gmac_iobase + RX_BQ_REG_EN);
+
+	writel(BITS_TX_BQ_DEPTH_EN, priv->gmac_iobase + TX_BQ_REG_EN);
+	writel(tx << DESC_WORD_SHIFT, priv->gmac_iobase + TX_BQ_DEPTH);
+	writel(0, priv->gmac_iobase + TX_BQ_REG_EN);
+
+	writel(BITS_TX_RQ_DEPTH_EN, priv->gmac_iobase + TX_RQ_REG_EN);
+	writel(tx << DESC_WORD_SHIFT, priv->gmac_iobase + TX_RQ_DEPTH);
+	writel(0, priv->gmac_iobase + TX_RQ_REG_EN);
+}
+
+static void higmac_set_rx_fq(struct higmac_netdev_local *priv,
+			     dma_addr_t phy_addr)
+{
+	writel(BITS_RX_FQ_START_ADDR_EN, priv->gmac_iobase + RX_FQ_REG_EN);
+	writel(phy_addr, priv->gmac_iobase + RX_FQ_START_ADDR);
+	writel(0, priv->gmac_iobase + RX_FQ_REG_EN);
+}
+
+static void higmac_set_rx_bq(struct higmac_netdev_local *priv,
+			     dma_addr_t phy_addr)
+{
+	writel(BITS_RX_BQ_START_ADDR_EN, priv->gmac_iobase + RX_BQ_REG_EN);
+	writel(phy_addr, priv->gmac_iobase + RX_BQ_START_ADDR);
+	writel(0, priv->gmac_iobase + RX_BQ_REG_EN);
+}
+
+static void higmac_set_tx_bq(struct higmac_netdev_local *priv,
+			     dma_addr_t phy_addr)
+{
+	writel(BITS_TX_BQ_START_ADDR_EN, priv->gmac_iobase + TX_BQ_REG_EN);
+	writel(phy_addr, priv->gmac_iobase + TX_BQ_START_ADDR);
+	writel(0, priv->gmac_iobase + TX_BQ_REG_EN);
+}
+
+static void higmac_set_tx_rq(struct higmac_netdev_local *priv,
+			     dma_addr_t phy_addr)
+{
+	writel(BITS_TX_RQ_START_ADDR_EN, priv->gmac_iobase + TX_RQ_REG_EN);
+	writel(phy_addr, priv->gmac_iobase + TX_RQ_START_ADDR);
+	writel(0, priv->gmac_iobase + TX_RQ_REG_EN);
+}
+
+static void higmac_hw_set_desc_addr(struct higmac_netdev_local *priv)
+{
+	u32 reg;
+	int i;
+
+	higmac_set_rx_fq(priv, priv->rx_fq.phys_addr);
+	higmac_set_rx_bq(priv, priv->rx_bq.phys_addr);
+	higmac_set_tx_rq(priv, priv->tx_rq.phys_addr);
+	higmac_set_tx_bq(priv, priv->tx_bq.phys_addr);
+
+	for (i = 1; i < priv->num_rxqs; i++) {
+		reg = RX_BQ_START_ADDR_QUEUE(i);
+		writel(BITS_RX_BQ_START_ADDR_EN,
+		       priv->gmac_iobase + RX_BQ_REG_EN);
+		writel(priv->pool[3 + i].phys_addr, priv->gmac_iobase + reg);
+		writel(0, priv->gmac_iobase + RX_BQ_REG_EN);
+	}
+}
+
+static void higmac_set_rss_cap(struct higmac_netdev_local *priv)
+{
+	u32 val = 0;
+
+	if (priv->has_rxhash_cap)
+		val |= BIT_RXHASH_CAP;
+	if (priv->has_rss_cap)
+		val |= BIT_RSS_CAP;
+	writel(val, priv->gmac_iobase + HW_CAP_EN);
+}
+
+static void higmac_hw_init(struct higmac_netdev_local *priv)
+{
+	u32 val;
+	u32 reg;
+	int i;
+
+#if defined(CONFIG_ARCH_HI3519) || defined(CONFIG_ARCH_HI3519V101) || \
+	defined(CONFIG_ARCH_HI3559) || defined(CONFIG_ARCH_HI3556) || \
+	defined(CONFIG_ARCH_HI3516AV200)
+	/* config AXI parameter for better performance. */
+	val = readl(priv->gmac_iobase + BURST_OUTSTANDING_REG);
+	val >>= BURST_OUTSTANDING_OFFSET;
+	if (!val)
+		writel(BURST4_OUTSTANDING1, priv->gmac_iobase +
+			BURST_OUTSTANDING_REG);
+#endif
+
+	/* disable and clear all interrupts */
+	writel(0, priv->gmac_iobase + ENA_PMU_INT);
+	writel(~0, priv->gmac_iobase + RAW_PMU_INT);
+
+	for (i = 1; i < priv->num_rxqs; i++) {
+		reg = RSS_ENA_INT_QUEUE(i);
+		writel(0, priv->gmac_iobase + reg);
+	}
+	writel(~0, priv->gmac_iobase + RSS_RAW_PMU_INT);
+
+	/* enable CRC erro packets filter */
+	val = readl(priv->gmac_iobase + REC_FILT_CONTROL);
+	val |= BIT_CRC_ERR_PASS;
+	writel(val, priv->gmac_iobase + REC_FILT_CONTROL);
+
+	/* set tx min packet length */
+	val = readl(priv->gmac_iobase + CRF_MIN_PACKET);
+	val &= ~BIT_MASK_TX_MIN_LEN;
+	val |= ETH_HLEN << BIT_OFFSET_TX_MIN_LEN;
+	writel(val, priv->gmac_iobase + CRF_MIN_PACKET);
+
+	/* fix bug for udp and ip error check */
+	writel(CONTROL_WORD_CONFIG, priv->gmac_iobase + CONTROL_WORD);
+
+	writel(0, priv->gmac_iobase + COL_SLOT_TIME);
+
+	writel(DUPLEX_HALF, priv->gmac_iobase + MAC_DUPLEX_HALF_CTRL);
+
+	/* FIXME: interrupt when rcv packets >= RX_BQ_INT_THRESHOLD */
+	val = RX_BQ_INT_THRESHOLD |
+		(TX_RQ_INT_THRESHOLD << BITS_OFFSET_TX_RQ_IN_TH);
+	writel(val, priv->gmac_iobase + IN_QUEUE_TH);
+
+	/* FIXME: rx_bq/tx_rq in timeout threshold */
+	writel(0x10000, priv->gmac_iobase + RX_BQ_IN_TIMEOUT_TH);
+
+	writel(0x18000, priv->gmac_iobase + TX_RQ_IN_TIMEOUT_TH);
+
+	higmac_set_desc_depth(priv, RX_DESC_NUM, TX_DESC_NUM);
+}
+
+static inline void higmac_irq_enable(struct higmac_netdev_local *ld)
+{
+	writel(RX_BQ_IN_INT | RX_BQ_IN_TIMEOUT_INT
+		| TX_RQ_IN_INT | TX_RQ_IN_TIMEOUT_INT,
+		ld->gmac_iobase + ENA_PMU_INT);
+}
+
+static inline void higmac_irq_enable_queue(struct higmac_netdev_local *ld,
+					   int rxq_id)
+{
+	if (rxq_id) {
+		u32 reg;
+
+		reg = RSS_ENA_INT_QUEUE(rxq_id);
+		writel(~0, ld->gmac_iobase + reg);
+	} else {
+		higmac_irq_enable(ld);
+	}
+}
+
+static inline void higmac_irq_enable_all_queue(struct higmac_netdev_local *ld)
+{
+	int i;
+
+	for (i = 0; i < ld->num_rxqs; i++)
+		higmac_irq_enable_queue(ld, i);
+}
+
+static inline void higmac_irq_disable(struct higmac_netdev_local *ld)
+{
+	writel(0, ld->gmac_iobase + ENA_PMU_INT);
+}
+
+static inline void higmac_irq_disable_queue(struct higmac_netdev_local *ld,
+					    int rxq_id)
+{
+	if (rxq_id) {
+		u32 reg;
+
+		reg = RSS_ENA_INT_QUEUE(rxq_id);
+		writel(0, ld->gmac_iobase + reg);
+	} else {
+		higmac_irq_disable(ld);
+	}
+}
+
+static inline void higmac_irq_disable_all_queue(struct higmac_netdev_local *ld)
+{
+	int i;
+
+	for (i = 0; i < ld->num_rxqs; i++)
+		higmac_irq_disable_queue(ld, i);
+}
+
+static inline bool higmac_queue_irq_disabled(struct higmac_netdev_local *ld,
+					     int rxq_id)
+{
+	u32 reg, val;
+
+	if (rxq_id)
+		reg = RSS_ENA_INT_QUEUE(rxq_id);
+	else
+		reg = ENA_PMU_INT;
+	val = readl(ld->gmac_iobase + reg);
+
+	return !val;
+}
+
+static inline void higmac_hw_desc_enable(struct higmac_netdev_local *ld)
+{
+	writel(0xF, ld->gmac_iobase + DESC_WR_RD_ENA);
+}
+
+static inline void higmac_hw_desc_disable(struct higmac_netdev_local *ld)
+{
+	writel(0, ld->gmac_iobase + DESC_WR_RD_ENA);
+}
+
+static inline void higmac_port_enable(struct higmac_netdev_local *ld)
+{
+	writel(BITS_TX_EN | BITS_RX_EN, ld->gmac_iobase + PORT_EN);
+}
+
+static inline void higmac_port_disable(struct higmac_netdev_local *ld)
+{
+	writel(0, ld->gmac_iobase + PORT_EN);
+}
+
+void higmac_set_flow_ctrl_params(struct higmac_netdev_local *ld)
+{
+	unsigned int rx_fq_empty_th;
+	unsigned int rx_fq_full_th;
+	unsigned int rx_bq_empty_th;
+	unsigned int rx_bq_full_th;
+	unsigned int rec_filter;
+
+	writel(ld->pause, ld->gmac_iobase + FC_TX_TIMER);
+	writel(ld->pause_interval, ld->gmac_iobase + PAUSE_THR);
+
+	rx_fq_empty_th = readl(ld->gmac_iobase + RX_FQ_ALEMPTY_TH);
+	rx_fq_empty_th &= ~(BITS_Q_PAUSE_TH_MASK << BITS_Q_PAUSE_TH_OFFSET);
+	rx_fq_empty_th |= (ld->flow_ctrl_active_threshold <<
+			BITS_Q_PAUSE_TH_OFFSET);
+	writel(rx_fq_empty_th, ld->gmac_iobase + RX_FQ_ALEMPTY_TH);
+
+	rx_fq_full_th = readl(ld->gmac_iobase + RX_FQ_ALFULL_TH);
+	rx_fq_full_th &= ~(BITS_Q_PAUSE_TH_MASK << BITS_Q_PAUSE_TH_OFFSET);
+	rx_fq_full_th |= (ld->flow_ctrl_deactive_threshold <<
+			BITS_Q_PAUSE_TH_OFFSET);
+	writel(rx_fq_full_th, ld->gmac_iobase + RX_FQ_ALFULL_TH);
+
+	rx_bq_empty_th = readl(ld->gmac_iobase + RX_BQ_ALEMPTY_TH);
+	rx_bq_empty_th &= ~(BITS_Q_PAUSE_TH_MASK << BITS_Q_PAUSE_TH_OFFSET);
+	rx_bq_empty_th |= (ld->flow_ctrl_active_threshold <<
+			BITS_Q_PAUSE_TH_OFFSET);
+	writel(rx_bq_empty_th, ld->gmac_iobase + RX_BQ_ALEMPTY_TH);
+
+	rx_bq_full_th = readl(ld->gmac_iobase + RX_BQ_ALFULL_TH);
+	rx_bq_full_th &= ~(BITS_Q_PAUSE_TH_MASK << BITS_Q_PAUSE_TH_OFFSET);
+	rx_bq_full_th |= (ld->flow_ctrl_deactive_threshold <<
+			BITS_Q_PAUSE_TH_OFFSET);
+	writel(rx_bq_full_th, ld->gmac_iobase + RX_BQ_ALFULL_TH);
+
+	writel(0, ld->gmac_iobase + CRF_TX_PAUSE);
+
+	rec_filter = readl(ld->gmac_iobase + REC_FILT_CONTROL);
+	rec_filter |= BIT_PAUSE_FRM_PASS;
+	writel(rec_filter, ld->gmac_iobase + REC_FILT_CONTROL);
+}
+
+void higmac_set_flow_ctrl_state(struct higmac_netdev_local *ld, int pause)
+{
+	unsigned int flow_rx_q_en;
+	unsigned int flow;
+
+	flow_rx_q_en = readl(ld->gmac_iobase + RX_PAUSE_EN);
+	flow_rx_q_en &= ~(BIT_RX_FQ_PAUSE_EN | BIT_RX_BQ_PAUSE_EN);
+	if (pause && (ld->flow_ctrl & FLOW_TX))
+		flow_rx_q_en |= (BIT_RX_FQ_PAUSE_EN | BIT_RX_BQ_PAUSE_EN);
+	writel(flow_rx_q_en, ld->gmac_iobase + RX_PAUSE_EN);
+
+	flow = readl(ld->gmac_iobase + PAUSE_EN);
+	flow &= ~(BIT_RX_FDFC | BIT_TX_FDFC);
+	if (pause) {
+		if (ld->flow_ctrl & FLOW_RX)
+			flow |= BIT_RX_FDFC;
+		if (ld->flow_ctrl & FLOW_TX)
+			flow |= BIT_TX_FDFC;
+	}
+	writel(flow, ld->gmac_iobase + PAUSE_EN);
+}
+
+static void higmac_set_flow_ctrl_args(struct higmac_netdev_local *ld)
+{
+	ld->flow_ctrl = flow_ctrl_en;
+	ld->pause = tx_flow_ctrl_pause_time;
+	ld->pause_interval = tx_flow_ctrl_pause_interval;
+	ld->flow_ctrl_active_threshold = tx_flow_ctrl_active_threshold;
+	ld->flow_ctrl_deactive_threshold = tx_flow_ctrl_deactive_threshold;
+}
+
+/* set gmac's multicast list, here we setup gmac's mc filter */
+static void higmac_gmac_multicast_list(struct net_device *dev)
+{
+	struct higmac_netdev_local *ld = netdev_priv(dev);
+	unsigned int rec_filter;
+
+	rec_filter = readl(ld->gmac_iobase + REC_FILT_CONTROL);
+	/* when set gmac in promisc mode
+	 * a. dev in IFF_PROMISC mode
+	 */
+	if ((dev->flags & IFF_PROMISC)) {
+		/* promisc mode.received all pkgs. */
+		rec_filter &= ~(BIT_BC_DROP_EN | BIT_MC_MATCH_EN |
+				BIT_UC_MATCH_EN);
+	} else {
+		/* drop uc pkgs with field 'DA' not match our's */
+		rec_filter |= BIT_UC_MATCH_EN;
+
+		if (dev->flags & IFF_BROADCAST)	/* no broadcast */
+			rec_filter &= ~BIT_BC_DROP_EN;
+		else
+			rec_filter |= BIT_BC_DROP_EN;
+
+		if (netdev_mc_empty(dev) || !(dev->flags & IFF_MULTICAST)) {
+			/* haven't join any mc group */
+			writel(0, ld->gmac_iobase + PORT_MC_ADDR_LOW);
+			writel(0, ld->gmac_iobase + PORT_MC_ADDR_HIGH);
+			rec_filter |= BIT_MC_MATCH_EN;
+		} else if (netdev_mc_count(dev) == 1 &&
+			(dev->flags & IFF_MULTICAST)) {
+			struct netdev_hw_addr *ha;
+			unsigned int d = 0;
+
+			netdev_for_each_mc_addr(ha, dev) {
+				d = (ha->addr[0] << 8) | (ha->addr[1]);
+				writel(d, ld->gmac_iobase + PORT_MC_ADDR_HIGH);
+
+				d = (ha->addr[2] << 24) | (ha->addr[3] << 16)
+					| (ha->addr[4] << 8) | (ha->addr[5]);
+				writel(d, ld->gmac_iobase + PORT_MC_ADDR_LOW);
+			}
+			rec_filter |= BIT_MC_MATCH_EN;
+		} else {
+			rec_filter &= ~BIT_MC_MATCH_EN;
+		}
+	}
+	writel(rec_filter, ld->gmac_iobase + REC_FILT_CONTROL);
+}
+
+/* the func stop the hw desc and relaim the software skb resource
+ * before reusing the gmac, you'd better reset the gmac
+ */
+void higmac_reclaim_rx_tx_resource(struct higmac_netdev_local *ld)
+{
+	unsigned long rxflags, txflags;
+	int rd_offset, wr_offset;
+	int i;
+
+	higmac_irq_disable_all_queue(ld);
+	higmac_hw_desc_disable(ld);
+	writel(STOP_RX_TX, ld->gmac_iobase + STOP_CMD);
+
+	spin_lock_irqsave(&ld->rxlock, rxflags);
+	/* rx_bq: logic write pointer */
+	wr_offset = readl(ld->gmac_iobase + RX_BQ_WR_ADDR);
+	/* rx_bq: software read pointer */
+	rd_offset = readl(ld->gmac_iobase + RX_BQ_RD_ADDR);
+	/* FIXME: prevent to reclaim skb in rx bottom half */
+	writel(wr_offset, ld->gmac_iobase + RX_BQ_RD_ADDR);
+
+	for (i = 1; i < ld->num_rxqs; i++) {
+		u32 rx_bq_wr_reg, rx_bq_rd_reg;
+
+		rx_bq_wr_reg = RX_BQ_WR_ADDR_QUEUE(i);
+		rx_bq_rd_reg = RX_BQ_RD_ADDR_QUEUE(i);
+
+		wr_offset = readl(ld->gmac_iobase + rx_bq_wr_reg);
+		writel(wr_offset, ld->gmac_iobase + rx_bq_rd_reg);
+	}
+
+	/* rx_fq: software write pointer */
+	wr_offset = readl(ld->gmac_iobase + RX_FQ_WR_ADDR);
+	/* rx_fq: logic read pointer */
+	rd_offset = readl(ld->gmac_iobase + RX_FQ_RD_ADDR);
+	if (!rd_offset)
+		rd_offset = (RX_DESC_NUM - 1) << DESC_BYTE_SHIFT;
+	else
+		rd_offset -= DESC_SIZE;
+	/* FIXME: stop to feed hw desc */
+	writel(rd_offset, ld->gmac_iobase + RX_FQ_WR_ADDR);
+
+	for (i = 0; i < ld->rx_fq.count; i++) {
+		if (!ld->rx_fq.skb[i])
+			ld->rx_fq.skb[i] = SKB_MAGIC;
+	}
+	spin_unlock_irqrestore(&ld->rxlock, rxflags);
+
+	/* no need to wait pkts in tx_rq finish to free all skb,
+	 * because higmac_xmit_reclaim is in the tx_lock,
+	 */
+	spin_lock_irqsave(&ld->txlock, txflags);
+	/* tx_rq: logic write */
+	wr_offset = readl(ld->gmac_iobase + TX_RQ_WR_ADDR);
+	/* tx_rq: software read */
+	rd_offset = readl(ld->gmac_iobase + TX_RQ_RD_ADDR);
+	/* FIXME: stop to reclaim tx skb */
+	writel(wr_offset, ld->gmac_iobase + TX_RQ_RD_ADDR);
+
+	/* tx_bq: logic read */
+	rd_offset = readl(ld->gmac_iobase + TX_BQ_RD_ADDR);
+	if (!rd_offset)
+		rd_offset = (TX_DESC_NUM - 1) << DESC_BYTE_SHIFT;
+	else
+		rd_offset -= DESC_SIZE;
+	/* FIXME: stop software tx skb */
+	writel(rd_offset, ld->gmac_iobase + TX_BQ_WR_ADDR);
+
+	for (i = 0; i < ld->tx_bq.count; i++) {
+		if (!ld->tx_bq.skb[i])
+			ld->tx_bq.skb[i] = SKB_MAGIC;
+	}
+	spin_unlock_irqrestore(&ld->txlock, txflags);
+}
+
+static void higmac_monitor_func(unsigned long arg);
+static void higmac_set_multicast_list(struct net_device *dev);
+
+static void higmac_hw_set_mac_addr(struct net_device *dev)
+{
+	struct higmac_netdev_local *priv = netdev_priv(dev);
+	unsigned char *mac = dev->dev_addr;
+	u32 val;
+
+	val = mac[1] | (mac[0] << 8);
+	writel(val, priv->gmac_iobase + STATION_ADDR_HIGH);
+
+	val = mac[5] | (mac[4] << 8) | (mac[3] << 16) | (mac[2] << 24);
+	writel(val, priv->gmac_iobase + STATION_ADDR_LOW);
+}
+
+static void higmac_rx_refill(struct higmac_netdev_local *priv);
+
+/* reset and re-config gmac */
+void higmac_restart(struct higmac_netdev_local *ld)
+{
+	unsigned long rxflags, txflags;
+	struct sk_buff *skb = NULL;
+	int i;
+
+	/* restart hw engine now */
+	higmac_mac_core_reset(ld);
+
+	spin_lock_irqsave(&ld->rxlock, rxflags);
+	spin_lock_irqsave(&ld->txlock, txflags);
+
+	for (i = 0; i < ld->rx_fq.count; i++) {
+		skb = ld->rx_fq.skb[i];
+		if (skb) {
+			ld->rx_skb[i] = NULL;
+			ld->rx_fq.skb[i] = NULL;
+			if (skb == SKB_MAGIC)
+				continue;
+			dev_kfree_skb_any(skb);
+			/* TODO: need to unmap the skb here
+			 * but there is no way to get the dma_addr here,
+			 * and unmap(TO_DEVICE) ops do nothing in fact,
+			 * so we ignore to call
+			 * dma_unmap_single(dev, dma_addr, skb->len,
+			 *      DMA_TO_DEVICE)
+			 */
+		}
+	}
+
+	for (i = 0; i < ld->tx_bq.count; i++) {
+		skb = ld->tx_bq.skb[i];
+		if (skb) {
+			ld->tx_skb[i] = NULL;
+			ld->tx_bq.skb[i] = NULL;
+			if (skb == SKB_MAGIC)
+				continue;
+			dev_kfree_skb_any(skb);
+			/* TODO: unmap the skb */
+		}
+	}
+
+	pmt_reg_restore(ld);
+	higmac_hw_init(ld);
+	higmac_hw_set_mac_addr(ld->netdev);
+	higmac_hw_set_desc_addr(ld);
+
+	/* we don't set macif here, it will be set in adjust_link */
+	if (ld->netdev->flags & IFF_UP) {
+		/* when resume, only do the following operations
+		 * when dev is up before suspend.
+		 */
+		higmac_rx_refill(ld);
+		higmac_set_multicast_list(ld->netdev);
+
+		higmac_hw_desc_enable(ld);
+		higmac_port_enable(ld);
+		higmac_irq_enable_all_queue(ld);
+	}
+	spin_unlock_irqrestore(&ld->txlock, txflags);
+	spin_unlock_irqrestore(&ld->rxlock, rxflags);
+}
+
+static int higmac_net_set_mac_address(struct net_device *dev, void *p)
+{
+	int ret;
+
+	ret = eth_mac_addr(dev, p);
+	if (!ret)
+		higmac_hw_set_mac_addr(dev);
+
+	return ret;
+}
+
+static void higmac_adjust_link(struct net_device *dev)
+{
+	struct higmac_netdev_local *priv = netdev_priv(dev);
+	struct phy_device *phy = priv->phy;
+	bool link_status_changed = false;
+
+	if (phy->link) {
+		if ((priv->old_speed != phy->speed) ||
+		    (priv->old_duplex != phy->duplex)) {
+			higmac_config_port(dev, phy->speed, phy->duplex);
+			higmac_set_flow_ctrl_state(priv, phy->pause);
+
+			if (priv->autoeee)
+				init_autoeee(priv);
+
+			link_status_changed = true;
+			priv->old_link = 1;
+			priv->old_speed = phy->speed;
+			priv->old_duplex = phy->duplex;
+		}
+	} else if (priv->old_link) {
+		link_status_changed = true;
+		priv->old_link = 0;
+		priv->old_speed = SPEED_UNKNOWN;
+		priv->old_duplex = DUPLEX_UNKNOWN;
+	}
+
+	if (link_status_changed && netif_msg_link(priv))
+		phy_print_status(phy);
+}
+
+int higmac_tx_avail(struct higmac_netdev_local *ld)
+{
+	int tx_bq_wr_offset, tx_bq_rd_offset;
+
+	tx_bq_wr_offset = readl(ld->gmac_iobase + TX_BQ_WR_ADDR);
+	tx_bq_rd_offset = readl(ld->gmac_iobase + TX_BQ_RD_ADDR);
+
+	return (tx_bq_rd_offset >> DESC_BYTE_SHIFT) + TX_DESC_NUM
+		- (tx_bq_wr_offset >> DESC_BYTE_SHIFT) - 1;
+}
+
+static int higmac_init_sg_desc_queue(struct higmac_netdev_local *ld)
+{
+	ld->sg_count = ld->tx_bq.count + HIGMAC_SG_DESC_ADD;
+	if (HAS_CAP_CCI(ld->hw_cap)) {
+		ld->dma_sg_desc = kmalloc_array(ld->sg_count,
+				sizeof(struct sg_desc),
+				GFP_KERNEL);
+		if (ld->dma_sg_desc)
+			ld->dma_sg_phy = virt_to_phys(ld->dma_sg_desc);
+	} else {
+		ld->dma_sg_desc = (struct sg_desc *)dma_alloc_coherent(ld->dev,
+				ld->sg_count * sizeof(struct sg_desc),
+				&ld->dma_sg_phy, GFP_KERNEL);
+	}
+
+	if (!ld->dma_sg_desc) {
+		pr_err("alloc sg desc dma error!\n");
+		return -ENOMEM;
+	}
+#ifdef HIGMAC_TSO_DEBUG
+	pr_info("Higmac dma_sg_phy: 0x%p\n", (void *)ld->dma_sg_phy);
+#endif
+
+	ld->sg_head = 0;
+	ld->sg_tail = 0;
+
+	return 0;
+}
+
+static void higmac_destroy_sg_desc_queue(struct higmac_netdev_local *ld)
+{
+	if (ld->dma_sg_desc) {
+		if (HAS_CAP_CCI(ld->hw_cap))
+			kfree(ld->dma_sg_desc);
+		else
+			dma_free_coherent(ld->dev,
+					  ld->sg_count * sizeof(struct sg_desc),
+					  ld->dma_sg_desc, ld->dma_sg_phy);
+		ld->dma_sg_desc = NULL;
+	}
+}
+
+static void higmac_monitor_func(unsigned long arg)
+{
+	struct net_device *dev = (struct net_device *)arg;
+	struct higmac_netdev_local *ld = netdev_priv(dev);
+
+	if (!ld || !netif_running(dev)) {
+		higmac_trace(7, "network driver is stopped.");
+		return;
+	}
+
+	spin_lock(&ld->rxlock);
+	higmac_rx_refill(ld);
+	spin_unlock(&ld->rxlock);
+
+	ld->monitor.expires = jiffies + HIGMAC_MONITOR_TIMER;
+	mod_timer(&ld->monitor, ld->monitor.expires);
+}
+
+static void higmac_rx_refill(struct higmac_netdev_local *priv)
+{
+	struct higmac_desc *desc;
+	struct sk_buff *skb;
+	u32 start, end, num, pos, i;
+	u32 len = HIETH_MAX_FRAME_SIZE;
+	dma_addr_t addr;
+
+	/* software write pointer */
+	start = dma_cnt(readl(priv->gmac_iobase + RX_FQ_WR_ADDR));
+	/* logic read pointer */
+	end = dma_cnt(readl(priv->gmac_iobase + RX_FQ_RD_ADDR));
+	num = CIRC_SPACE(start, end, RX_DESC_NUM);
+
+	for (i = 0, pos = start; i < num; i++) {
+		if (priv->rx_fq.skb[pos] || priv->rx_skb[pos])
+			break;
+
+		skb = netdev_alloc_skb_ip_align(priv->netdev, len);
+		if (unlikely(!skb))
+			break;
+
+		if (!HAS_CAP_CCI(priv->hw_cap)) {
+			addr = dma_map_single(priv->dev, skb->data, len,
+					      DMA_FROM_DEVICE);
+			if (dma_mapping_error(priv->dev, addr)) {
+				dev_kfree_skb_any(skb);
+				break;
+			}
+		} else {
+			addr = virt_to_phys(skb->data);
+		}
+
+		desc = priv->rx_fq.desc + pos;
+		desc->data_buff_addr = addr;
+		priv->rx_fq.skb[pos] = skb;
+		priv->rx_skb[pos] = skb;
+
+		desc->buffer_len = len - 1;
+		desc->data_len = 0;
+		desc->fl = 0;
+		desc->descvid = DESC_VLD_FREE;
+		desc->skb_id = pos;
+
+		pos = dma_ring_incr(pos, RX_DESC_NUM);
+	}
+
+	/* This barrier is important here.  It is required to ensure
+	 * the ARM CPU flushes it's DMA write buffers before proceeding
+	 * to the next instruction, to ensure that GMAC will see
+	 * our descriptor changes in memory
+	 */
+	HIGMAC_SYNC_BARRIER();
+
+	if (pos != start)
+		writel(dma_byte(pos), priv->gmac_iobase + RX_FQ_WR_ADDR);
+}
+
+static int higmac_rx(struct net_device *dev, int limit, int rxq_id)
+{
+	struct higmac_netdev_local *ld = netdev_priv(dev);
+	struct sk_buff *skb;
+	struct higmac_desc *desc;
+	dma_addr_t addr;
+	u32 start, end, num, pos, i, len;
+	u32 rx_bq_rd_reg, rx_bq_wr_reg;
+	u16 skb_id;
+
+	rx_bq_rd_reg = RX_BQ_RD_ADDR_QUEUE(rxq_id);
+	rx_bq_wr_reg = RX_BQ_WR_ADDR_QUEUE(rxq_id);
+
+	/* software read pointer */
+	start = dma_cnt(readl(ld->gmac_iobase + rx_bq_rd_reg));
+	/* logic write pointer */
+	end = dma_cnt(readl(ld->gmac_iobase + rx_bq_wr_reg));
+	num = CIRC_CNT(end, start, RX_DESC_NUM);
+	if (num > limit)
+		num = limit;
+
+	/* ensure get updated desc */
+	rmb();
+	for (i = 0, pos = start; i < num; i++) {
+		if (rxq_id)
+			desc = ld->pool[3 + rxq_id].desc + pos;
+		else
+			desc = ld->rx_bq.desc + pos;
+		skb_id = desc->skb_id;
+
+		spin_lock(&ld->rxlock);
+		skb = ld->rx_skb[skb_id];
+		if (unlikely(!skb)) {
+			spin_unlock(&ld->rxlock);
+			netdev_err(dev, "inconsistent rx_skb\n");
+			break;
+		}
+
+		/* data consistent check */
+		if (unlikely(skb != ld->rx_fq.skb[skb_id])) {
+			netdev_err(dev, "desc->skb(0x%p),rx_fq.skb[%d](0x%p)\n",
+				   skb, skb_id, ld->rx_fq.skb[skb_id]);
+			if (ld->rx_fq.skb[skb_id] == SKB_MAGIC) {
+				spin_unlock(&ld->rxlock);
+				goto next;
+			}
+			WARN_ON(1);
+		} else {
+			ld->rx_fq.skb[skb_id] = NULL;
+		}
+		spin_unlock(&ld->rxlock);
+
+		len = desc->data_len;
+
+		if (!HAS_CAP_CCI(ld->hw_cap)) {
+			addr = desc->data_buff_addr;
+			dma_unmap_single(ld->dev, addr, HIETH_MAX_FRAME_SIZE,
+					 DMA_FROM_DEVICE);
+		}
+
+		skb_put(skb, len);
+		if (skb->len > HIETH_MAX_FRAME_SIZE) {
+			netdev_err(dev, "rcv len err, len = %d\n", skb->len);
+			dev->stats.rx_errors++;
+			dev->stats.rx_length_errors++;
+			dev_kfree_skb_any(skb);
+			goto next;
+		}
+
+		skb->protocol = eth_type_trans(skb, dev);
+		skb->ip_summed = CHECKSUM_NONE;
+#if defined(CONFIG_HIGMAC_RXCSUM)
+		if (dev->features & NETIF_F_RXCSUM) {
+			int hdr_csum_done =
+				desc->header_csum_done;
+			int payload_csum_done =
+				desc->payload_csum_done;
+			int hdr_csum_err =
+				desc->header_csum_err;
+			int payload_csum_err =
+				desc->payload_csum_err;
+
+			if (hdr_csum_done && payload_csum_done) {
+				if (unlikely(hdr_csum_err ||
+					     payload_csum_err)) {
+					dev->stats.rx_errors++;
+					dev->stats.rx_crc_errors++;
+					dev_kfree_skb_any(skb);
+					goto next;
+				} else {
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+				}
+			}
+		}
+#endif
+		if ((dev->features & NETIF_F_RXHASH) && desc->has_hash)
+			skb_set_hash(skb, desc->rxhash, desc->l3_hash ?
+				     PKT_HASH_TYPE_L3 : PKT_HASH_TYPE_L4);
+
+		skb_record_rx_queue(skb, rxq_id);
+
+		napi_gro_receive(&ld->q_napi[rxq_id].napi, skb);
+		dev->stats.rx_packets++;
+		dev->stats.rx_bytes += skb->len;
+		dev->last_rx = jiffies;
+next:
+		spin_lock(&ld->rxlock);
+		ld->rx_skb[skb_id] = NULL;
+		spin_unlock(&ld->rxlock);
+		pos = dma_ring_incr(pos, RX_DESC_NUM);
+	}
+
+	if (pos != start)
+		writel(dma_byte(pos), ld->gmac_iobase + rx_bq_rd_reg);
+
+	spin_lock(&ld->rxlock);
+	higmac_rx_refill(ld);
+	spin_unlock(&ld->rxlock);
+
+	return num;
+}
+
+#ifdef HIGMAC_TSO_DEBUG
+unsigned int id_send;
+unsigned int id_free;
+struct send_pkt_info pkt_rec[MAX_RECORD];
+#endif
+
+static int higmac_check_tx_err(struct higmac_netdev_local *ld,
+			       struct higmac_tso_desc *tx_bq_desc,
+			       unsigned int desc_pos)
+{
+	unsigned int tx_err = tx_bq_desc->tx_err;
+
+	if (unlikely(tx_err & ERR_ALL)) {
+		struct sg_desc *desc_cur;
+		int *sg_word;
+		int i;
+
+		WARN((tx_err & ERR_ALL),
+		     "TX ERR: desc1=0x%x, desc2=0x%x, desc5=0x%x\n",
+		     tx_bq_desc->data_buff_addr,
+		     tx_bq_desc->desc1.val, tx_bq_desc->tx_err);
+
+		desc_cur = ld->dma_sg_desc + ld->tx_bq.sg_desc_offset[desc_pos];
+		sg_word = (int *)desc_cur;
+		for (i = 0; i < sizeof(struct sg_desc) / sizeof(int); i++)
+			pr_err("%s,%d: sg_desc word[%d]=0x%x\n",
+			       __func__, __LINE__, i, sg_word[i]);
+
+		return -1;
+	}
+
+	return 0;
+}
+
+static int higmac_xmit_release_gso(struct higmac_netdev_local *ld,
+				   struct higmac_tso_desc *tx_bq_desc,
+				   unsigned int desc_pos)
+{
+	int pkt_type;
+	int nfrags = tx_bq_desc->desc1.tx.nfrags_num;
+	dma_addr_t addr;
+	size_t len;
+
+	if (unlikely(higmac_check_tx_err(ld, tx_bq_desc, desc_pos) < 0)) {
+		/* dev_close */
+		higmac_irq_disable_all_queue(ld);
+		higmac_hw_desc_disable(ld);
+
+		netif_carrier_off(ld->netdev);
+		netif_stop_queue(ld->netdev);
+
+		phy_stop(ld->phy);
+		del_timer_sync(&ld->monitor);
+		return -1;
+	}
+
+	if (tx_bq_desc->desc1.tx.tso_flag || nfrags)
+		pkt_type = PKT_SG;
+	else
+		pkt_type = PKT_NORMAL;
+
+	if (pkt_type == PKT_NORMAL) {
+		if (!HAS_CAP_CCI(ld->hw_cap)) {
+			addr = tx_bq_desc->data_buff_addr;
+			len = tx_bq_desc->desc1.tx.data_len;
+			dma_unmap_single(ld->dev, addr, len, DMA_TO_DEVICE);
+		}
+	} else {
+		if (!HAS_CAP_CCI(ld->hw_cap)) {
+			struct sg_desc *desc_cur;
+			unsigned int desc_offset;
+			int i;
+
+			desc_offset = ld->tx_bq.sg_desc_offset[desc_pos];
+			WARN_ON(desc_offset != ld->sg_tail);
+			desc_cur = ld->dma_sg_desc + desc_offset;
+
+			addr = desc_cur->linear_addr;
+			len = desc_cur->linear_len;
+			dma_unmap_single(ld->dev, addr, len, DMA_TO_DEVICE);
+			for (i = 0; i < nfrags; i++) {
+				addr = desc_cur->frags[i].addr;
+				len = desc_cur->frags[i].size;
+				dma_unmap_page(ld->dev, addr, len,
+					       DMA_TO_DEVICE);
+			}
+		}
+
+		ld->sg_tail = (ld->sg_tail + 1) % ld->sg_count;
+	}
+
+#ifdef HIGMAC_TSO_DEBUG
+	pkt_rec[id_free].status = 0;
+	id_free++;
+	if (id_free == MAX_RECORD)
+		id_free = 0;
+#endif
+
+	return 0;
+}
+
+static void higmac_xmit_reclaim(struct net_device *dev)
+{
+	struct sk_buff *skb;
+	struct higmac_desc *desc;
+	struct higmac_tso_desc *tso_desc;
+	struct higmac_netdev_local *priv = netdev_priv(dev);
+	unsigned int bytes_compl = 0, pkts_compl = 0;
+	u32 start, end, num, pos, i;
+	dma_addr_t addr;
+	int ret;
+
+	spin_lock(&priv->txlock);
+
+	/* software read */
+	start = dma_cnt(readl(priv->gmac_iobase + TX_RQ_RD_ADDR));
+	/* logic write */
+	end = dma_cnt(readl(priv->gmac_iobase + TX_RQ_WR_ADDR));
+	num = CIRC_CNT(end, start, TX_DESC_NUM);
+
+	for (i = 0, pos = start; i < num; i++) {
+		skb = priv->tx_skb[pos];
+		if (unlikely(!skb)) {
+			netdev_err(dev, "inconsistent tx_skb\n");
+			break;
+		}
+
+		if (skb != priv->tx_bq.skb[pos]) {
+			netdev_err(dev, "wired, tx skb[%d](%p) != skb(%p)\n",
+				   pos, priv->tx_bq.skb[pos], skb);
+			if (priv->tx_bq.skb[pos] == SKB_MAGIC)
+				goto next;
+		}
+
+		pkts_compl++;
+		bytes_compl += skb->len;
+		desc = priv->tx_rq.desc + pos;
+		if (priv->tso_supported) {
+			tso_desc = (struct higmac_tso_desc *)desc;
+			ret = higmac_xmit_release_gso(priv, tso_desc, pos);
+			if (ret < 0)
+				break;
+		} else if (!HAS_CAP_CCI(priv->hw_cap)) {
+			addr = desc->data_buff_addr;
+			dma_unmap_single(priv->dev, addr, skb->len,
+					 DMA_TO_DEVICE);
+		}
+		priv->tx_bq.skb[pos] = NULL;
+next:
+		priv->tx_skb[pos] = NULL;
+		dev_consume_skb_any(skb);
+		pos = dma_ring_incr(pos, TX_DESC_NUM);
+	}
+
+	if (pos != start)
+		writel(dma_byte(pos), priv->gmac_iobase + TX_RQ_RD_ADDR);
+
+	if (pkts_compl || bytes_compl)
+		netdev_completed_queue(dev, pkts_compl, bytes_compl);
+
+	if (unlikely(netif_queue_stopped(priv->netdev)) && pkts_compl)
+		netif_wake_queue(priv->netdev);
+
+	spin_unlock(&priv->txlock);
+}
+
+static int higmac_poll(struct napi_struct *napi, int budget)
+{
+	struct higmac_napi *q_napi = container_of(napi,
+					struct higmac_napi, napi);
+	struct higmac_netdev_local *priv = q_napi->ndev_priv;
+	struct net_device *dev = priv->netdev;
+	int work_done = 0, task = budget;
+	u32 ints, num;
+	u32 raw_int_reg, raw_int_mask;
+
+	if (q_napi->rxq_id) {
+		raw_int_reg = RSS_RAW_PMU_INT;
+		raw_int_mask = DEF_INT_MASK_QUEUE(q_napi->rxq_id);
+	} else {
+		raw_int_reg = RAW_PMU_INT;
+		raw_int_mask = DEF_INT_MASK;
+	}
+
+	do {
+		if (!q_napi->rxq_id)
+			higmac_xmit_reclaim(dev);
+		num = higmac_rx(dev, task, q_napi->rxq_id);
+		work_done += num;
+		task -= num;
+		if (work_done >= budget)
+			break;
+
+		ints = readl(priv->gmac_iobase + raw_int_reg);
+		ints &= raw_int_mask;
+		writel(ints, priv->gmac_iobase + raw_int_reg);
+	} while (ints);
+
+	if (work_done < budget) {
+		napi_complete(napi);
+		higmac_irq_enable_queue(priv, q_napi->rxq_id);
+	}
+
+	return work_done;
+}
+
+static irqreturn_t higmac_interrupt(int irq, void *dev_id)
+{
+	struct higmac_napi *q_napi = (struct higmac_napi *)dev_id;
+	struct higmac_netdev_local *ld = q_napi->ndev_priv;
+	u32 ints;
+	u32 raw_int_reg, raw_int_mask;
+
+	if (higmac_queue_irq_disabled(ld, q_napi->rxq_id))
+		return IRQ_NONE;
+
+	if (q_napi->rxq_id) {
+		raw_int_reg = RSS_RAW_PMU_INT;
+		raw_int_mask = DEF_INT_MASK_QUEUE(q_napi->rxq_id);
+	} else {
+		raw_int_reg = RAW_PMU_INT;
+		raw_int_mask = DEF_INT_MASK;
+	}
+
+	ints = readl(ld->gmac_iobase + raw_int_reg);
+	ints &= raw_int_mask;
+	writel(ints, ld->gmac_iobase + raw_int_reg);
+
+	if (likely(ints)) {
+		higmac_irq_disable_queue(ld, q_napi->rxq_id);
+		napi_schedule(&q_napi->napi);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static inline __be16 higmac_get_l3_proto(struct sk_buff *skb)
+{
+	__be16 l3_proto;
+
+	l3_proto = skb->protocol;
+	if (skb->protocol == htons(ETH_P_8021Q))
+		l3_proto = vlan_get_protocol(skb);
+
+	return l3_proto;
+}
+
+static inline unsigned int higmac_get_l4_proto(struct sk_buff *skb)
+{
+	__be16 l3_proto;
+	unsigned int l4_proto = IPPROTO_MAX;
+
+	l3_proto = higmac_get_l3_proto(skb);
+	if (l3_proto == htons(ETH_P_IP))
+		l4_proto = ip_hdr(skb)->protocol;
+	else if (l3_proto == htons(ETH_P_IPV6))
+		l4_proto = ipv6_hdr(skb)->nexthdr;
+
+	return l4_proto;
+}
+
+static inline bool higmac_skb_is_ipv6(struct sk_buff *skb)
+{
+	return (higmac_get_l3_proto(skb) == htons(ETH_P_IPV6));
+}
+
+static inline bool higmac_skb_is_udp(struct sk_buff *skb)
+{
+	return (higmac_get_l4_proto(skb) == IPPROTO_UDP);
+}
+
+static int higmac_check_hw_capability_for_udp(struct sk_buff *skb)
+{
+	struct ethhdr *eth;
+
+	/* hardware can't dea with UFO broadcast packet */
+	eth = (struct ethhdr *)(skb->data);
+	if (skb_is_gso(skb) && is_broadcast_ether_addr(eth->h_dest))
+		return -ENOTSUPP;
+
+	return 0;
+}
+
+static int higmac_check_hw_capability_for_ipv6(struct sk_buff *skb)
+{
+	unsigned int l4_proto = IPPROTO_MAX;
+
+	l4_proto = ipv6_hdr(skb)->nexthdr;
+
+	if ((l4_proto != IPPROTO_TCP) && (l4_proto != IPPROTO_UDP)) {
+		/* when IPv6 next header is not tcp or udp,
+		 * it means that IPv6 next header is extension header.
+		 * Hardware can't deal with this case,
+		 * so do checksumming by software or do GSO by software.
+		 */
+		if (skb_is_gso(skb))
+			return -ENOTSUPP;
+
+		if (skb->ip_summed == CHECKSUM_PARTIAL &&
+		    skb_checksum_help(skb))
+			return -EFAULT;
+	}
+
+	return 0;
+}
+
+static inline bool higmac_skb_is_ipv4_with_options(struct sk_buff *skb)
+{
+	return ((higmac_get_l3_proto(skb) == htons(ETH_P_IP)) &&
+		(ip_hdr(skb)->ihl > 5));
+}
+
+static int higmac_check_hw_capability(struct sk_buff *skb)
+{
+	int ret = 0;
+
+	/* if tcp_mtu_probe() use (2 * tp->mss_cache) as probe_size,
+	 * the linear data length will be larger than 2048,
+	 * the MAC can't handle it, so let the software do it.
+	 */
+	if (skb_is_gso(skb) && (skb_headlen(skb) > 2048))
+		return -ENOTSUPP;
+
+	if (higmac_skb_is_ipv6(skb)) {
+		ret = higmac_check_hw_capability_for_ipv6(skb);
+		if (ret)
+			return ret;
+	}
+
+	if (higmac_skb_is_udp(skb)) {
+		ret = higmac_check_hw_capability_for_udp(skb);
+		if (ret)
+			return ret;
+	}
+
+	if (((skb->ip_summed == CHECKSUM_PARTIAL) || skb_is_gso(skb)) &&
+	    higmac_skb_is_ipv4_with_options(skb))
+		return -ENOTSUPP;
+
+	return 0;
+}
+
+static void higmac_do_udp_checksum(struct sk_buff *skb)
+{
+	int offset;
+	__wsum csum;
+	__sum16 udp_csum;
+
+	offset = skb_checksum_start_offset(skb);
+	WARN_ON(offset >= skb_headlen(skb));
+	csum = skb_checksum(skb, offset, skb->len - offset, 0);
+
+	offset += skb->csum_offset;
+	WARN_ON(offset + sizeof(__sum16) > skb_headlen(skb));
+	udp_csum = csum_fold(csum);
+	if (udp_csum == 0)
+		udp_csum = CSUM_MANGLED_0;
+
+	*(__sum16 *)(skb->data + offset) = udp_csum;
+
+	skb->ip_summed = CHECKSUM_NONE;
+}
+
+static void higmac_get_pkt_info(struct higmac_netdev_local *ld,
+				struct sk_buff *skb,
+				struct higmac_tso_desc *tx_bq_desc)
+{
+	int nfrags = skb_shinfo(skb)->nr_frags;
+
+	__be16 l3_proto;	/* level 3 protocol */
+	unsigned int l4_proto = IPPROTO_MAX;
+	unsigned int max_mss = ETH_DATA_LEN;
+	unsigned char coe_enable = 0;
+	int max_data_len = skb->len - ETH_HLEN;
+
+	if (likely(skb->ip_summed == CHECKSUM_PARTIAL))
+		coe_enable = 1;
+
+	tx_bq_desc->desc1.val = 0;
+
+	if (skb_is_gso(skb)) {
+		tx_bq_desc->desc1.tx.tso_flag = 1;
+		tx_bq_desc->desc1.tx.sg_flag = 1;
+	} else if (nfrags) {
+		tx_bq_desc->desc1.tx.sg_flag = 1;
+	}
+
+	l3_proto = skb->protocol;
+	if (skb->protocol == htons(ETH_P_8021Q)) {
+		l3_proto = vlan_get_protocol(skb);
+		tx_bq_desc->desc1.tx.vlan_flag = 1;
+		max_data_len -= VLAN_HLEN;
+	}
+
+	if (l3_proto == htons(ETH_P_IP)) {
+		struct iphdr *iph;
+
+		iph = ip_hdr(skb);
+		tx_bq_desc->desc1.tx.ip_ver = PKT_IPV4;
+		tx_bq_desc->desc1.tx.ip_hdr_len = iph->ihl;
+
+		if ((max_data_len >= GSO_MAX_SIZE) &&
+		    (ntohs(iph->tot_len) <= (iph->ihl << 2)))
+			iph->tot_len = htons(GSO_MAX_SIZE - 1);
+
+		max_mss -= iph->ihl * WORD_TO_BYTE;
+		l4_proto = iph->protocol;
+	} else if (l3_proto == htons(ETH_P_IPV6)) {
+		tx_bq_desc->desc1.tx.ip_ver = PKT_IPV6;
+		tx_bq_desc->desc1.tx.ip_hdr_len = PKT_IPV6_HDR_LEN;
+		max_mss -= PKT_IPV6_HDR_LEN * WORD_TO_BYTE;
+		l4_proto = ipv6_hdr(skb)->nexthdr;
+	} else {
+		coe_enable = 0;
+	}
+
+	if (l4_proto == IPPROTO_TCP) {
+		tx_bq_desc->desc1.tx.prot_type = PKT_TCP;
+		tx_bq_desc->desc1.tx.prot_hdr_len = tcp_hdr(skb)->doff;
+		max_mss -= tcp_hdr(skb)->doff * WORD_TO_BYTE;
+	} else if (l4_proto == IPPROTO_UDP) {
+		tx_bq_desc->desc1.tx.prot_type = PKT_UDP;
+		tx_bq_desc->desc1.tx.prot_hdr_len = PKT_UDP_HDR_LEN;
+		if (l3_proto == htons(ETH_P_IPV6))
+			max_mss -= sizeof(struct frag_hdr);
+	} else {
+		coe_enable = 0;
+	}
+
+	if (skb_is_gso(skb))
+		tx_bq_desc->desc1.tx.data_len =
+			(skb_shinfo(skb)->gso_size > max_mss) ? max_mss :
+					skb_shinfo(skb)->gso_size;
+	else
+		tx_bq_desc->desc1.tx.data_len = skb->len;
+
+	if (coe_enable && skb_is_gso(skb) && (l4_proto == IPPROTO_UDP))
+		higmac_do_udp_checksum(skb);
+
+	if (coe_enable)
+		tx_bq_desc->desc1.tx.coe_flag = 1;
+
+	tx_bq_desc->desc1.tx.nfrags_num = nfrags;
+
+	tx_bq_desc->desc1.tx.hw_own = DESC_VLD_BUSY;
+}
+
+static int higmac_xmit_gso(struct higmac_netdev_local *ld, struct sk_buff *skb,
+			   struct higmac_tso_desc *tx_bq_desc,
+			   unsigned int desc_pos)
+{
+	int pkt_type = PKT_NORMAL;
+	int nfrags = skb_shinfo(skb)->nr_frags;
+	dma_addr_t addr;
+	int ret;
+
+	if (skb_is_gso(skb) || nfrags) {
+		/* TSO pkt or SG pkt */
+		pkt_type = PKT_SG;
+	} else {		/* Normal pkt */
+		pkt_type = PKT_NORMAL;
+	}
+
+	ret = higmac_check_hw_capability(skb);
+	if (unlikely(ret))
+		return ret;
+
+	higmac_get_pkt_info(ld, skb, tx_bq_desc);
+
+	if (pkt_type == PKT_NORMAL) {
+		if (!HAS_CAP_CCI(ld->hw_cap)) {
+			addr = dma_map_single(ld->dev, skb->data, skb->len,
+					      DMA_TO_DEVICE);
+			ret = dma_mapping_error(ld->dev, addr);
+			if (unlikely(ret)) {
+				pr_err("Normal Packet DMA Mapping fail.\n");
+				return -EFAULT;
+			}
+			tx_bq_desc->data_buff_addr = addr;
+		} else {
+			tx_bq_desc->data_buff_addr = virt_to_phys(skb->data);
+		}
+	} else {
+		struct sg_desc *desc_cur;
+		int i;
+
+		if (unlikely(((ld->sg_head + 1) % ld->sg_count) ==
+			ld->sg_tail)) {
+			/* SG pkt, but sg desc all used */
+			pr_err("WARNING: sg desc all used.\n");
+			return -EBUSY;
+		}
+
+		desc_cur = ld->dma_sg_desc + ld->sg_head;
+
+		/* TODO: deal with ipv6_id */
+		if (tx_bq_desc->desc1.tx.tso_flag &&
+		    tx_bq_desc->desc1.tx.ip_ver == PKT_IPV6 &&
+		    tx_bq_desc->desc1.tx.prot_type == PKT_UDP) {
+			desc_cur->ipv6_id = ntohl(skb_shinfo(skb)->ip6_frag_id);
+		}
+
+		desc_cur->total_len = skb->len;
+		desc_cur->linear_len = skb_headlen(skb);
+		if (!HAS_CAP_CCI(ld->hw_cap)) {
+			addr = dma_map_single(ld->dev, skb->data,
+					      desc_cur->linear_len,
+					      DMA_TO_DEVICE);
+			ret = dma_mapping_error(ld->dev, addr);
+			if (unlikely(ret)) {
+				pr_err("DMA Mapping fail.");
+				return -EFAULT;
+			}
+			desc_cur->linear_addr = addr;
+		} else {
+			desc_cur->linear_addr = virt_to_phys(skb->data);
+		}
+
+		for (i = 0; i < nfrags; i++) {
+			skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+			int len = frag->size;
+
+			if (!HAS_CAP_CCI(ld->hw_cap)) {
+				addr = skb_frag_dma_map(ld->dev, frag, 0, len,
+							DMA_TO_DEVICE);
+				ret = dma_mapping_error(ld->dev, addr);
+				if (unlikely(ret)) {
+					pr_err("skb frag DMA Mapping fail.");
+					return -EFAULT;
+				}
+				desc_cur->frags[i].addr = addr;
+			} else {
+				desc_cur->frags[i].addr =
+					page_to_phys(skb_frag_page(frag)) +
+					frag->page_offset;
+			}
+			desc_cur->frags[i].size = len;
+		}
+		tx_bq_desc->data_buff_addr = ld->dma_sg_phy +
+			ld->sg_head * sizeof(struct sg_desc);
+		ld->tx_bq.sg_desc_offset[desc_pos] = ld->sg_head;
+
+		ld->sg_head = (ld->sg_head + 1) % ld->sg_count;
+	}
+
+#ifdef HIGMAC_TSO_DEBUG
+	memcpy(&pkt_rec[id_send].desc, tx_bq_desc,
+	       sizeof(struct higmac_tso_desc));
+	pkt_rec[id_send].status = 1;
+	id_send++;
+	if (id_send == MAX_RECORD)
+		id_send = 0;
+#endif
+	return 0;
+}
+
+static netdev_tx_t higmac_net_xmit(struct sk_buff *skb, struct net_device *dev);
+
+static netdev_tx_t higmac_sw_gso(struct higmac_netdev_local *ld,
+				 struct sk_buff *skb)
+{
+	struct sk_buff *segs, *curr_skb;
+	int gso_segs = skb_shinfo(skb)->gso_segs;
+
+	if (gso_segs == 0 && skb_shinfo(skb)->gso_size != 0)
+		gso_segs = DIV_ROUND_UP(skb->len, skb_shinfo(skb)->gso_size);
+
+	/* Estimate the number of fragments in the worst case */
+	if (unlikely(higmac_tx_avail(ld) < gso_segs)) {
+		netif_stop_queue(ld->netdev);
+		if (higmac_tx_avail(ld) < gso_segs) {
+			ld->netdev->stats.tx_dropped++;
+			ld->netdev->stats.tx_fifo_errors++;
+			return NETDEV_TX_BUSY;
+		}
+
+		netif_wake_queue(ld->netdev);
+	}
+
+	segs = skb_gso_segment(skb, ld->netdev->features & ~(NETIF_F_CSUM_MASK |
+					NETIF_F_SG | NETIF_F_GSO_SOFTWARE));
+
+	if (IS_ERR_OR_NULL(segs))
+		goto drop;
+
+	do {
+		curr_skb = segs;
+		segs = segs->next;
+		curr_skb->next = NULL;
+		higmac_net_xmit(curr_skb, ld->netdev);
+	} while (segs);
+
+	dev_kfree_skb_any(skb);
+	return NETDEV_TX_OK;
+
+drop:
+	dev_kfree_skb_any(skb);
+	ld->netdev->stats.tx_dropped++;
+	return NETDEV_TX_OK;
+}
+
+static netdev_tx_t higmac_net_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct higmac_netdev_local *ld = netdev_priv(dev);
+	struct higmac_desc *desc;
+	dma_addr_t addr;
+	unsigned long txflags;
+	int ret;
+	u32 pos;
+
+	if (skb->len < ETH_HLEN) {
+		dev_kfree_skb_any(skb);
+		dev->stats.tx_errors++;
+		dev->stats.tx_dropped++;
+		return NETDEV_TX_OK;
+	}
+
+	/* if adding higmac_xmit_reclaim here, iperf tcp client
+	 * performance will be affected, from 550M(avg) to 513M~300M
+	 */
+
+	/* software write pointer */
+	pos = dma_cnt(readl(ld->gmac_iobase + TX_BQ_WR_ADDR));
+
+	spin_lock_irqsave(&ld->txlock, txflags);
+
+	if (unlikely(ld->tx_skb[pos] || ld->tx_bq.skb[pos])) {
+		dev->stats.tx_dropped++;
+		dev->stats.tx_fifo_errors++;
+		netif_stop_queue(dev);
+		spin_unlock_irqrestore(&ld->txlock, txflags);
+
+		return NETDEV_TX_BUSY;
+	}
+
+	ld->tx_bq.skb[pos] = skb;
+	ld->tx_skb[pos] = skb;
+
+	desc = ld->tx_bq.desc + pos;
+
+	if (ld->tso_supported) {
+		ret = higmac_xmit_gso(ld, skb,
+				      (struct higmac_tso_desc *)desc,
+				      pos);
+		if (unlikely(ret < 0)) {
+			ld->tx_skb[pos] = NULL;
+			ld->tx_bq.skb[pos] = NULL;
+			spin_unlock_irqrestore(&ld->txlock, txflags);
+
+			if (ret == -ENOTSUPP)
+				return higmac_sw_gso(ld, skb);
+
+			dev_kfree_skb_any(skb);
+			dev->stats.tx_dropped++;
+			return NETDEV_TX_OK;
+		}
+	} else {
+		if (!HAS_CAP_CCI(ld->hw_cap)) {
+			addr = dma_map_single(ld->dev, skb->data, skb->len,
+					      DMA_TO_DEVICE);
+			if (unlikely(dma_mapping_error(ld->dev, addr))) {
+				dev_kfree_skb_any(skb);
+				dev->stats.tx_dropped++;
+				ld->tx_skb[pos] = NULL;
+				ld->tx_bq.skb[pos] = NULL;
+				spin_unlock_irqrestore(&ld->txlock, txflags);
+				return NETDEV_TX_OK;
+			}
+			desc->data_buff_addr = addr;
+		} else {
+			desc->data_buff_addr = virt_to_phys(skb->data);
+		}
+		desc->buffer_len = HIETH_MAX_FRAME_SIZE - 1;
+		desc->data_len = skb->len;
+		desc->fl = DESC_FL_FULL;
+		desc->descvid = DESC_VLD_BUSY;
+	}
+
+	/* This barrier is important here.  It is required to ensure
+	 * the ARM CPU flushes it's DMA write buffers before proceeding
+	 * to the next instruction, to ensure that GMAC will see
+	 * our descriptor changes in memory
+	 */
+	HIGMAC_SYNC_BARRIER();
+
+	pos = dma_ring_incr(pos, TX_DESC_NUM);
+	writel(dma_byte(pos), ld->gmac_iobase + TX_BQ_WR_ADDR);
+
+	netif_trans_update(dev);
+	dev->stats.tx_packets++;
+	dev->stats.tx_bytes += skb->len;
+	netdev_sent_queue(dev, skb->len);
+
+	spin_unlock_irqrestore(&ld->txlock, txflags);
+
+	return NETDEV_TX_OK;
+}
+
+void higmac_enable_napi(struct higmac_netdev_local *priv)
+{
+	struct higmac_napi *q_napi;
+	int i;
+
+	for (i = 0; i < priv->num_rxqs; i++) {
+		q_napi = &priv->q_napi[i];
+		napi_enable(&q_napi->napi);
+	}
+}
+
+void higmac_disable_napi(struct higmac_netdev_local *priv)
+{
+	struct higmac_napi *q_napi;
+	int i;
+
+	for (i = 0; i < priv->num_rxqs; i++) {
+		q_napi = &priv->q_napi[i];
+		napi_disable(&q_napi->napi);
+	}
+}
+
+static int higmac_net_open(struct net_device *dev)
+{
+	struct higmac_netdev_local *ld = netdev_priv(dev);
+	unsigned long flags;
+
+	clk_prepare_enable(ld->macif_clk);
+	clk_prepare_enable(ld->clk);
+
+	/* If we configure mac address by
+	 * "ifconfig ethX hw ether XX:XX:XX:XX:XX:XX",
+	 * the ethX must be down state and mac core clock is disabled
+	 * which results the mac address has not been configured
+	 * in mac core register.
+	 * So we must set mac address again here,
+	 * because mac core clock is enabled at this time
+	 * and we can configure mac address to mac core register.
+	 */
+	higmac_hw_set_mac_addr(dev);
+
+	/* We should use netif_carrier_off() here,
+	 * because the default state should be off.
+	 * And this call should before phy_start().
+	 */
+	netif_carrier_off(dev);
+	higmac_enable_napi(ld);
+	phy_start(ld->phy);
+
+	higmac_hw_desc_enable(ld);
+	higmac_port_enable(ld);
+	higmac_irq_enable_all_queue(ld);
+
+	spin_lock_irqsave(&ld->rxlock, flags);
+	higmac_rx_refill(ld);
+	spin_unlock_irqrestore(&ld->rxlock, flags);
+
+	ld->monitor.expires = jiffies + HIGMAC_MONITOR_TIMER;
+	mod_timer(&ld->monitor, ld->monitor.expires);
+
+	netif_start_queue(dev);
+
+	return 0;
+}
+
+static int higmac_net_close(struct net_device *dev)
+{
+	struct higmac_netdev_local *ld = netdev_priv(dev);
+
+	higmac_irq_disable_all_queue(ld);
+	higmac_hw_desc_disable(ld);
+
+	higmac_disable_napi(ld);
+
+	netif_carrier_off(dev);
+	netif_stop_queue(dev);
+
+	phy_stop(ld->phy);
+	del_timer_sync(&ld->monitor);
+
+	clk_disable_unprepare(ld->clk);
+	clk_disable_unprepare(ld->macif_clk);
+
+	return 0;
+}
+
+static void higmac_net_timeout(struct net_device *dev)
+{
+	dev->stats.tx_errors++;
+
+	pr_err("tx timeout!\n");
+}
+
+static void higmac_set_multicast_list(struct net_device *dev)
+{
+	higmac_gmac_multicast_list(dev);
+}
+
+static inline void higmac_enable_rxcsum_drop(struct higmac_netdev_local *ld,
+					     bool drop)
+{
+	unsigned int v;
+
+	v = readl(ld->gmac_iobase + TSO_COE_CTRL);
+	if (drop)
+		v |= COE_ERR_DROP;
+	else
+		v &= ~COE_ERR_DROP;
+	writel(v, ld->gmac_iobase + TSO_COE_CTRL);
+}
+
+static int higmac_set_features(struct net_device *dev,
+			       netdev_features_t features)
+{
+	struct higmac_netdev_local *ld = netdev_priv(dev);
+	netdev_features_t changed = dev->features ^ features;
+
+	if (changed & NETIF_F_RXCSUM) {
+		if (features & NETIF_F_RXCSUM)
+			higmac_enable_rxcsum_drop(ld, true);
+		else
+			higmac_enable_rxcsum_drop(ld, false);
+	}
+
+	return 0;
+}
+
+static struct net_device_stats *higmac_net_get_stats(struct net_device *dev)
+{
+	return &dev->stats;
+}
+
+static void higmac_get_drvinfo(struct net_device *net_dev,
+			       struct ethtool_drvinfo *info)
+{
+	strncpy(info->driver, "higmac driver", 15);
+	strncpy(info->version, "higmac v200", 15);
+	strncpy(info->bus_info, "platform", 15);
+}
+
+static unsigned int higmac_get_link(struct net_device *net_dev)
+{
+	struct higmac_netdev_local *ld = netdev_priv(net_dev);
+
+	return ld->phy->link ? HIGMAC_LINKED : 0;
+}
+
+static int higmac_get_settings(struct net_device *net_dev,
+			       struct ethtool_cmd *cmd)
+{
+	struct higmac_netdev_local *ld = netdev_priv(net_dev);
+
+	if (ld->phy)
+		return phy_ethtool_gset(ld->phy, cmd);
+
+	return -EINVAL;
+}
+
+static int higmac_set_settings(struct net_device *net_dev,
+			       struct ethtool_cmd *cmd)
+{
+	struct higmac_netdev_local *ld = netdev_priv(net_dev);
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (ld->phy)
+		return phy_ethtool_sset(ld->phy, cmd);
+
+	return -EINVAL;
+}
+
+static void higmac_get_pauseparam(struct net_device *net_dev,
+				  struct ethtool_pauseparam *pause)
+{
+	struct higmac_netdev_local *ld = netdev_priv(net_dev);
+
+	pause->rx_pause = 0;
+	pause->tx_pause = 0;
+	pause->autoneg = ld->phy->autoneg;
+
+	if (ld->flow_ctrl & FLOW_RX)
+		pause->rx_pause = 1;
+	if (ld->flow_ctrl & FLOW_TX)
+		pause->tx_pause = 1;
+}
+
+static int higmac_set_pauseparam(struct net_device *net_dev,
+				 struct ethtool_pauseparam *pause)
+{
+	struct higmac_netdev_local *ld = netdev_priv(net_dev);
+	struct phy_device *phy = ld->phy;
+	int new_pause = FLOW_OFF;
+	int ret = 0;
+
+	if (pause->rx_pause)
+		new_pause |= FLOW_RX;
+	if (pause->tx_pause)
+		new_pause |= FLOW_TX;
+
+	if (new_pause != ld->flow_ctrl)
+		ld->flow_ctrl = new_pause;
+
+	higmac_set_flow_ctrl_state(ld, phy->pause);
+	phy->advertising &= ~SUPPORTED_Pause;
+	if (ld->flow_ctrl)
+		phy->advertising |= SUPPORTED_Pause;
+
+	if (phy->autoneg) {
+		if (netif_running(net_dev))
+			return phy_start_aneg(phy);
+	}
+
+	return ret;
+}
+
+static u32 higmac_ethtool_getmsglevel(struct net_device *ndev)
+{
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+
+	return priv->msg_enable;
+}
+
+static void higmac_ethtool_setmsglevel(struct net_device *ndev, u32 level)
+{
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+
+	priv->msg_enable = level;
+}
+
+static u32 higmac_get_rxfh_key_size(struct net_device *ndev)
+{
+	return RSS_HASH_KEY_SIZE;
+}
+
+static u32 higmac_get_rxfh_indir_size(struct net_device *ndev)
+{
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+
+	return priv->rss_info.ind_tbl_size;
+}
+
+static int higmac_get_rxfh(struct net_device *ndev, u32 *indir, u8 *hkey,
+			   u8 *hfunc)
+{
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+	struct higmac_rss_info *rss = &priv->rss_info;
+
+	if (hfunc)
+		*hfunc = ETH_RSS_HASH_TOP;
+
+	if (hkey)
+		memcpy(hkey, rss->key, RSS_HASH_KEY_SIZE);
+
+	if (indir) {
+		int i;
+
+		for (i = 0; i < rss->ind_tbl_size; i++)
+			indir[i] = rss->ind_tbl[i];
+	}
+
+	return 0;
+}
+
+static void higmac_get_rss_key(struct higmac_netdev_local *priv)
+{
+	struct higmac_rss_info *rss = &priv->rss_info;
+	u32 hkey;
+
+	hkey = readl(priv->gmac_iobase + RSS_HASH_KEY);
+	*((u32 *)rss->key) = hkey;
+}
+
+static void higmac_set_rss_key(struct higmac_netdev_local *priv)
+{
+	struct higmac_rss_info *rss = &priv->rss_info;
+
+	writel(*((u32 *)rss->key), priv->gmac_iobase + RSS_HASH_KEY);
+}
+
+static int higmac_wait_rss_ready(struct higmac_netdev_local *priv)
+{
+	void __iomem *base = priv->gmac_iobase;
+	int i, timeout = 10000;
+
+	for (i = 0; !(readl(base + RSS_IND_TBL) & BIT_IND_TBL_READY); i++) {
+		if (i == timeout) {
+			netdev_err(priv->netdev, "wait rss ready timeout!\n");
+			return -ETIMEDOUT;
+		}
+		usleep_range(10, 20);
+	}
+
+	return 0;
+}
+
+static void higmac_config_rss(struct higmac_netdev_local *priv)
+{
+	struct higmac_rss_info *rss = &priv->rss_info;
+	u32 rss_val;
+	int i;
+
+	for (i = 0; i < rss->ind_tbl_size; i++) {
+		if (higmac_wait_rss_ready(priv))
+			break;
+		rss_val = BIT_IND_TLB_WR | (rss->ind_tbl[i] << 8) | i;
+		writel(rss_val, priv->gmac_iobase + RSS_IND_TBL);
+	}
+}
+
+static void higmac_get_rss(struct higmac_netdev_local *priv)
+{
+	struct higmac_rss_info *rss = &priv->rss_info;
+	u32 rss_val;
+	int i;
+
+	for (i = 0; i < rss->ind_tbl_size; i++) {
+		if (higmac_wait_rss_ready(priv))
+			break;
+		writel(i, priv->gmac_iobase + RSS_IND_TBL);
+		if (higmac_wait_rss_ready(priv))
+			break;
+		rss_val = readl(priv->gmac_iobase + RSS_IND_TBL);
+		rss->ind_tbl[i] = (rss_val >> 10) & 0x3;
+	}
+}
+
+static int higmac_set_rxfh(struct net_device *ndev, const u32 *indir,
+			   const u8 *hkey, const u8 hfunc)
+{
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+	struct higmac_rss_info *rss = &priv->rss_info;
+
+	if (hfunc != ETH_RSS_HASH_NO_CHANGE && hfunc != ETH_RSS_HASH_TOP)
+		return -EOPNOTSUPP;
+
+	if (indir) {
+		int i;
+
+		for (i = 0; i < rss->ind_tbl_size; i++)
+			rss->ind_tbl[i] = indir[i];
+	}
+
+	if (hkey) {
+		memcpy(rss->key, hkey, RSS_HASH_KEY_SIZE);
+		higmac_set_rss_key(priv);
+	}
+
+	higmac_config_rss(priv);
+
+	return 0;
+}
+
+static int higmac_get_rss_hash_opts(struct higmac_netdev_local *priv,
+				    struct ethtool_rxnfc *info)
+{
+	u32 hash_cfg = priv->rss_info.hash_cfg;
+
+	info->data = 0;
+
+	switch (info->flow_type) {
+	case TCP_V4_FLOW:
+		if (hash_cfg & TCPV4_L3_HASH_EN)
+			info->data |= RXH_IP_SRC | RXH_IP_DST;
+		if (hash_cfg & TCPV4_L4_HASH_EN)
+			info->data |= RXH_L4_B_0_1 | RXH_L4_B_2_3;
+		if (hash_cfg & TCPV4_VLAN_HASH_EN)
+			info->data |= RXH_VLAN;
+		break;
+	case TCP_V6_FLOW:
+		if (hash_cfg & TCPV6_L3_HASH_EN)
+			info->data |= RXH_IP_SRC | RXH_IP_DST;
+		if (hash_cfg & TCPV6_L4_HASH_EN)
+			info->data |= RXH_L4_B_0_1 | RXH_L4_B_2_3;
+		if (hash_cfg & TCPV6_VLAN_HASH_EN)
+			info->data |= RXH_VLAN;
+		break;
+	case UDP_V4_FLOW:
+		if (hash_cfg & UDPV4_L3_HASH_EN)
+			info->data |= RXH_IP_SRC | RXH_IP_DST;
+		if (hash_cfg & UDPV4_L4_HASH_EN)
+			info->data |= RXH_L4_B_0_1 | RXH_L4_B_2_3;
+		if (hash_cfg & UDPV4_VLAN_HASH_EN)
+			info->data |= RXH_VLAN;
+		break;
+	case UDP_V6_FLOW:
+		if (hash_cfg & UDPV6_L3_HASH_EN)
+			info->data |= RXH_IP_SRC | RXH_IP_DST;
+		if (hash_cfg & UDPV6_L4_HASH_EN)
+			info->data |= RXH_L4_B_0_1 | RXH_L4_B_2_3;
+		if (hash_cfg & UDPV6_VLAN_HASH_EN)
+			info->data |= RXH_VLAN;
+		break;
+	case IPV4_FLOW:
+		if (hash_cfg & IPV4_L3_HASH_EN)
+			info->data |= RXH_IP_SRC | RXH_IP_DST;
+		if (hash_cfg & IPV4_VLAN_HASH_EN)
+			info->data |= RXH_VLAN;
+		break;
+	case IPV6_FLOW:
+		if (hash_cfg & IPV6_L3_HASH_EN)
+			info->data |= RXH_IP_SRC | RXH_IP_DST;
+		if (hash_cfg & IPV6_VLAN_HASH_EN)
+			info->data |= RXH_VLAN;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int higmac_get_rxnfc(struct net_device *ndev,
+			    struct ethtool_rxnfc *info, u32 *rules)
+{
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+	int ret = -EOPNOTSUPP;
+
+	switch (info->cmd) {
+	case ETHTOOL_GRXRINGS:
+		info->data = priv->num_rxqs;
+		ret = 0;
+		break;
+	case ETHTOOL_GRXFH:
+		return higmac_get_rss_hash_opts(priv, info);
+	default:
+		break;
+	}
+	return ret;
+}
+
+static void higmac_config_hash_policy(struct higmac_netdev_local *priv)
+{
+	writel(priv->rss_info.hash_cfg, priv->gmac_iobase + RSS_HASH_CONFIG);
+}
+
+static int higmac_set_rss_hash_opts(struct higmac_netdev_local *priv,
+				    struct ethtool_rxnfc *info)
+{
+	u32 hash_cfg = priv->rss_info.hash_cfg;
+
+	netdev_info(priv->netdev, "Set RSS flow type = %d, data = %lld\n",
+		    info->flow_type, info->data);
+
+	if (!(info->data & RXH_IP_SRC) || !(info->data & RXH_IP_DST))
+		return -EINVAL;
+
+	switch (info->flow_type) {
+	case TCP_V4_FLOW:
+		switch (info->data & (RXH_L4_B_0_1 | RXH_L4_B_2_3)) {
+		case 0:
+			hash_cfg &= ~TCPV4_L4_HASH_EN;
+			break;
+		case (RXH_L4_B_0_1 | RXH_L4_B_2_3):
+			hash_cfg |= TCPV4_L4_HASH_EN;
+			break;
+		default:
+			return -EINVAL;
+		}
+		if (info->data & RXH_VLAN)
+			hash_cfg |= TCPV4_VLAN_HASH_EN;
+		else
+			hash_cfg &= ~TCPV4_VLAN_HASH_EN;
+		break;
+	case TCP_V6_FLOW:
+		switch (info->data & (RXH_L4_B_0_1 | RXH_L4_B_2_3)) {
+		case 0:
+			hash_cfg &= ~TCPV6_L4_HASH_EN;
+			break;
+		case (RXH_L4_B_0_1 | RXH_L4_B_2_3):
+			hash_cfg |= TCPV6_L4_HASH_EN;
+			break;
+		default:
+			return -EINVAL;
+		}
+		if (info->data & RXH_VLAN)
+			hash_cfg |= TCPV6_VLAN_HASH_EN;
+		else
+			hash_cfg &= ~TCPV6_VLAN_HASH_EN;
+		break;
+	case UDP_V4_FLOW:
+		switch (info->data & (RXH_L4_B_0_1 | RXH_L4_B_2_3)) {
+		case 0:
+			hash_cfg &= ~UDPV4_L4_HASH_EN;
+			break;
+		case (RXH_L4_B_0_1 | RXH_L4_B_2_3):
+			hash_cfg |= UDPV4_L4_HASH_EN;
+			break;
+		default:
+			return -EINVAL;
+		}
+		if (info->data & RXH_VLAN)
+			hash_cfg |= UDPV4_VLAN_HASH_EN;
+		else
+			hash_cfg &= ~UDPV4_VLAN_HASH_EN;
+		break;
+	case UDP_V6_FLOW:
+		switch (info->data & (RXH_L4_B_0_1 | RXH_L4_B_2_3)) {
+		case 0:
+			hash_cfg &= ~UDPV6_L4_HASH_EN;
+			break;
+		case (RXH_L4_B_0_1 | RXH_L4_B_2_3):
+			hash_cfg |= UDPV6_L4_HASH_EN;
+			break;
+		default:
+			return -EINVAL;
+		}
+		if (info->data & RXH_VLAN)
+			hash_cfg |= UDPV6_VLAN_HASH_EN;
+		else
+			hash_cfg &= ~UDPV6_VLAN_HASH_EN;
+		break;
+	case IPV4_FLOW:
+		if (info->data & (RXH_L4_B_0_1 | RXH_L4_B_2_3))
+			return -EINVAL;
+		if (info->data & RXH_VLAN)
+			hash_cfg |= IPV4_VLAN_HASH_EN;
+		else
+			hash_cfg &= ~IPV4_VLAN_HASH_EN;
+		break;
+	case IPV6_FLOW:
+		if (info->data & (RXH_L4_B_0_1 | RXH_L4_B_2_3))
+			return -EINVAL;
+		if (info->data & RXH_VLAN)
+			hash_cfg |= IPV6_VLAN_HASH_EN;
+		else
+			hash_cfg &= ~IPV6_VLAN_HASH_EN;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	priv->rss_info.hash_cfg = hash_cfg;
+	higmac_config_hash_policy(priv);
+
+	return 0;
+}
+
+static int higmac_set_rxnfc(struct net_device *ndev, struct ethtool_rxnfc *info)
+{
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+
+	switch (info->cmd) {
+	case ETHTOOL_SRXFH:
+		return higmac_set_rss_hash_opts(priv, info);
+	default:
+		break;
+	}
+	return -EOPNOTSUPP;
+}
+
+static const struct ethtool_ops hieth_ethtools_ops = {
+	.get_drvinfo = higmac_get_drvinfo,
+	.get_link = higmac_get_link,
+	.get_settings = higmac_get_settings,
+	.set_settings = higmac_set_settings,
+	.get_pauseparam = higmac_get_pauseparam,
+	.set_pauseparam = higmac_set_pauseparam,
+	.get_msglevel = higmac_ethtool_getmsglevel,
+	.set_msglevel = higmac_ethtool_setmsglevel,
+	.get_rxfh_key_size = higmac_get_rxfh_key_size,
+	.get_rxfh_indir_size = higmac_get_rxfh_indir_size,
+	.get_rxfh = higmac_get_rxfh,
+	.set_rxfh = higmac_set_rxfh,
+	.get_rxnfc = higmac_get_rxnfc,
+	.set_rxnfc = higmac_set_rxnfc,
+};
+
+static const struct net_device_ops hieth_netdev_ops = {
+	.ndo_open = higmac_net_open,
+	.ndo_stop = higmac_net_close,
+	.ndo_start_xmit = higmac_net_xmit,
+	.ndo_tx_timeout = higmac_net_timeout,
+	.ndo_set_rx_mode = higmac_set_multicast_list,
+	.ndo_set_features = higmac_set_features,
+	.ndo_do_ioctl = higmac_ioctl,
+	.ndo_set_mac_address = higmac_net_set_mac_address,
+	.ndo_change_mtu = eth_change_mtu,
+	.ndo_get_stats = higmac_net_get_stats,
+};
+
+static int higmac_of_get_param(struct higmac_netdev_local *ld,
+			       struct device_node *node)
+{
+	/* get auto eee */
+	ld->autoeee = of_property_read_bool(node, "autoeee");
+	/* get internal flag */
+	ld->internal_phy =
+		of_property_read_bool(node, "internal-phy");
+
+	return 0;
+}
+
+static int KSZ8051MNL_phy_fix(struct phy_device *phy_dev)
+{
+	u32 v;
+	int ret;
+
+	if (phy_dev->interface != PHY_INTERFACE_MODE_RMII)
+		return 0;
+
+	ret = phy_read(phy_dev, 0x1F);
+	if (ret < 0)
+		return ret;
+	v = ret;
+	v |= (1 << 7);		/* set phy RMII 50MHz clk; */
+	phy_write(phy_dev, 0x1F, v);
+
+	ret = phy_read(phy_dev, 0x16);
+	if (ret < 0)
+		return ret;
+	v = ret;
+	v |= (1 << 1);		/* set phy RMII override; */
+	phy_write(phy_dev, 0x16, v);
+
+	return 0;
+}
+
+static int KSZ8081RNB_phy_fix(struct phy_device *phy_dev)
+{
+	u32 v;
+	int ret;
+
+	if (phy_dev->interface != PHY_INTERFACE_MODE_RMII)
+		return 0;
+
+	ret = phy_read(phy_dev, 0x1F);
+	if (ret < 0)
+		return ret;
+	v = ret;
+	v |= (1 << 7);		/* set phy RMII 50MHz clk; */
+	phy_write(phy_dev, 0x1F, v);
+
+	return 0;
+}
+
+static int rtl8211e_phy_fix(struct phy_device *phy_dev)
+{
+	u32 v;
+	int ret;
+
+	/* select Extension page */
+	phy_write(phy_dev, 0x1f, 0x7);
+	/* switch ExtPage 164 */
+	phy_write(phy_dev, 0x1e, 0xa4);
+
+	/* config RGMII rx pin io driver max */
+	ret = phy_read(phy_dev, 0x1c);
+	if (ret < 0)
+		return ret;
+	v = ret;
+	v = (v & 0xff03) | 0xfc;
+	phy_write(phy_dev, 0x1c, v);
+
+	/* select to page 0 */
+	phy_write(phy_dev, 0x1f, 0);
+
+	return 0;
+}
+
+static void phy_register_fixups(void)
+{
+	phy_register_fixup_for_uid(PHY_ID_KSZ8051MNL, DEFAULT_PHY_MASK,
+				   KSZ8051MNL_phy_fix);
+	phy_register_fixup_for_uid(PHY_ID_KSZ8081RNB, DEFAULT_PHY_MASK,
+				   KSZ8081RNB_phy_fix);
+	phy_register_fixup_for_uid(REALTEK_PHY_ID_8211E, REALTEK_PHY_MASK,
+				   rtl8211e_phy_fix);
+}
+
+static void higmac_verify_flow_ctrl_args(void)
+{
+#if defined(CONFIG_TX_FLOW_CTRL_SUPPORT)
+	flow_ctrl_en |= FLOW_TX;
+#endif
+#if defined(CONFIG_RX_FLOW_CTRL_SUPPORT)
+	flow_ctrl_en |= FLOW_RX;
+#endif
+	if (tx_flow_ctrl_active_threshold < FC_ACTIVE_MIN ||
+	    tx_flow_ctrl_active_threshold > FC_ACTIVE_MAX)
+		tx_flow_ctrl_active_threshold = FC_ACTIVE_DEFAULT;
+
+	if (tx_flow_ctrl_deactive_threshold < FC_DEACTIVE_MIN ||
+	    tx_flow_ctrl_deactive_threshold > FC_DEACTIVE_MAX)
+		tx_flow_ctrl_deactive_threshold = FC_DEACTIVE_DEFAULT;
+
+	if (tx_flow_ctrl_active_threshold >= tx_flow_ctrl_deactive_threshold) {
+		tx_flow_ctrl_active_threshold = FC_ACTIVE_DEFAULT;
+		tx_flow_ctrl_deactive_threshold = FC_DEACTIVE_DEFAULT;
+	}
+
+	if (tx_flow_ctrl_pause_time < 0 ||
+	    tx_flow_ctrl_pause_time > FC_PAUSE_TIME_MAX)
+		tx_flow_ctrl_pause_time = FC_PAUSE_TIME_DEFAULT;
+
+	if (tx_flow_ctrl_pause_interval < 0 ||
+	    tx_flow_ctrl_pause_interval > FC_PAUSE_TIME_MAX)
+		tx_flow_ctrl_pause_interval = FC_PAUSE_INTERVAL_DEFAULT;
+
+	/* pause interval should not bigger than pause time,
+	 * but should not too smaller to avoid sending too many pause frame.
+	 */
+	if ((tx_flow_ctrl_pause_interval > tx_flow_ctrl_pause_time) ||
+	    (tx_flow_ctrl_pause_interval < (tx_flow_ctrl_pause_time >> 1)))
+		tx_flow_ctrl_pause_interval = tx_flow_ctrl_pause_time;
+}
+
+static void higmac_destroy_hw_desc_queue(struct higmac_netdev_local *priv)
+{
+	int i;
+
+	for (i = 0; i < QUEUE_NUMS + RSS_NUM_RXQS - 1; i++) {
+		if (priv->pool[i].desc) {
+			if (HAS_CAP_CCI(priv->hw_cap))
+				kfree(priv->pool[i].desc);
+			else
+				dma_free_coherent(priv->dev, priv->pool[i].size,
+						  priv->pool[i].desc,
+						  priv->pool[i].phys_addr);
+			priv->pool[i].desc = NULL;
+		}
+	}
+
+	kfree(priv->rx_fq.skb);
+	kfree(priv->tx_bq.skb);
+	priv->rx_fq.skb = NULL;
+	priv->tx_bq.skb = NULL;
+
+	if (priv->tso_supported) {
+		kfree(priv->tx_bq.sg_desc_offset);
+		priv->tx_bq.sg_desc_offset = NULL;
+	}
+
+	kfree(priv->tx_skb);
+	priv->tx_skb = NULL;
+
+	kfree(priv->rx_skb);
+	priv->rx_skb = NULL;
+}
+
+static int higmac_init_hw_desc_queue(struct higmac_netdev_local *priv)
+{
+	struct device *dev = priv->dev;
+	struct higmac_desc *virt_addr;
+	dma_addr_t phys_addr = 0;
+	int size, i;
+
+	priv->rx_fq.count = RX_DESC_NUM;
+	priv->rx_bq.count = RX_DESC_NUM;
+	priv->tx_bq.count = TX_DESC_NUM;
+	priv->tx_rq.count = TX_DESC_NUM;
+
+	for (i = 1; i < RSS_NUM_RXQS; i++)
+		priv->pool[3 + i].count = RX_DESC_NUM;
+
+	for (i = 0; i < (QUEUE_NUMS + RSS_NUM_RXQS - 1); i++) {
+		size = priv->pool[i].count * sizeof(struct higmac_desc);
+		if (HAS_CAP_CCI(priv->hw_cap)) {
+			virt_addr = kmalloc(size, GFP_KERNEL);
+			if (virt_addr)
+				phys_addr = virt_to_phys(virt_addr);
+		} else {
+			virt_addr = dma_alloc_coherent(dev, size, &phys_addr,
+						       GFP_KERNEL);
+		}
+		if (!virt_addr)
+			goto error_free_pool;
+
+		memset(virt_addr, 0, size);
+		priv->pool[i].size = size;
+		priv->pool[i].desc = virt_addr;
+		priv->pool[i].phys_addr = phys_addr;
+	}
+	priv->rx_fq.skb = kzalloc(priv->rx_fq.count
+				* sizeof(struct sk_buff *), GFP_KERNEL);
+	if (!priv->rx_fq.skb)
+		goto error_free_pool;
+
+	priv->rx_skb = kzalloc(priv->rx_fq.count
+			     * sizeof(struct sk_buff *), GFP_KERNEL);
+	if (!priv->rx_skb)
+		goto error_free_pool;
+
+	priv->tx_bq.skb = kzalloc(priv->tx_bq.count
+				* sizeof(struct sk_buff *), GFP_KERNEL);
+	if (!priv->tx_bq.skb)
+		goto error_free_pool;
+
+	priv->tx_skb = kzalloc(priv->tx_bq.count
+			     * sizeof(struct sk_buff *), GFP_KERNEL);
+	if (!priv->tx_skb)
+		goto error_free_pool;
+
+	if (priv->tso_supported) {
+		priv->tx_bq.sg_desc_offset = kzalloc(priv->tx_bq.count
+						   * sizeof(int), GFP_KERNEL);
+		if (!priv->tx_bq.sg_desc_offset)
+			goto error_free_pool;
+	}
+
+	higmac_hw_set_desc_addr(priv);
+	if (HAS_CAP_CCI(priv->hw_cap))
+		pr_info("higmac: ETH MAC supporte CCI.\n");
+
+	return 0;
+
+error_free_pool:
+	higmac_destroy_hw_desc_queue(priv);
+
+	return -ENOMEM;
+}
+
+void higmac_init_napi(struct higmac_netdev_local *priv)
+{
+	struct higmac_napi *q_napi;
+	int i;
+
+	for (i = 0; i < priv->num_rxqs; i++) {
+		q_napi = &priv->q_napi[i];
+		q_napi->rxq_id = i;
+		q_napi->ndev_priv = priv;
+		netif_napi_add(priv->netdev, &q_napi->napi, higmac_poll,
+			       NAPI_POLL_WEIGHT);
+	}
+}
+
+void higmac_destroy_napi(struct higmac_netdev_local *priv)
+{
+	struct higmac_napi *q_napi;
+	int i;
+
+	for (i = 0; i < priv->num_rxqs; i++) {
+		q_napi = &priv->q_napi[i];
+		netif_napi_del(&q_napi->napi);
+	}
+}
+
+int higmac_request_irqs(struct platform_device *pdev,
+			struct higmac_netdev_local *priv)
+{
+	struct device *dev = priv->dev;
+	int ret;
+	int i;
+
+	for (i = 0; i < priv->num_rxqs; i++) {
+		ret = platform_get_irq(pdev, i);
+		if (ret < 0) {
+			dev_err(dev, "No irq[%d] resource, ret=%d\n", i, ret);
+			return ret;
+		}
+		priv->irq[i] = ret;
+
+		ret = devm_request_irq(dev, priv->irq[i], higmac_interrupt,
+				       IRQF_SHARED, pdev->name,
+				       &priv->q_napi[i]);
+		if (ret) {
+			dev_err(dev, "devm_request_irq failed, ret=%d\n", ret);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int higmac_dev_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *node = dev->of_node;
+	struct net_device *ndev;
+	struct higmac_netdev_local *priv;
+	struct resource *res;
+	const char *mac_addr;
+	unsigned int hw_cap;
+	int ret;
+	int num_rxqs;
+
+	higmac_verify_flow_ctrl_args();
+
+	if (of_device_is_compatible(node, "hisilicon,higmac-v5"))
+		num_rxqs = RSS_NUM_RXQS;
+	else
+		num_rxqs = 1;
+
+	ndev = alloc_etherdev_mqs(sizeof(struct higmac_netdev_local), 1,
+				  num_rxqs);
+	if (!ndev)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ndev);
+	SET_NETDEV_DEV(ndev, dev);
+
+	priv = netdev_priv(ndev);
+	priv->dev = dev;
+	priv->netdev = ndev;
+	priv->num_rxqs = num_rxqs;
+
+	if (of_device_is_compatible(node, "hisilicon,higmac-v3"))
+		priv->hw_cap |= HW_CAP_CCI;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, MEM_GMAC_IOBASE);
+	priv->gmac_iobase = devm_ioremap_resource(dev, res);
+	if (IS_ERR(priv->gmac_iobase)) {
+		ret = PTR_ERR(priv->gmac_iobase);
+		goto out_free_netdev;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM,
+				    MEM_MACIF_IOBASE);
+	priv->macif_base = devm_ioremap_resource(dev, res);
+	if (IS_ERR(priv->macif_base)) {
+		ret = PTR_ERR(priv->macif_base);
+		goto out_free_netdev;
+	}
+
+	priv->port_rst = devm_reset_control_get(dev, HIGMAC_PORT_RST_NAME);
+	if (IS_ERR(priv->port_rst)) {
+		ret = PTR_ERR(priv->port_rst);
+		goto out_free_netdev;
+	}
+
+	priv->macif_rst = devm_reset_control_get(dev, HIGMAC_MACIF_RST_NAME);
+	if (IS_ERR(priv->macif_rst)) {
+		ret = PTR_ERR(priv->macif_rst);
+		goto out_free_netdev;
+	}
+
+	priv->phy_rst = devm_reset_control_get(dev, HIGMAC_PHY_RST_NAME);
+	if (IS_ERR(priv->phy_rst))
+		priv->phy_rst = NULL;
+
+	priv->clk = devm_clk_get(&pdev->dev, HIGMAC_MAC_CLK_NAME);
+	if (IS_ERR(priv->clk)) {
+		netdev_err(ndev, "failed to get clk\n");
+		ret = -ENODEV;
+		goto out_free_netdev;
+	}
+
+	ret = clk_prepare_enable(priv->clk);
+	if (ret < 0) {
+		netdev_err(ndev, "failed to enable clk %d\n", ret);
+		goto out_free_netdev;
+	}
+
+	priv->macif_clk = devm_clk_get(&pdev->dev, HIGMAC_MACIF_CLK_NAME);
+	if (IS_ERR(priv->macif_clk))
+		priv->macif_clk = NULL;
+
+	if (priv->macif_clk) {
+		ret = clk_prepare_enable(priv->macif_clk);
+		if (ret < 0) {
+			netdev_err(ndev, "failed enable macif_clk %d\n", ret);
+			goto out_clk_disable;
+		}
+	}
+
+	higmac_mac_core_reset(priv);
+
+	/* phy reset, should be early than "of_mdiobus_register".
+	 * becausue "of_mdiobus_register" will read PHY register by MDIO.
+	 */
+	higmac_hw_phy_reset(priv);
+
+	higmac_of_get_param(priv, node);
+
+	ret = of_get_phy_mode(node);
+	if (ret < 0) {
+		netdev_err(ndev, "not find phy-mode\n");
+		goto out_macif_clk_disable;
+	}
+	priv->phy_mode = ret;
+
+	priv->phy_node = of_parse_phandle(node, "phy-handle", 0);
+	if (!priv->phy_node) {
+		netdev_err(ndev, "not find phy-handle\n");
+		ret = -EINVAL;
+		goto out_macif_clk_disable;
+	}
+
+	mac_addr = of_get_mac_address(node);
+	if (mac_addr)
+		ether_addr_copy(ndev->dev_addr, mac_addr);
+	if (!is_valid_ether_addr(ndev->dev_addr)) {
+		eth_hw_addr_random(ndev);
+		netdev_warn(ndev, "using random MAC address %pM\n",
+			    ndev->dev_addr);
+	}
+
+	higmac_hw_set_mac_addr(ndev);
+
+	hw_cap = readl(priv->gmac_iobase + CRF_MIN_PACKET);
+	priv->tso_supported = HAS_TSO_CAP(hw_cap);
+	priv->has_rxhash_cap = HAS_RXHASH_CAP(hw_cap);
+	priv->has_rss_cap = HAS_RSS_CAP(hw_cap);
+
+	higmac_set_rss_cap(priv);
+	higmac_get_rss_key(priv);
+	if (priv->has_rss_cap) {
+		priv->rss_info.ind_tbl_size = RSS_INDIRECTION_TABLE_SIZE;
+		higmac_get_rss(priv);
+	}
+
+	if (priv->has_rxhash_cap) {
+		priv->rss_info.hash_cfg = DEF_HASH_CFG;
+		higmac_config_hash_policy(priv);
+	}
+
+	/* init hw controller */
+	higmac_hw_init(priv);
+
+	/* TODO: phy fix here?? other way ??? */
+	phy_register_fixups();
+
+	priv->phy = of_phy_connect(ndev, priv->phy_node,
+				   &higmac_adjust_link, 0, priv->phy_mode);
+	if (!priv->phy) {
+		ret = -ENODEV;
+		goto out_phy_node;
+	}
+
+	/* If the phy_id is mostly Fs, there is no device there */
+	if ((priv->phy->phy_id & 0x1fffffff) == 0x1fffffff ||
+	    priv->phy->phy_id == 0) {
+		pr_info("phy %d not found\n", priv->phy->mdio.addr);
+		ret = -ENODEV;
+		goto out_phy_disconnect;
+	}
+
+	pr_info("attached PHY %d to driver %s, PHY_ID=0x%x\n",
+		priv->phy->mdio.addr, priv->phy->drv->name, priv->phy->phy_id);
+
+	/* Stop Advertising 1000BASE Capability if interface is not RGMII */
+	if ((priv->phy_mode == PHY_INTERFACE_MODE_MII) ||
+	    (priv->phy_mode == PHY_INTERFACE_MODE_RMII)) {
+		priv->phy->advertising &= ~(SUPPORTED_1000baseT_Half |
+					    SUPPORTED_1000baseT_Full);
+
+		/* Internal FE phy's reg BMSR bit8 is wrong, make the kernel
+		 * believe it has the 1000base Capability, so fix it here
+		 */
+		if (priv->phy->phy_id == HISILICON_PHY_ID_FESTAV200)
+			priv->phy->supported &= ~(ADVERTISED_1000baseT_Full |
+						  ADVERTISED_1000baseT_Half);
+	}
+
+	higmac_set_flow_ctrl_args(priv);
+	higmac_set_flow_ctrl_params(priv);
+	priv->phy->supported |= SUPPORTED_Pause;
+	if (priv->flow_ctrl)
+		priv->phy->advertising |= SUPPORTED_Pause;
+
+	if (priv->autoeee)
+		init_autoeee(priv);
+
+	ret = higmac_request_irqs(pdev, priv);
+	if (ret)
+		goto out_phy_disconnect;
+
+	higmac_init_napi(priv);
+	spin_lock_init(&priv->rxlock);
+	spin_lock_init(&priv->txlock);
+	spin_lock_init(&priv->pmtlock);
+
+	/* init netdevice */
+	ndev->irq = priv->irq[0];
+	ndev->watchdog_timeo = 3 * HZ;
+	ndev->netdev_ops = &hieth_netdev_ops;
+	ndev->ethtool_ops = &hieth_ethtools_ops;
+
+	if (priv->has_rxhash_cap)
+		ndev->hw_features |= NETIF_F_RXHASH;
+	if (priv->has_rss_cap)
+		ndev->hw_features |= NETIF_F_NTUPLE;
+	if (priv->tso_supported) {
+		ndev->hw_features |= NETIF_F_SG |
+			NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
+			NETIF_F_TSO | NETIF_F_TSO6 | NETIF_F_UFO;
+	}
+#if defined(CONFIG_HIGMAC_RXCSUM)
+	ndev->hw_features |= NETIF_F_RXCSUM;
+	higmac_enable_rxcsum_drop(priv, true);
+#endif
+
+	ndev->features |= ndev->hw_features;
+	ndev->features |= NETIF_F_HIGHDMA | NETIF_F_GSO;
+	ndev->vlan_features |= ndev->features;
+
+	init_timer(&priv->monitor);
+	priv->monitor.function = higmac_monitor_func;
+	priv->monitor.data = (unsigned long)ndev;
+	priv->monitor.expires = jiffies + HIGMAC_MONITOR_TIMER;
+
+	device_set_wakeup_capable(priv->dev, 1);
+	/* TODO: when we can let phy powerdown?
+	 * In some mode, we don't want phy powerdown,
+	 * so I set wakeup enable all the time
+	 */
+	device_set_wakeup_enable(priv->dev, 1);
+
+	priv->wol_enable = false;
+
+	priv->msg_enable = netif_msg_init(debug, DEFAULT_MSG_ENABLE);
+
+	/* init hw desc queue */
+	ret = higmac_init_hw_desc_queue(priv);
+	if (ret)
+		goto _error_hw_desc_queue;
+
+	if (priv->tso_supported) {
+		ret = higmac_init_sg_desc_queue(priv);
+		if (ret)
+			goto _error_sg_desc_queue;
+	}
+
+	/* register netdevice */
+	ret = register_netdev(priv->netdev);
+	if (ret) {
+		pr_err("register_ndev failed!");
+		goto _error_sg_desc_queue;
+	}
+
+	/* reset queue here to make BQL only reset once.
+	 * if we put netdev_reset_queue() in higmac_net_open(),
+	 * the BQL will be reset when ifconfig eth0 down and up,
+	 * but the tx ring is not cleared before.
+	 * As a result, the NAPI poll will call netdev_completed_queue()
+	 * and BQL throw a bug.
+	 */
+	netdev_reset_queue(ndev);
+
+	clk_disable_unprepare(priv->clk);
+	if (priv->macif_clk)
+		clk_disable_unprepare(priv->macif_clk);
+
+	pr_info("ETH: %s, phy_addr=%d\n",
+		phy_modes(priv->phy_mode), priv->phy->mdio.addr);
+
+	return ret;
+
+_error_sg_desc_queue:
+	if (priv->tso_supported)
+		higmac_destroy_sg_desc_queue(priv);
+
+_error_hw_desc_queue:
+	higmac_destroy_hw_desc_queue(priv);
+	higmac_destroy_napi(priv);
+out_phy_disconnect:
+	phy_disconnect(priv->phy);
+out_phy_node:
+	of_node_put(priv->phy_node);
+out_macif_clk_disable:
+	if (priv->macif_clk)
+		clk_disable_unprepare(priv->macif_clk);
+out_clk_disable:
+	clk_disable_unprepare(priv->clk);
+out_free_netdev:
+	free_netdev(ndev);
+
+	return ret;
+}
+
+static int higmac_dev_remove(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+
+	/* TODO: stop the gmac and free all resource */
+	del_timer_sync(&priv->monitor);
+	higmac_destroy_napi(priv);
+
+	unregister_netdev(ndev);
+
+	higmac_reclaim_rx_tx_resource(priv);
+
+	if (priv->tso_supported)
+		higmac_destroy_sg_desc_queue(priv);
+	higmac_destroy_hw_desc_queue(priv);
+
+	phy_disconnect(priv->phy);
+	of_node_put(priv->phy_node);
+
+	free_netdev(ndev);
+
+	return 0;
+}
+
+#include "pm.c"
+#ifdef CONFIG_PM
+
+static void higmac_disable_irq(struct higmac_netdev_local *priv)
+{
+	int i;
+
+	for (i = 0; i < priv->num_rxqs; i++)
+		disable_irq(priv->irq[i]);
+}
+
+static void higmac_enable_irq(struct higmac_netdev_local *priv)
+{
+	int i;
+
+	for (i = 0; i < priv->num_rxqs; i++)
+		enable_irq(priv->irq[i]);
+}
+
+int higmac_dev_suspend(struct platform_device *pdev, pm_message_t state)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+
+	higmac_disable_irq(priv);
+	/* If support Wake on LAN, we should not disconnect phy
+	 * because it will call phy_suspend to power down phy.
+	 */
+	if (!priv->wol_enable)
+		phy_disconnect(priv->phy);
+	del_timer_sync(&priv->monitor);
+	/* If suspend when netif is not up, the napi_disable will run into
+	 * dead loop and dpm_drv_timeout will give warning.
+	 */
+	if (netif_running(ndev))
+		higmac_disable_napi(priv);
+	netif_device_detach(ndev);
+
+	netif_carrier_off(ndev);
+
+	/* If netdev is down, MAC clock is disabled.
+	 * So if we want to reclaim MAC rx and tx resource,
+	 * we must first enable MAC clock and then disable it.
+	 */
+	if (!(ndev->flags & IFF_UP))
+		clk_prepare_enable(priv->clk);
+
+	higmac_reclaim_rx_tx_resource(priv);
+
+	if (!(ndev->flags & IFF_UP))
+		clk_disable_unprepare(priv->clk);
+
+	pmt_enter(priv);
+
+	if (!priv->wol_enable) {	/* if no WOL, then poweroff */
+		/* pr_info("power off gmac.\n"); */
+		/* no need to call genphy_resume() in resume,
+		 * because we reset everything
+		 */
+		genphy_suspend(priv->phy);	/* power down phy */
+		msleep(20);
+		higmac_hw_all_clk_disable(priv);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(higmac_dev_suspend);
+
+int higmac_dev_resume(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+	int ret = 0;
+
+	/* If we support Wake on LAN, we doesn't call clk_disable.
+	 * But when we resume, the uboot may off mac clock and reset phy
+	 * by re-write the mac CRG register.
+	 * So we first call clk_disable, and then clk_enable.
+	 */
+	if (priv->wol_enable)
+		higmac_hw_all_clk_disable(priv);
+
+	higmac_hw_all_clk_enable(priv);
+	/* internal FE_PHY: enable clk and reset  */
+	higmac_hw_phy_reset(priv);
+
+	/* If netdev is down, MAC clock is disabled.
+	 * So if we want to restart MAC and re-initialize it,
+	 * we must first enable MAC clock and then disable it.
+	 */
+	if (!(ndev->flags & IFF_UP))
+		clk_prepare_enable(priv->clk);
+
+	/* power on gmac */
+	higmac_restart(priv);
+
+	/* If support WoL, we didn't disconnect phy.
+	 * But when we resume, we reset PHY, so we want to
+	 * call phy_connect to make phy_fixup excuted.
+	 * This is important for internal PHY fix.
+	 */
+	if (priv->wol_enable)
+		phy_disconnect(priv->phy);
+
+	ret = phy_connect_direct(ndev, priv->phy, higmac_adjust_link,
+				 priv->phy_mode);
+	if (ret)
+		return ret;
+
+	/* If we suspend and resume when net device is down,
+	 * some operations are unnecessary.
+	 */
+	if (ndev->flags & IFF_UP) {
+		priv->monitor.expires = jiffies + HIGMAC_MONITOR_TIMER;
+		mod_timer(&priv->monitor, priv->monitor.expires);
+		priv->old_link = 0;
+		priv->old_speed = SPEED_UNKNOWN;
+		priv->old_duplex = DUPLEX_UNKNOWN;
+	}
+	if (netif_running(ndev))
+		higmac_enable_napi(priv);
+	netif_device_attach(ndev);
+	if (ndev->flags & IFF_UP)
+		phy_start(priv->phy);
+	higmac_enable_irq(priv);
+
+	pmt_exit(priv);
+
+	if (!(ndev->flags & IFF_UP))
+		clk_disable_unprepare(priv->clk);
+
+	return 0;
+}
+EXPORT_SYMBOL(higmac_dev_resume);
+#else
+#define higmac_dev_suspend	NULL
+#define higmac_dev_resume	NULL
+#endif
+
+static const struct of_device_id higmac_of_match[] = {
+	{.compatible = "hisilicon,higmac",},
+	{.compatible = "hisilicon,higmac-v1",},
+	{.compatible = "hisilicon,higmac-v2",},
+	{.compatible = "hisilicon,higmac-v3",},
+	{.compatible = "hisilicon,higmac-v4",},
+	{.compatible = "hisilicon,higmac-v5",},
+	{ },
+};
+
+MODULE_DEVICE_TABLE(of, higmac_of_match);
+
+static struct platform_driver higmac_dev_driver = {
+	.probe = higmac_dev_probe,
+	.remove = higmac_dev_remove,
+	.suspend = higmac_dev_suspend,
+	.resume = higmac_dev_resume,
+	.driver = {
+		   .owner = THIS_MODULE,
+		   .name = HIGMAC_DRIVER_NAME,
+		   .of_match_table = higmac_of_match,
+		   },
+};
+
+#include "proc-dev.c"
+
+static int __init higmac_init(void)
+{
+	int ret = 0;
+
+	ret = platform_driver_register(&higmac_dev_driver);
+	if (ret)
+		return ret;
+
+	higmac_proc_create();
+
+	return ret;
+}
+
+static void __exit higmac_exit(void)
+{
+	platform_driver_unregister(&higmac_dev_driver);
+
+	higmac_proc_destroy();
+}
+
+module_init(higmac_init);
+module_exit(higmac_exit);
+
+MODULE_AUTHOR("ZMJUN");
+MODULE_DESCRIPTION("Hisilicon double GMAC driver, base on driver higmacv200 by CHH");
+MODULE_LICENSE("GPL v2");
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/higmac.h linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/higmac.h
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/higmac.h	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/higmac.h	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,613 @@
+#ifndef __HIGMAC_H__
+#define __HIGMAC_H__
+
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/netdevice.h>
+#include <linux/list.h>
+#include <linux/phy.h>
+#include <linux/io.h>
+#include <linux/interrupt.h>
+
+#define STATION_ADDR_LOW		0x0000
+#define STATION_ADDR_HIGH		0x0004
+#define MAC_DUPLEX_HALF_CTRL		0x0008
+
+#define PORT_MODE			0x0040
+
+#define PORT_EN				0x0044
+#define BITS_TX_EN			BIT(2)
+#define BITS_RX_EN			BIT(1)
+
+#define FC_TX_TIMER			0x001C
+
+#define PAUSE_THR			0x0038
+
+#define PAUSE_EN			0x0048
+#define BIT_RX_FDFC			BIT(0)
+#define BIT_TX_FDFC			BIT(1)
+
+#define RX_PAUSE_EN			0x02A4
+#define BIT_RX_FQ_PAUSE_EN		BIT(0)
+#define BIT_RX_BQ_PAUSE_EN		BIT(1)
+
+#define CRF_TX_PAUSE			0x0340
+
+#define BITS_Q_PAUSE_TH_OFFSET		16
+#define BITS_Q_PAUSE_TH_MASK		0xFFFF
+
+#define REC_FILT_CONTROL		0x0064
+#define BIT_CRC_ERR_PASS		BIT(5)
+#define BIT_PAUSE_FRM_PASS		BIT(4)
+#define BIT_VLAN_DROP_EN		BIT(3)
+#define BIT_BC_DROP_EN			BIT(2)
+#define BIT_MC_MATCH_EN			BIT(1)
+#define BIT_UC_MATCH_EN			BIT(0)
+
+#define	PORT_MC_ADDR_LOW		0x0068
+#define	PORT_MC_ADDR_HIGH		0x006C
+
+#define MODE_CHANGE_EN			0x01b4
+#define BIT_MODE_CHANGE_EN		BIT(0)
+
+#define COL_SLOT_TIME			0x01c0
+
+#define CRF_MIN_PACKET			0x0210
+#define BIT_OFFSET_TX_MIN_LEN		8
+#define BIT_MASK_TX_MIN_LEN		GENMASK(13, 8)
+
+#define CONTROL_WORD			0x0214
+#define CONTROL_WORD_CONFIG		0x640
+
+#define TSO_COE_CTRL			0x02e8
+#define BIT_COE_IPHDR_DROP		BIT(4)
+#define BIT_COE_PAYLOAD_DROP		BIT(5)
+#define BIT_COE_IPV6_UDP_ZERO_DROP	BIT(6)
+#define COE_ERR_DROP			(BIT_COE_IPHDR_DROP | \
+					BIT_COE_PAYLOAD_DROP | \
+					BIT_COE_IPV6_UDP_ZERO_DROP)
+
+#define RX_FQ_START_ADDR		0x0500
+#define RX_FQ_DEPTH			0x0504
+#define RX_FQ_WR_ADDR			0x0508
+#define BITS_RX_FQ_WR_ADDR		MK_BITS(0, 21)
+#define RX_FQ_RD_ADDR			0x050c
+#define BITS_RX_FQ_RD_ADDR		MK_BITS(0, 21)
+#define RX_FQ_VLDDESC_CNT		0x0510
+#define BITS_RX_FQ_VLDDESC_CNT		MK_BITS(0, 16)
+#define RX_FQ_ALEMPTY_TH		0x0514
+#define BITS_RX_FQ_ALEMPTY_TH		MK_BITS(0, 16)
+#define RX_FQ_REG_EN			0x0518
+#define BITS_RX_FQ_START_ADDR_EN	BIT(2)
+#define BITS_RX_FQ_DEPTH_EN		BIT(1)
+#define BITS_RX_FQ_RD_ADDR_EN		MK_BITS(0, 1)
+#define RX_FQ_ALFULL_TH			0x051c
+#define BITS_RX_FQ_ALFULL_TH		MK_BITS(0, 16)
+
+#define RX_BQ_START_ADDR		0x0520
+#define RX_BQ_DEPTH			0x0524
+#define RX_BQ_WR_ADDR			0x0528
+#define RX_BQ_RD_ADDR			0x052c
+#define RX_BQ_FREE_DESC_CNT		0x0530
+#define BITS_RX_BQ_FREE_DESC_CNT	MK_BITS(0, 16)
+#define RX_BQ_ALEMPTY_TH		0x0534
+#define BITS_RX_BQ_ALEMPTY_TH		MK_BITS(0, 16)
+#define RX_BQ_REG_EN			0x0538
+#define BITS_RX_BQ_START_ADDR_EN	BIT(2)
+#define BITS_RX_BQ_DEPTH_EN		BIT(1)
+#define BITS_RX_BQ_WR_ADDR_EN		MK_BITS(0, 1)
+#define RX_BQ_ALFULL_TH			0x053c
+#define BITS_RX_BQ_ALFULL_TH		MK_BITS(0, 16)
+
+#define TX_BQ_START_ADDR		0x0580
+#define TX_BQ_DEPTH			0x0584
+#define TX_BQ_WR_ADDR			0x0588
+#define BITS_TX_BQ_WR_ADDR		MK_BITS(0, 21)
+#define TX_BQ_RD_ADDR			0x058c
+#define BITS_TX_BQ_RD_ADDR		MK_BITS(0, 21)
+#define TX_BQ_VLDDESC_CNT		0x0590
+#define BITS_TX_BQ_VLDDESC_CNT		MK_BITS(0, 16)
+#define TX_BQ_ALEMPTY_TH		0x0594
+#define BITS_TX_BQ_ALEMPTY_TH		MK_BITS(0, 16)
+#define TX_BQ_REG_EN			0x0598
+#define BITS_TX_BQ_START_ADDR_EN	BIT(2)
+#define BITS_TX_BQ_DEPTH_EN		BIT(1)
+#define BITS_TX_BQ_RD_ADDR_EN		MK_BITS(0, 1)
+#define TX_BQ_ALFULL_TH			0x059c
+#define BITS_TX_BQ_ALFULL_TH		MK_BITS(0, 16)
+
+#define TX_RQ_START_ADDR		0x05a0
+#define TX_RQ_DEPTH			0x05a4
+#define TX_RQ_WR_ADDR			0x05a8
+#define BITS_TX_RQ_WR_ADDR		MK_BITS(0, 21)
+#define TX_RQ_RD_ADDR			0x05ac
+#define BITS_TX_RQ_RD_ADDR		MK_BITS(0, 21)
+#define TX_RQ_FREE_DESC_CNT		0x05b0
+#define BITS_TX_RQ_FREE_DESC_CNT	MK_BITS(0, 16)
+#define TX_RQ_ALEMPTY_TH		0x05b4
+#define BITS_TX_RQ_ALEMPTY_TH		MK_BITS(0, 16)
+#define TX_RQ_REG_EN			0x05b8
+#define BITS_TX_RQ_START_ADDR_EN	BIT(2)
+#define BITS_TX_RQ_DEPTH_EN		BIT(1)
+#define BITS_TX_RQ_WR_ADDR_EN		MK_BITS(0, 1)
+#define TX_RQ_ALFULL_TH			0x05bc
+#define BITS_TX_RQ_ALFULL_TH		MK_BITS(0, 16)
+
+#define RAW_PMU_INT			0x05c0
+#define ENA_PMU_INT			0x05c4
+
+#define DESC_WR_RD_ENA					0x05CC
+
+#define IN_QUEUE_TH					0x05d8
+#define BITS_OFFSET_TX_RQ_IN_TH				16
+
+#define RX_BQ_IN_TIMEOUT_TH				0x05E0
+
+#define TX_RQ_IN_TIMEOUT_TH				0x05e4
+
+#define STOP_CMD			0x05e8
+#define BITS_TX_STOP_EN			BIT(1)
+#define BITS_RX_STOP_EN			BIT(0)
+#define	STOP_RX_TX			(BITS_TX_STOP_EN | BITS_RX_STOP_EN)
+
+#define HW_CAP_EN			0x0c00
+#define BIT_RSS_CAP			BIT(0)
+#define BIT_RXHASH_CAP			BIT(1)
+#define RSS_HASH_KEY			0x0c04
+#define RSS_HASH_CONFIG			0x0c08
+#define TCPV4_L3_HASH_EN		BIT(0)
+#define TCPV4_L4_HASH_EN		BIT(1)
+#define TCPV4_VLAN_HASH_EN		BIT(2)
+#define UDPV4_L3_HASH_EN		BIT(4)
+#define UDPV4_L4_HASH_EN		BIT(5)
+#define UDPV4_VLAN_HASH_EN		BIT(6)
+#define IPV4_L3_HASH_EN			BIT(8)
+#define IPV4_VLAN_HASH_EN		BIT(9)
+#define TCPV6_L3_HASH_EN		BIT(12)
+#define TCPV6_L4_HASH_EN		BIT(13)
+#define TCPV6_VLAN_HASH_EN		BIT(14)
+#define UDPV6_L3_HASH_EN		BIT(16)
+#define UDPV6_L4_HASH_EN		BIT(17)
+#define UDPV6_VLAN_HASH_EN		BIT(18)
+#define IPV6_L3_HASH_EN			BIT(20)
+#define IPV6_VLAN_HASH_EN		BIT(21)
+#define DEF_HASH_CFG			0x377377
+
+#define RSS_IND_TBL			0x0c0c
+#define BIT_IND_TBL_READY		BIT(13)
+#define BIT_IND_TLB_WR			BIT(12)
+#define RSS_RAW_PMU_INT			0x0c10
+#define RSS_QUEUE1_START_ADDR		0x0c20
+#define RX_BQ_START_ADDR_QUEUE(i)	(RSS_QUEUE1_START_ADDR + \
+					((i) - 1) * 0x10)
+#define RSS_QUEUE1_DEPTH		0x0c24
+#define RX_BQ_WR_ADDR_QUEUE1		0x0c28
+#define RX_BQ_RD_ADDR_QUEUE1		0x0c2c
+#define RSS_QUEUE1_ENA_INT		0x0c90
+#define RSS_ENA_INT_QUEUE(i)		(RSS_QUEUE1_ENA_INT + ((i) - 1) * 0x4)
+#define RX_BQ_DEPTH_QUEUE(i)		(RSS_QUEUE1_DEPTH + ((i) - 1) * 0x10)
+#define RX_BQ_WR_ADDR_QUEUE(i)		((i) ? (RX_BQ_WR_ADDR_QUEUE1 + \
+					((i) - 1) * 0x10) : RX_BQ_WR_ADDR)
+#define RX_BQ_RD_ADDR_QUEUE(i)		((i) ? (RX_BQ_RD_ADDR_QUEUE1 + \
+					((i) - 1) * 0x10) : RX_BQ_RD_ADDR)
+
+#define DEF_INT_MASK_QUEUE(i)		(0x3 << (2 * ((i) - 1)))
+
+/* AXI burst and outstanding config */
+#define BURST_OUTSTANDING_REG		0x3014
+#define BURST4_OUTSTANDING1		0x81ff
+#define BURST_OUTSTANDING_OFFSET	16
+
+#define GMAC_SPEED_1000			0x05
+#define GMAC_SPEED_100			0x01
+#define GMAC_SPEED_10			0x00
+
+enum higmac_tx_err {
+	ERR_NONE = 0,
+	ERR_DESC_CFG = (1 << 0),
+	ERR_DATA_LEN = (1 << 1),
+	ERR_DESC_NFRAG_NUM = (1 << 2),
+	ERR_DESC_IP_HDR_LEN = (1 << 3),
+	ERR_DESC_PROT_HDR_LEN = (1 << 4),
+	ERR_DESC_MTU = (1 << 5),
+	ERR_LINK_SGPKT_LEN = (1 << 8),
+	ERR_LINK_TSOPKT_LINEAR = (1 << 9),
+	ERR_LINK_NFRAG_LEN = (1 << 10),
+	ERR_LINK_TOTAL_LEN = (1 << 11),
+	ERR_HDR_TCP_BCMC = (1 << 12),
+	ERR_HDR_UDP_BC = (1 << 13),
+	ERR_HDR_VLAN_IP_TYPE = (1 << 14),
+	ERR_HDR_IP_TYPE = (1 << 15),
+	ERR_HDR_IP_VERSION = (1 << 16),
+	ERR_HDR_IP_HDR_LEN = (1 << 17),
+	ERR_HDR_IP_TOTAL_LEN = (1 << 18),
+	ERR_HDR_IPV6_TTL_PROT = (1 << 19),
+	ERR_HDR_IPV4_OFFSET = (1 << 20),
+	ERR_HDR_IPV4_TTL_PROT = (1 << 21),
+	ERR_HDR_UDP_LEN = (1 << 22),
+	ERR_HDR_TCP_LEN = (1 << 23),
+	ERR_DESC = (ERR_DESC_CFG | ERR_DATA_LEN |
+			ERR_DESC_NFRAG_NUM | ERR_DESC_IP_HDR_LEN |
+			ERR_DESC_PROT_HDR_LEN | ERR_DESC_MTU),
+	ERR_LINK = (ERR_LINK_SGPKT_LEN | ERR_LINK_TSOPKT_LINEAR |
+			ERR_LINK_NFRAG_LEN | ERR_LINK_TOTAL_LEN),
+	ERR_HDR = (ERR_HDR_TCP_BCMC | ERR_HDR_UDP_BC |
+			ERR_HDR_VLAN_IP_TYPE | ERR_HDR_IP_TYPE |
+			ERR_HDR_IP_VERSION | ERR_HDR_IP_HDR_LEN |
+			ERR_HDR_IP_TOTAL_LEN | ERR_HDR_IPV6_TTL_PROT |
+			ERR_HDR_IPV4_OFFSET | ERR_HDR_IPV4_TTL_PROT |
+			ERR_HDR_UDP_LEN | ERR_HDR_TCP_LEN),
+	ERR_ALL = (ERR_DESC | ERR_LINK | ERR_HDR),
+};
+
+#define HIGMAC_DRIVER_NAME	"hi_gmac_v200"
+
+#define HIGMAC_MAC_CLK_NAME	"higmac_clk"
+#define HIGMAC_MACIF_CLK_NAME	"macif_clk"
+
+#define HIGMAC_PORT_RST_NAME	"port_reset"
+#define HIGMAC_MACIF_RST_NAME	"macif_reset"
+#define HIGMAC_PHY_RST_NAME	"phy_reset"
+
+#define HIGMAC_TSO_DEBUG
+
+#include "tso.h"
+
+#if defined(CONFIG_ARCH_HI3519) || defined(CONFIG_ARCH_HI3519V101) || \
+	defined(CONFIG_ARCH_HI3516AV200)
+#ifdef readl
+#undef readl
+#undef readl_relaxed
+#undef writel
+#undef writel_relaxed
+#define readl		hi_readl
+#define readl_relaxed	hi_readl_relaxed
+#define writel		hi_writel
+#define writel_relaxed	hi_writel_relaxed
+#endif /* readl */
+#endif /* defined(CONFIG_ARCH_HI3519) || defined(CONFIG_HI3519V101) */
+
+#define HIGMAC_IOSIZE			(0x1000)
+#define HIGMAC_OFFSET			(HIGMAC_IOSIZE)
+
+#define RX_BQ_IN_INT			BIT(17)
+#define TX_RQ_IN_INT			BIT(19)
+#define RX_BQ_IN_TIMEOUT_INT		BIT(28)
+#define TX_RQ_IN_TIMEOUT_INT		BIT(29)
+
+#define DEF_INT_MASK			(RX_BQ_IN_INT | RX_BQ_IN_TIMEOUT_INT | \
+					TX_RQ_IN_INT | TX_RQ_IN_TIMEOUT_INT)
+
+/* write or read descriptor need memory barrier */
+#define HIGMAC_SYNC_BARRIER() do { isb(); smp_mb(); } while (0)
+
+#define HISILICON_PHY_ID_FESTAV200	(0x20669823)
+#define PHY_ID_KSZ8051MNL               (0x00221550)
+#define PHY_ID_KSZ8081RNB               (0x00221560)
+#define DEFAULT_PHY_MASK                (0xfffffff0)
+#define REALTEK_PHY_ID_8211E		(0x001cc915)
+#define REALTEK_PHY_MASK		(0x001fffff)
+
+enum {
+	GMAC_PORT0,
+	GMAC_PORT1,
+	GMAC_MAX_PORT,
+};
+
+enum {
+	MEM_GMAC_IOBASE,
+	MEM_MACIF_IOBASE,
+	MEM_FWD_IOBASE,
+	MEM_CTRL_IOBASE,
+};
+
+#define HIGMAC_LINKED		BIT(0)
+#define HIGMAC_DUP_FULL		BIT(1)
+#define HIGMAC_SPD_10M		BIT(2)
+#define HIGMAC_SPD_100M		BIT(3)
+#define HIGMAC_SPD_1000M	BIT(4)
+/* Flow Control defines */
+#define FLOW_OFF        0
+#define FLOW_RX         1
+#define FLOW_TX         2
+#define FLOW_AUTO       (FLOW_TX | FLOW_RX)
+
+#define FC_ACTIVE_MIN		1
+#define FC_ACTIVE_DEFAULT	16
+#define FC_ACTIVE_MAX		127
+#define FC_DEACTIVE_MIN		1
+#define FC_DEACTIVE_DEFAULT	32
+#define FC_DEACTIVE_MAX		127
+
+#define FC_PAUSE_TIME_DEFAULT		0xFFFF
+#define FC_PAUSE_INTERVAL_DEFAULT	0xFFFF
+#define FC_PAUSE_TIME_MAX		0xFFFF
+
+#define RX_BQ_INT_THRESHOLD	0x40	/* TODO: */
+#define TX_RQ_INT_THRESHOLD	0x20	/* TODO: */
+
+#define HIGMAC_MONITOR_TIMER	(msecs_to_jiffies(200))
+
+#define HIETH_MAX_FRAME_SIZE	(1600 + 128)
+#define SKB_SIZE		(HIETH_MAX_FRAME_SIZE)
+
+#define DESC_VLD_FREE		0
+#define DESC_VLD_BUSY		1
+
+#define DESC_FL_FIRST		2
+#define DESC_FL_MID		0
+#define DESC_FL_LAST		1
+#define DESC_FL_FULL		3
+
+#if defined(CONFIG_HIGMAC_DESC_4WORD)
+#define DESC_WORD_SHIFT		2
+#else
+#define DESC_WORD_SHIFT		3
+#endif
+#define DESC_BYTE_SHIFT		(DESC_WORD_SHIFT + 2)
+#define DESC_WORD_CNT		(1 << DESC_WORD_SHIFT)
+#define DESC_SIZE		(1 << DESC_BYTE_SHIFT)
+
+#define RX_DESC_NUM			1024
+#define TX_DESC_NUM			1024
+
+/* DMA descriptor ring helpers */
+#define dma_ring_incr(n, s)		(((n) + 1) & ((s) - 1))
+#define dma_cnt(n)			((n) >> DESC_BYTE_SHIFT)
+#define dma_byte(n)			((n) << DESC_BYTE_SHIFT)
+
+#define RSS_HASH_KEY_SIZE		4
+#define RSS_INDIRECTION_TABLE_SIZE	128
+#define RSS_NUM_RXQS		4
+
+#define HW_CAP_TSO			BIT(0)
+#define HW_CAP_RXCSUM			BIT(1)
+#define HW_CAP_CCI			BIT(2)
+#define HAS_CAP_TSO(hw_cap)		((hw_cap) & HW_CAP_TSO)
+#define HAS_CAP_RXCSUM(hw_cap)		((hw_cap) & HW_CAP_RXCSUM)
+#define HAS_CAP_CCI(hw_cap)		((hw_cap) & HW_CAP_CCI)
+
+#if defined(CONFIG_HIGMAC_DESC_4WORD)
+struct higmac_desc {
+	unsigned int data_buff_addr;
+
+	unsigned int buffer_len:11;
+#if defined(CONFIG_HIGMAC_RXCSUM)
+	unsigned int reserve2:1;
+	unsigned int payload_csum_err:1;
+	unsigned int header_csum_err:1;
+	unsigned int payload_csum_done:1;
+	unsigned int header_csum_done:1;
+#else
+	unsigned int reserve2:5;
+#endif
+	unsigned int data_len:11;
+	unsigned int reserve1:2;
+	unsigned int fl:2;
+	unsigned int descvid:1;
+
+	unsigned int rxhash;
+	unsigned int reserve3:8;
+	unsigned int l3_hash:1;
+	unsigned int has_hash:1;
+	unsigned int skb_id:14;
+	unsigned int reserve31:8;
+};
+
+struct higmac_tso_desc {
+	unsigned int data_buff_addr;
+	union {
+		struct {
+			unsigned int prot_hdr_len:4;
+			unsigned int ip_hdr_len:4;
+			unsigned int prot_type:1;
+			unsigned int ip_ver:1;
+			unsigned int vlan_flag:1;
+			unsigned int nfrags_num:5;
+			unsigned int data_len:11;
+			unsigned int reservel:1;
+			unsigned int tso_flag:1;
+			unsigned int coe_flag:1;
+			unsigned int sg_flag:1;
+			unsigned int hw_own:1;
+		} tx;
+		unsigned int val;
+	} desc1;
+	unsigned int reserve_desc2;
+	unsigned int tx_err;
+};
+#else
+struct higmac_desc {
+	unsigned int data_buff_addr;
+
+	unsigned int buffer_len:11;
+#if defined(CONFIG_HIGMAC_RXCSUM)
+	unsigned int reserve2:1;
+	unsigned int payload_csum_err:1;
+	unsigned int header_csum_err:1;
+	unsigned int payload_csum_done:1;
+#else
+	unsigned int reserve2:5;
+#endif
+	unsigned int data_len:11;
+	unsigned int reserve1:2;
+	unsigned int fl:2;
+	unsigned int descvid:1;
+
+	unsigned int rxhash;
+	unsigned int reserve3:8;
+	unsigned int l3_hash:1;
+	unsigned int has_hash:1;
+	unsigned int skb_id:14;
+	unsigned int reserve31:8;
+
+	unsigned int reserve4;
+	unsigned int reserve5;
+	unsigned int reserve6;
+	unsigned int reserve7;
+};
+
+struct higmac_tso_desc {
+	unsigned int data_buff_addr;
+	union {
+		struct {
+			unsigned int prot_hdr_len:4;
+			unsigned int ip_hdr_len:4;
+			unsigned int prot_type:1;
+			unsigned int ip_ver:1;
+			unsigned int vlan_flag:1;
+			unsigned int nfrags_num:5;
+			unsigned int data_len:11;
+			unsigned int reservel:1;
+			unsigned int tso_flag:1;
+			unsigned int coe_flag:1;
+			unsigned int sg_flag:1;
+			unsigned int hw_own:1;
+		} tx;
+		unsigned int val;
+	} desc1;
+	unsigned int reserve_desc2;
+	unsigned int reserve3;
+
+	unsigned int tx_err;
+	unsigned int reserve5;
+	unsigned int reserve6;
+	unsigned int reserve7;
+};
+#endif
+
+#define SKB_MAGIC	((struct sk_buff *)0x5a)
+
+struct higmac_napi {
+	struct napi_struct napi;
+	struct higmac_netdev_local *ndev_priv;
+	int rxq_id;
+};
+
+struct higmac_rss_info {
+	u32 hash_cfg;
+	u32 ind_tbl_size;
+	u8 ind_tbl[RSS_INDIRECTION_TABLE_SIZE];
+	u8 key[RSS_HASH_KEY_SIZE];
+};
+
+#define QUEUE_NUMS	(4)
+struct higmac_netdev_local {
+#define HIGMAC_SG_DESC_ADD	(64U)
+	struct sg_desc *dma_sg_desc ____cacheline_aligned;
+	dma_addr_t dma_sg_phy;
+	unsigned int sg_head;
+	unsigned int sg_tail;
+	unsigned int sg_count;
+
+	void __iomem *gmac_iobase;
+	void __iomem *macif_base;
+	int index;		/* 0 -- mac0, 1 -- mac1 */
+
+	u32 hw_cap;
+	bool tso_supported;
+	bool has_rxhash_cap;
+	bool has_rss_cap;
+	int num_rxqs;
+	struct higmac_napi q_napi[RSS_NUM_RXQS];
+	int irq[RSS_NUM_RXQS];
+	struct higmac_rss_info rss_info;
+
+	struct reset_control *port_rst;
+	struct reset_control *macif_rst;
+	struct reset_control *phy_rst;
+
+	struct {
+		struct higmac_desc *desc;
+		dma_addr_t phys_addr;
+		int *sg_desc_offset;
+
+		/* how many desc in the desc pool */
+		unsigned int count;
+		struct sk_buff **skb;
+
+		/* sizeof(desc) * count */
+		unsigned int size;
+	} pool[QUEUE_NUMS + RSS_NUM_RXQS - 1];
+#define rx_fq		pool[0]
+#define rx_bq		pool[1]
+#define tx_bq		pool[2]
+#define tx_rq		pool[3]
+
+	struct sk_buff **tx_skb;
+	struct sk_buff **rx_skb;
+
+	struct device *dev;
+	struct net_device *netdev;
+	struct clk *clk;
+	struct clk *macif_clk;
+
+	struct higmac_adapter *adapter;
+
+	struct timer_list monitor;
+
+	char phy_name[MII_BUS_ID_SIZE];
+	struct phy_device *phy;
+	struct device_node *phy_node;
+	phy_interface_t phy_mode;
+	bool autoeee;
+	bool internal_phy;
+	int (*eee_init)(struct phy_device *phy_dev);
+
+	unsigned int flow_ctrl;
+	unsigned int pause;
+	unsigned int pause_interval;
+	unsigned int flow_ctrl_active_threshold;
+	unsigned int flow_ctrl_deactive_threshold;
+
+	int old_link;
+	int old_speed;
+	int old_duplex;
+
+	/* receive packet lock */
+	spinlock_t rxlock;
+	/* transmit packet lock */
+	spinlock_t txlock;
+	/* power management lock */
+	spinlock_t pmtlock;
+
+	int dev_state;		/* INIT/OPEN/CLOSE */
+	char pm_state;
+	bool wol_enable;
+	u32 msg_enable;
+#define INIT			(0)	/* power off gmac */
+#define OPEN			(1)	/* power on gmac */
+#define CLOSE			(2)	/* power off gmac */
+};
+
+enum tso_version {
+	VER_NO_TSO = 0x0,
+	VER_BYTE_SPLICE = 0x1,
+	VER_SG_COE = 0x2,
+	VER_TSO = 0x3,
+};
+
+#ifdef HIGMAC_TSO_DEBUG
+#define MAX_RECORD	(100)
+struct send_pkt_info {
+	struct higmac_tso_desc desc;
+	int status;
+};
+#endif
+
+int higmac_tx_avail(struct higmac_netdev_local *ld);
+
+/* board related func */
+void higmac_mac_core_reset(struct higmac_netdev_local *priv);
+void higmac_hw_internal_phy_reset(struct higmac_netdev_local *priv);
+void higmac_hw_external_phy_reset(struct higmac_netdev_local *priv);
+void higmac_internal_phy_clk_disable(struct higmac_netdev_local *priv);
+void higmac_internal_phy_clk_enable(struct higmac_netdev_local *priv);
+void higmac_hw_all_clk_disable(struct higmac_netdev_local *priv);
+void higmac_hw_all_clk_enable(struct higmac_netdev_local *priv);
+
+/* board independent func */
+void higmac_hw_phy_reset(struct higmac_netdev_local *priv);
+
+void pmt_reg_restore(struct higmac_netdev_local *ld);
+#endif
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/Kconfig linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/Kconfig
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/Kconfig	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/Kconfig	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,96 @@
+#
+# higmac family network device configuration
+#
+
+menuconfig HIETH_GMAC
+	tristate "hieth gmac family network device support"
+	select PHYLIB
+	select RESET_CONTROLLER
+	help
+	  This selects the hieth gmac family network device.
+	  The gigabit switch fabric (GSF) receives and transmits data over Ethernet
+	  ports at 10/100/1000 Mbit/s in full-duplex or half-duplex mode.
+	  The Ethernet port exchanges data with the CPU port, and supports
+	  the energy efficient Ethernet (EEE) and wake on LAN (WoL) functions.
+
+if HIETH_GMAC
+
+config HIGMAC_DESC_4WORD
+        bool "higmac descriptor size is 4 words"
+        default y
+        help
+	  This define the size of higmac descriptor structure.
+	  In the newest version, descriptor size is 4 words.
+	  But in some old version, the size is 8 words.
+	  The default value is true.
+
+config HIGMAC_RXCSUM
+        bool "higmac Receive checksumming offload supported"
+        default y
+        help
+	  This indicate MAC support Receive checksumming offload.
+	  Support IPv4 and IPv6, tcp and udp.
+	  The default value is enabled.
+	  If old version MAC does not support, disable this option please.
+
+config RX_FLOW_CTRL_SUPPORT
+	bool "rx flow ctrl supported"
+	default y
+	help
+	  Rx flow ctrl supported, default is enabled.
+	  When we received pause frame,
+	  we will stop transmiting data frame for some time.
+	  The stopping time is the time filled in pause frame.
+
+config TX_FLOW_CTRL_SUPPORT
+	bool "tx flow ctrl supported"
+	default y
+	help
+	  Tx flow ctrl supported, default is enabled.
+	  When we has no buffer to receive packet,
+	  we will send pause frame.
+	  When buffer is available, we will send zero-quanta pause frame.
+
+config TX_FLOW_CTRL_PAUSE_TIME
+	hex "tx flow ctrl pause time"
+	default "0xFFFF"
+	help
+	  The pause time filled in the sending pause frame.
+	  The unit is the time for transmiting 512 bit data.
+	  This value is 16 bit, so its value is 0x0000~0xFFFF.
+	  The default value is 0xFFFF.
+
+config TX_FLOW_CTRL_PAUSE_INTERVAL
+	hex "tx flow ctrl pause interval"
+	default "0xFFFF"
+	help
+	  The interval time for sending pause frame.
+	  When the remainint amount of receive queue is below tx flow ctrl active threshold,
+	  we will wait this time to transmiting pause frame.
+	  The unit is the time for transmiting 512 bit data.
+	  This value is 16 bit, so its value is 0x0000~0xFFFF.
+	  The default value is 0xFFFF.
+
+config TX_FLOW_CTRL_ACTIVE_THRESHOLD
+	int "tx flow ctrl active threshold"
+	default "16"
+	range 1 127
+	help
+	  The threshold for activing tx flow ctrl.
+	  When the left amount of receive queue descriptors is below this threshold,
+	  hardware will send pause frame immediately.
+	  We advise this value is set smaller than 64. Too bigger is not a good choice.
+	  This value must be smaller than tx flow ctrl deactive threshold.
+
+config TX_FLOW_CTRL_DEACTIVE_THRESHOLD
+	int "tx flow ctrl deactive threshold"
+	default "32"
+	range 1 127
+	help
+	  The threshold for deactiving tx flow ctrl.
+	  When the left amount of receive queue descriptors is above or equal with this threshold,
+	  hardware will exit flow control state.
+	  We advise this value is set smaller than 64. Too bigger is not a good choice.
+	  This value must be larger than tx flow ctrl active threshold.
+
+endif # HIETH_GMAC
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/Makefile linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/Makefile
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/Makefile	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/Makefile	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,2 @@
+obj-$(CONFIG_HIETH_GMAC) += hieth-gmac.o
+hieth-gmac-objs := board.o higmac.o autoeee/autoeee.o autoeee/phy_id_table.o
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/pm.c linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/pm.c
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/pm.c	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/pm.c	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,359 @@
+#include <linux/crc16.h>
+#include "higmac.h"
+
+#define N			(31)
+#define FILTERS			(4)
+struct pm_config {
+	unsigned char index;	/* bit0--eth0 bit1--eth1 */
+	unsigned char uc_pkts_enable;
+	unsigned char magic_pkts_enable;
+	unsigned char wakeup_pkts_enable;
+	struct {
+		unsigned int mask_bytes:N;
+		unsigned int reserved:1;	/* userspace ignore this bit */
+		unsigned char offset;	/* >= 12 */
+		unsigned char value[N];	/* byte string */
+		unsigned char valid;	/* valid filter */
+	} filter[FILTERS];
+};
+
+struct pm_reg_config {
+	unsigned int pmt_ctrl;
+	unsigned int pmt_mask0;
+	unsigned int pmt_mask1;
+	unsigned int pmt_mask2;
+	unsigned int pmt_mask3;
+	unsigned int pmt_cmd;
+	unsigned int pmt_offset;
+	unsigned int pmt_crc1_0;
+	unsigned int pmt_crc3_2;
+};
+
+struct pm_reg_config pm_reg_config_backup;
+
+#define PMT_CTRL		0xa00
+#define PMT_MASK0		0xa04
+#define PMT_MASK1		0xa08
+#define PMT_MASK2		0xa0c
+#define PMT_MASK3		0xa10
+#define PMT_CMD			0xa14
+#define PMT_OFFSET		0xa18
+#define PMT_CRC1_0		0xa1c
+#define PMT_CRC3_2		0xa20
+#define MASK_INVALID_BIT	BIT(31)
+
+static void init_crc_table(void);
+static unsigned short compute_crc(char *message, int nbytes);
+static unsigned short calculate_crc16(char *buf, unsigned int mask)
+{
+	char data[N];
+	int i, len = 0;
+
+	memset(data, 0, sizeof(data));
+
+	for (i = 0; i < N; i++) {
+		if (mask & 0x1)
+			data[len++] = buf[i];
+
+		mask >>= 1;
+	}
+
+	return compute_crc(data, len);
+}
+
+/* use this func in config pm func */
+void _pmt_reg_backup(struct higmac_netdev_local *ld)
+{
+	pm_reg_config_backup.pmt_ctrl = readl(ld->gmac_iobase + PMT_CTRL);
+	pm_reg_config_backup.pmt_mask0 = readl(ld->gmac_iobase + PMT_MASK0);
+	pm_reg_config_backup.pmt_mask1 = readl(ld->gmac_iobase + PMT_MASK1);
+	pm_reg_config_backup.pmt_mask2 = readl(ld->gmac_iobase + PMT_MASK2);
+	pm_reg_config_backup.pmt_mask3 = readl(ld->gmac_iobase + PMT_MASK3);
+	pm_reg_config_backup.pmt_cmd = readl(ld->gmac_iobase + PMT_CMD);
+	pm_reg_config_backup.pmt_offset = readl(ld->gmac_iobase + PMT_OFFSET);
+	pm_reg_config_backup.pmt_crc1_0 = readl(ld->gmac_iobase + PMT_CRC1_0);
+	pm_reg_config_backup.pmt_crc3_2 = readl(ld->gmac_iobase + PMT_CRC3_2);
+}
+
+#define	PM_SET			(1)
+#define PM_CLEAR		(0)
+
+int pmt_config_gmac(struct pm_config *config, struct higmac_netdev_local *ld)
+{
+	unsigned int v = 0, cmd = 0, offset = 0;
+	unsigned short crc[FILTERS] = { 0 };
+	unsigned long flags;
+	int reg_mask = 0;
+	int i;
+
+	if (!ld)
+		return -EINVAL;
+
+	spin_lock_irqsave(&ld->pmtlock, flags);
+	if (config->wakeup_pkts_enable) {
+		/* disable wakeup_pkts_enable before reconfig? */
+		v = readl(ld->gmac_iobase + PMT_CTRL);
+		v &= ~BIT(2);
+		writel(v, ld->gmac_iobase + PMT_CTRL);	/* any side effect? */
+	} else {
+		goto config_ctrl;
+	}
+
+/* filter.valid		mask.valid	mask_bytes	effect
+ *	0		*		*		no use the filter
+ *	1		0		*	all pkts can wake-up(non-exist)
+ *	1		1		0		all pkts can wake-up
+ *	1		1		!0		normal filter
+ */
+	/* setup filter */
+	for (i = 0; i < FILTERS; i++) {
+		if (config->filter[i].valid) {
+			if (config->filter[i].offset < 12)
+				continue;
+			/* offset and valid bit */
+			offset |= config->filter[i].offset << (i * 8);
+			cmd |= BIT(i * 8);	/* valid bit */
+			/* mask */
+			reg_mask = PMT_MASK0 + (i * 4);
+
+			/* for logic, mask valid bit(bit31) must set to 0,
+			 * 0 is enable
+			 */
+			v = config->filter[i].mask_bytes;
+			v &= ~BIT(31);
+			writel(v, ld->gmac_iobase + reg_mask);
+
+			/* crc */
+			crc[i] = calculate_crc16(config->filter[i].value, v);
+			if (i <= 1) {	/* for filter0 and filter 1 */
+				v = readl(ld->gmac_iobase + PMT_CRC1_0);
+				v &= ~(0xFFFF << (16 * i));
+				v |= crc[i] << (16 * i);
+				writel(v, ld->gmac_iobase + PMT_CRC1_0);
+			} else {	/* filter2 and filter3 */
+				v = readl(ld->gmac_iobase + PMT_CRC3_2);
+				v &= ~(0xFFFF << (16 * (i - 2)));
+				v |= crc[i] << (16 * (i - 2));
+				writel(v, ld->gmac_iobase + PMT_CRC3_2);
+			}
+		}
+	}
+
+	if (cmd) {
+		writel(offset, ld->gmac_iobase + PMT_OFFSET);
+		writel(cmd, ld->gmac_iobase + PMT_CMD);
+	}
+
+config_ctrl:
+	v = 0;
+	if (config->uc_pkts_enable)
+		v |= BIT(9);	/* uc pkts wakeup */
+	if (config->wakeup_pkts_enable)
+		v |= BIT(2);	/* use filter framework */
+	if (config->magic_pkts_enable)
+		v |= BIT(1);	/* magic pkts wakeup */
+
+	v |= 3 << 5;		/* clear irq status */
+	writel(v, ld->gmac_iobase + PMT_CTRL);
+
+	_pmt_reg_backup(ld);
+
+	spin_unlock_irqrestore(&ld->pmtlock, flags);
+
+	return 0;
+}
+
+/* pmt_config will overwrite pre-config */
+int pmt_config(struct net_device *ndev, struct pm_config *config)
+{
+	static int init;
+	int ret = -EINVAL;
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+
+	if (!init)
+		init_crc_table();
+
+	ret = pmt_config_gmac(config, priv);
+	if (ret)
+		return ret;
+
+	priv->pm_state = PM_SET;
+	priv->wol_enable = true;
+	device_set_wakeup_enable(priv->dev, 1);
+
+	return ret;
+}
+
+inline bool pmt_enter(struct higmac_netdev_local *ld)
+{
+	int pm = false;
+	unsigned long flags;
+
+	spin_lock_irqsave(&ld->pmtlock, flags);
+	if (ld->pm_state == PM_SET) {
+		int v;
+
+		v = readl(ld->gmac_iobase + PMT_CTRL);
+		v |= BIT(0);	/* enter power down */
+		v |= BIT(3);	/* enable wakeup irq */
+		v |= 3 << 5;	/* clear irq status */
+		writel(v, ld->gmac_iobase + PMT_CTRL);
+
+		ld->pm_state = PM_CLEAR;
+		pm = true;
+	}
+	spin_unlock_irqrestore(&ld->pmtlock, flags);
+	return pm;
+}
+
+inline void pmt_exit(struct higmac_netdev_local *ld)
+{
+	int v;
+	unsigned long flags;
+
+	/* logic auto exit power down mode */
+	spin_lock_irqsave(&ld->pmtlock, flags);
+
+	v = readl(ld->gmac_iobase + PMT_CTRL);
+	v &= ~BIT(0);		/* enter power down */
+	v &= ~BIT(3);		/* enable wakeup irq */
+
+	v |= 3 << 5;		/* clear irq status */
+	writel(v, ld->gmac_iobase + PMT_CTRL);
+
+	spin_unlock_irqrestore(&ld->pmtlock, flags);
+
+	ld->wol_enable = false;
+	/* device_set_wakeup_enable(ld->dev, 0); */
+}
+
+void pmt_reg_restore(struct higmac_netdev_local *ld)
+{
+	unsigned int v;
+	unsigned long flags;
+
+	spin_lock_irqsave(&ld->pmtlock, flags);
+	v = pm_reg_config_backup.pmt_mask0;
+	writel(v, ld->gmac_iobase + PMT_MASK0);
+
+	v = pm_reg_config_backup.pmt_mask1;
+	writel(v, ld->gmac_iobase + PMT_MASK1);
+
+	v = pm_reg_config_backup.pmt_mask2;
+	writel(v, ld->gmac_iobase + PMT_MASK2);
+
+	v = pm_reg_config_backup.pmt_mask3;
+	writel(v, ld->gmac_iobase + PMT_MASK3);
+
+	v = pm_reg_config_backup.pmt_cmd;
+	writel(v, ld->gmac_iobase + PMT_CMD);
+
+	v = pm_reg_config_backup.pmt_offset;
+	writel(v, ld->gmac_iobase + PMT_OFFSET);
+
+	v = pm_reg_config_backup.pmt_crc1_0;
+	writel(v, ld->gmac_iobase + PMT_CRC1_0);
+
+	v = pm_reg_config_backup.pmt_crc3_2;
+	writel(v, ld->gmac_iobase + PMT_CRC3_2);
+
+	v = pm_reg_config_backup.pmt_ctrl;
+	writel(v, ld->gmac_iobase + PMT_CTRL);
+	spin_unlock_irqrestore(&ld->pmtlock, flags);
+}
+
+/* ========the following code copy from Synopsys DWC_gmac_crc_example.c====== */
+#define CRC16			/* Change it to CRC16 for CRC16 Computation */
+
+#if defined(CRC16)
+#define CRC_NAME		"CRC-16"
+#define POLYNOMIAL		0x8005
+#define INITIAL_REMAINDER	0xFFFF
+#define FINAL_XOR_VALUE		0x0000
+#define REVERSE_DATA
+#undef REVERSE_REMAINDER
+#endif
+
+#define WIDTH    (8 * sizeof(unsigned short))
+#define TOPBIT   BIT(WIDTH - 1)
+
+#ifdef REVERSE_DATA
+#undef  REVERSE_DATA
+#define REVERSE_DATA(X)		((unsigned char)reverse((X), 8))
+#else
+#undef  REVERSE_DATA
+#define REVERSE_DATA(X)		(X)
+#endif
+
+#ifdef REVERSE_REMAINDER
+#undef  REVERSE_REMAINDER
+#define REVERSE_REMAINDER(X)	((unsigned short)reverse((X), WIDTH))
+#else
+#undef  REVERSE_REMAINDER
+#define REVERSE_REMAINDER(X)	(X)
+#endif
+
+static unsigned short crc_table[256];
+
+/* Reverse the data
+ * Input1: Data to be reversed
+ * Input2: number of bits in the data
+ * Output: The reversed data
+ */
+static unsigned int reverse(unsigned int data, unsigned char nbits)
+{
+	unsigned int reversed = 0x00000000;
+	unsigned char bit;
+
+	/* Reverse the data about the center bit. */
+	for (bit = 0; bit < nbits; ++bit) {
+		/* If the LSB bit is set, set the reflection of it. */
+		if (data & 0x01)
+			reversed |= BIT((nbits - 1) - bit);
+
+		data = (data >> 1);
+	}
+	return reversed;
+}
+
+/* This Initializes the partial CRC look up table */
+static void init_crc_table(void)
+{
+	unsigned short remainder;
+	int dividend;
+	unsigned char bit;
+
+	/* Compute the remainder of each possible dividend. */
+	for (dividend = 0; dividend < 256; ++dividend) {
+		/* Start with the dividend followed by zeros. */
+		remainder = (unsigned short)(dividend << (WIDTH - 8));
+
+		/* Perform modulo-2 division, a bit at a time. */
+		for (bit = 8; bit > 0; --bit) {
+			/* Try to divide the current data bit. */
+			if (remainder & TOPBIT)
+				remainder = (remainder << 1) ^ POLYNOMIAL;
+			else
+				remainder = (remainder << 1);
+		}
+
+		/* Store the result into the table. */
+		crc_table[dividend] = remainder;
+	}
+}
+
+static unsigned short compute_crc(char *message, int nbytes)
+{
+	unsigned short remainder = INITIAL_REMAINDER;
+	int byte;
+	unsigned char data;
+
+	/* Divide the message by the polynomial, a byte at a time. */
+	for (byte = 0; byte < nbytes; ++byte) {
+		data = REVERSE_DATA(message[byte]) ^ (remainder >> (WIDTH - 8));
+		remainder = crc_table[data] ^ (remainder << 8);
+	}
+
+	/* The final remainder is the CRC. */
+	return (REVERSE_REMAINDER(remainder) ^ FINAL_XOR_VALUE);
+}
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/proc-dev.c linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/proc-dev.c
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/proc-dev.c	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/proc-dev.c	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,111 @@
+#include "sockioctl.h"
+
+/* debug code */
+static int set_suspend(int eth_n)
+{
+	return 0;
+}
+
+/* debug code */
+static int set_resume(int eth_n)
+{
+	/* higmac_dev_driver.resume(&higmac_platform_device); */
+	return 0;
+}
+
+static int hw_states_read(struct seq_file *m, void *v)
+{
+	return 0;
+}
+
+static struct proc_dir_entry *higmac_proc_root;
+
+#define proc_open(name)	\
+static int proc_open_##name(struct inode *inode, struct file *file) \
+{ \
+	return single_open(file, name, PDE_DATA(inode)); \
+} \
+
+proc_open(hw_states_read);
+
+static struct proc_file {
+	char *name;
+	const struct file_operations ops;
+
+} proc_file[] = {
+	{
+		.name = "hw_stats",
+		.ops = {
+			.open           = proc_open_hw_states_read,
+			.read           = seq_read,
+			.llseek         = seq_lseek,
+			.release        = single_release,
+		},
+	}
+};
+
+/* /proc/higmac/
+ *	|---hw_stats
+ *	|---skb_pools
+ */
+void higmac_proc_create(void)
+{
+	int i;
+
+	higmac_proc_root = proc_mkdir("higmac", NULL);
+	if (!higmac_proc_root)
+		return;
+
+	for (i = 0; i < ARRAY_SIZE(proc_file); i++) {
+		struct proc_dir_entry *entry;
+
+		entry = proc_create(proc_file[i].name, 0000, higmac_proc_root,
+				    &proc_file[i].ops);
+		if (!entry)
+			pr_err("failed to create %s\n", proc_file[i].name);
+	}
+}
+
+void higmac_proc_destroy(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(proc_file); i++)
+		remove_proc_entry(proc_file[i].name, higmac_proc_root);
+
+	remove_proc_entry("higmac", NULL);
+}
+
+int higmac_ioctl(struct net_device *ndev, struct ifreq *rq, int cmd)
+{
+	struct higmac_netdev_local *priv = netdev_priv(ndev);
+	struct pm_config pm_config;
+	int val = 0;
+
+	switch (cmd) {
+	case SIOCSETPM:
+		if (copy_from_user(&pm_config, rq->ifr_data, sizeof(pm_config)))
+			return -EFAULT;
+		return pmt_config(ndev, &pm_config);
+
+	case SIOCSETSUSPEND:
+		if (copy_from_user(&val, rq->ifr_data, sizeof(val)))
+			return -EFAULT;
+		return set_suspend(val);
+
+	case SIOCSETRESUME:
+		if (copy_from_user(&val, rq->ifr_data, sizeof(val)))
+			return -EFAULT;
+		return set_resume(val);
+
+	default:
+		if (!netif_running(ndev))
+			return -EINVAL;
+
+		if (!priv->phy)
+			return -EINVAL;
+
+		return phy_mii_ioctl(priv->phy, rq, cmd);
+	}
+	return 0;
+}
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/sockioctl.h linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/sockioctl.h
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/sockioctl.h	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/sockioctl.h	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,12 @@
+#ifndef _SOCKIOCTL_H_
+#define _SOCKIOCTL_H_
+
+#include <linux/sockios.h>
+
+#define SIOCSETPM	(SIOCDEVPRIVATE + 4)	/* set pmt wake up config */
+#define SIOCSETSUSPEND	(SIOCDEVPRIVATE + 5)	/* call dev->suspend, debug */
+#define SIOCSETRESUME	(SIOCDEVPRIVATE + 6)	/* call dev->resume, debug */
+
+int higmac_ioctl(struct net_device *net_dev, struct ifreq *rq, int cmd);
+
+#endif
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/tso.h linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/tso.h
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/tso.h	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/tso.h	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,53 @@
+#ifndef __HIETH_TSO_H
+#define __HIETH_TSO_H
+
+#define SG_FLAG		BIT(30)
+#define COE_FLAG	BIT(29)
+#define TSO_FLAG	BIT(28)
+#define VLAN_FLAG	BIT(10)
+#define IPV6_FLAG	BIT(9)
+#define UDP_FLAG	BIT(8)
+
+#define PKT_IPV6_HDR_LEN	10
+#define PKT_UDP_HDR_LEN		2
+#define WORD_TO_BYTE		4
+enum {
+	PKT_NORMAL,
+	PKT_SG
+};
+
+enum {
+	PKT_IPV4,
+	PKT_IPV6
+};
+
+enum {
+	PKT_TCP,
+	PKT_UDP
+};
+
+struct frags_info {
+	/* Word(2*i+2) */
+	u32 addr;
+	/* Word(2*i+3) */
+	u32 size:16;
+	u32 reserved:16;
+};
+
+struct sg_desc {
+	/* Word0 */
+	u32 total_len:17;
+	u32 reserv:15;
+	/* Word1 */
+	u32 ipv6_id;
+	/* Word2 */
+	u32 linear_addr;
+	/* Word3 */
+	u32 linear_len:16;
+	u32 reserv3:16;
+	/* MAX_SKB_FRAGS = 17 */
+	struct frags_info frags[18];
+	/* struct frags_info frags[MAX_SKB_FRAGS]; */
+};
+
+#endif
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/util.h linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/util.h
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/higmac/util.h	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/higmac/util.h	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,29 @@
+#ifndef __HIGMAC_UTIL_H__
+#define __HIGMAC_UTIL_H__
+
+#define HIGMAC_TRACE_LEVEL 10
+
+#define higmac_trace(level, msg...) do { \
+	if ((level) >= HIGMAC_TRACE_LEVEL) { \
+		pr_info("higmac_trace:%s:%d: ", __FILE__, __LINE__); \
+		printk(msg); \
+		printk("\n"); \
+	} \
+} while (0)
+
+#define higmac_error(args...) do { \
+	pr_err("higmac:%s:%d: ", __FILE__, __LINE__); \
+	printk(args); \
+	printk("\n"); \
+} while (0)
+
+#define higmac_assert(cond) do { \
+	if (!(cond)) \
+		pr_alert("Assert:higmac:%s:%d\n", \
+			__FILE__, \
+			__LINE__);\
+} while (0)
+
+#define MK_BITS(shift, nbits) ((((shift) & 0x1F) << 16) | ((nbits) & 0x3F))
+
+#endif
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/hisi-femac/festa_v272_2723.h linux-4.9.37_modified/drivers/net/ethernet/hisilicon/hisi-femac/festa_v272_2723.h
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/hisi-femac/festa_v272_2723.h	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/hisi-femac/festa_v272_2723.h	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,102 @@
+0x33f9, 0xbd,
+0x33fa, 0x34,
+0x33fb, 0x00,
+0x33fc, 0x39,
+0x3400, 0x39,
+0x3401, 0xCC,
+0x3402, 0x27,
+0x3403, 0x23,
+0x3404, 0xFD,
+0x3405, 0xFF,
+0x3406, 0xF0,
+0x3407, 0x20,
+0x3408, 0x00,
+0x3409, 0x3C,
+0x340A, 0x3C,
+0x340B, 0x30,
+0x340C, 0xF6,
+0x340D, 0x00,
+0x340E, 0x4A,
+0x340F, 0xC4,
+0x3410, 0x7F,
+0x3411, 0xE7,
+0x3412, 0x01,
+0x3413, 0xF6,
+0x3414, 0x01,
+0x3415, 0xBE,
+0x3416, 0xC1,
+0x3417, 0x02,
+0x3418, 0x27,
+0x3419, 0x0E,
+0x341A, 0xE6,
+0x341B, 0x01,
+0x341C, 0xC1,
+0x341D, 0x14,
+0x341E, 0x27,
+0x341F, 0x08,
+0x3420, 0xC1,
+0x3421, 0x18,
+0x3422, 0x25,
+0x3423, 0x09,
+0x3424, 0xC1,
+0x3425, 0x1B,
+0x3426, 0x22,
+0x3427, 0x05,
+0x3428, 0xC6,
+0x3429, 0x5C,
+0x342A, 0xF7,
+0x342B, 0x20,
+0x342C, 0xA1,
+0x342D, 0xF6,
+0x342E, 0x01,
+0x342F, 0xBF,
+0x3430, 0xC1,
+0x3431, 0x01,
+0x3432, 0x26,
+0x3433, 0x29,
+0x3434, 0xF6,
+0x3435, 0x30,
+0x3436, 0x55,
+0x3437, 0xC0,
+0x3438, 0x05,
+0x3439, 0xE7,
+0x343A, 0x01,
+0x343B, 0xC1,
+0x343C, 0x13,
+0x343D, 0x23,
+0x343E, 0x04,
+0x343F, 0xC6,
+0x3440, 0x13,
+0x3441, 0xE7,
+0x3442, 0x01,
+0x3443, 0x18,
+0x3444, 0xFE,
+0x3445, 0x30,
+0x3446, 0x4C,
+0x3447, 0x18,
+0x3448, 0x3A,
+0x3449, 0x18,
+0x344A, 0xE6,
+0x344B, 0x00,
+0x344C, 0x58,
+0x344D, 0x58,
+0x344E, 0x58,
+0x344F, 0x58,
+0x3450, 0x58,
+0x3451, 0xE7,
+0x3452, 0x00,
+0x3453, 0xF6,
+0x3454, 0x20,
+0x3455, 0x04,
+0x3456, 0xC4,
+0x3457, 0x1F,
+0x3458, 0xEA,
+0x3459, 0x00,
+0x345A, 0xF7,
+0x345B, 0x20,
+0x345C, 0x04,
+0x345D, 0x38,
+0x345E, 0x38,
+0x345F, 0x39,
+0x3400, 0x01,
+0x33f8, 0x01
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/hisi-femac/hisi_femac.c linux-4.9.37_modified/drivers/net/ethernet/hisilicon/hisi-femac/hisi_femac.c
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/hisi-femac/hisi_femac.c	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/hisi-femac/hisi_femac.c	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,1691 @@
+/*
+ * Hisilicon Fast Ethernet MAC Driver
+ *
+ * Copyright (c) 2016 HiSilicon Technologies Co., Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program. If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/circ_buf.h>
+#include <linux/clk.h>
+#include <linux/etherdevice.h>
+#include <linux/if_ether.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/platform_device.h>
+#include <linux/reset.h>
+#include <linux/tcp.h>
+#include <net/ipv6.h>
+#include <net/protocol.h>
+
+#include "phy_fix.h"
+
+/* MAC control register list */
+#define MAC_PORTSEL			0x0200
+#define MAC_PORTSEL_STAT_CPU		BIT(0)
+#define MAC_PORTSEL_RMII		BIT(1)
+#define MAC_PORTSET			0x0208
+#define MAC_PORTSET_DUPLEX_FULL		BIT(0)
+#define MAC_PORTSET_LINKED		BIT(1)
+#define MAC_PORTSET_SPEED_100M		BIT(2)
+#define MAC_SET				0x0210
+#define MAX_FRAME_SIZE			1600
+#define MAX_FRAME_SIZE_MASK		GENMASK(10, 0)
+#define BIT_PAUSE_EN			BIT(18)
+#define RX_COALESCE_SET			0x0340
+#define RX_COALESCED_FRAME_OFFSET	24
+#define RX_COALESCED_FRAMES		8
+#define RX_COALESCED_TIMER		0x74
+#define QLEN_SET			0x0344
+#define RX_DEPTH_OFFSET			8
+#define MAX_HW_FIFO_DEPTH		64
+#define HW_TX_FIFO_DEPTH		12
+#define HW_RX_FIFO_DEPTH		(MAX_HW_FIFO_DEPTH - HW_TX_FIFO_DEPTH)
+#define FC_LEVEL			0x0348
+#define BITS_FC_ACTIVE_THR_OFFSET	8
+#define FC_DEACTIVE_THR_MASK		GENMASK(5, 0)
+#define FC_ACTIVE_THR_MASK		GENMASK(13, 8)
+#define BIT_FC_EN			BIT(14)
+#define IQFRM_DES			0x0354
+#define RX_FRAME_LEN_MASK		GENMASK(11, 0)
+#define BITS_PAYLOAD_ERR_OFFSET		28
+#define BITS_PAYLOAD_ERR_MASK		0x1
+#define BITS_HEADER_ERR_OFFSET		29
+#define BITS_HEADER_ERR_MASK		0x1
+#define BITS_PAYLOAD_DONE_OFFSET	30
+#define BITS_PAYLOAD_DONE_MASK		0x1
+#define BITS_HEADER_DONE_OFFSET		31
+#define BITS_HEADER_DONE_MASK		0x1
+#define IQ_ADDR				0x0358
+#define EQ_ADDR				0x0360
+#define EQFRM_LEN			0x0364
+#define ADDRQ_STAT			0x036C
+#define TX_CNT_INUSE_MASK		GENMASK(5, 0)
+#define BIT_TX_READY			BIT(24)
+#define BIT_RX_READY			BIT(25)
+#define RX_COE_CTRL			0x0380
+#define BIT_COE_IPV6_UDP_ZERO_DROP	BIT(13)
+#define BIT_COE_PAYLOAD_DROP		BIT(14)
+#define BIT_COE_IPHDR_DROP		BIT(15)
+#define COE_ERR_DROP			(BIT_COE_IPHDR_DROP | \
+					BIT_COE_PAYLOAD_DROP | \
+					BIT_COE_IPV6_UDP_ZERO_DROP)
+#define TSO_DBG_EN			0x03A4
+#define BITS_TSO_DBG_EN			BIT(31)
+#define TSO_DBG_STATE			0x03A8
+#define TSO_DBG_ADDR			0x03AC
+#define TSO_DBG_TX_INFO			0x03B0
+#define TSO_DBG_TX_ERR			0x03B4
+/* global control register list */
+#define GLB_HOSTMAC_L32			0x0000
+#define GLB_HOSTMAC_H16			0x0004
+#define GLB_SOFT_RESET			0x0008
+#define SOFT_RESET_ALL			BIT(0)
+#define GLB_FWCTRL			0x0010
+#define FWCTRL_VLAN_ENABLE		BIT(0)
+#define FWCTRL_FW2CPU_ENA		BIT(5)
+#define FWCTRL_FWALL2CPU		BIT(7)
+#define GLB_MACTCTRL			0x0014
+#define MACTCTRL_UNI2CPU		BIT(1)
+#define MACTCTRL_MULTI2CPU		BIT(3)
+#define MACTCTRL_BROAD2CPU		BIT(5)
+#define MACTCTRL_MACT_ENA		BIT(7)
+#define GLB_IRQ_STAT			0x0030
+#define GLB_IRQ_ENA			0x0034
+#define IRQ_ENA_PORT0_MASK		GENMASK(7, 0)
+#define IRQ_ENA_PORT0			BIT(18)
+#define IRQ_ENA_ALL			BIT(19)
+#define GLB_IRQ_RAW			0x0038
+#define IRQ_INT_RX_RDY			BIT(0)
+#define IRQ_INT_TX_PER_PACKET		BIT(1)
+#define IRQ_INT_TX_FIFO_EMPTY		BIT(6)
+#define IRQ_INT_MULTI_RXRDY		BIT(7)
+#define INT_TX_ERR			BIT(8)
+#define DEF_INT_MASK			(IRQ_INT_MULTI_RXRDY | \
+					IRQ_INT_TX_PER_PACKET | \
+					IRQ_INT_TX_FIFO_EMPTY)
+#define GLB_MAC_L32_BASE		0x0100
+#define GLB_MAC_H16_BASE		0x0104
+#define MACFLT_HI16_MASK		GENMASK(15, 0)
+#define BIT_MACFLT_ENA			BIT(17)
+#define BIT_MACFLT_FW2CPU		BIT(21)
+#define GLB_MAC_H16(reg)		(GLB_MAC_H16_BASE + ((reg) * 0x8))
+#define GLB_MAC_L32(reg)		(GLB_MAC_L32_BASE + ((reg) * 0x8))
+#define MAX_MAC_FILTER_NUM		8
+#define MAX_UNICAST_ADDRESSES		2
+#define MAX_MULTICAST_ADDRESSES		(MAX_MAC_FILTER_NUM - \
+					MAX_UNICAST_ADDRESSES)
+/* software tx and rx queue number, should be power of 2 */
+#define TXQ_NUM				64
+#define RXQ_NUM				128
+#define FEMAC_POLL_WEIGHT		16
+#define HW_CAP_TSO			BIT(0)
+#define HW_CAP_RXCSUM			BIT(1)
+#define HAS_TSO_CAP(hw_cap)		((hw_cap) & HW_CAP_TSO)
+#define HAS_RXCSUM_CAP(hw_cap)		((hw_cap) & HW_CAP_RXCSUM)
+#define RXBUF_ADDR_ALIGN_SIZE		64UL
+/* UDP header len is 2 word */
+#define UDP_HDR_LEN			2
+/* IPv6 header len is 10 word */
+#define IPV6_HDR_LEN			10
+#define WORD_TO_BYTE			4
+
+#define BIT_OFFSET_NFRAGS_NUM		11
+#define BIT_OFFSET_PROT_HEADER_LEN	16
+#define BIT_OFFSET_IP_HEADER_LEN	20
+#define BIT_FLAG_SG			BIT(26)
+#define BIT_FLAG_TXCSUM			BIT(27)
+#define BIT_FLAG_UDP			BIT(28)
+#define BIT_FLAG_IPV6			BIT(29)
+#define BIT_FLAG_VLAN			BIT(30)
+#define BIT_FLAG_TSO			BIT(31)
+
+#define PHY_RESET_DELAYS_PROPERTY	"hisilicon,phy-reset-delays-us"
+
+/* The threshold for activing tx flow ctrl.
+ * When the left amount of receive queue descriptors is below this threshold,
+ * hardware will send pause frame immediately.
+ * We advise this value is set between 1 and 10.
+ * Too bigger is not a good choice.
+ * This value must be smaller than tx flow ctrl deactive threshold.
+ */
+#define TX_FLOW_CTRL_ACTIVE_THRESHOLD	3
+/* The threshold for deactiving tx flow ctrl.
+ * When the left amount of receive queue descriptors is
+ * above or equal with this threshold,
+ * hardware will exit flow control state.
+ * We advise this value is set between 1 and 10.
+ * Too bigger is not a good choice.
+ * This value must be larger than tx flow ctrl active threshold.
+ */
+#define TX_FLOW_CTRL_DEACTIVE_THRESHOLD	5
+#define FC_ACTIVE_MIN			1
+#define FC_ACTIVE_DEFAULT		3
+#define FC_ACTIVE_MAX			31
+#define FC_DEACTIVE_MIN			1
+#define FC_DEACTIVE_DEFAULT		5
+#define FC_DEACTIVE_MAX			31
+
+enum phy_reset_delays {
+	PRE_DELAY,
+	PULSE,
+	POST_DELAY,
+	DELAYS_NUM,
+};
+
+struct hisi_femac_queue {
+	struct sk_buff **skb;
+	dma_addr_t *dma_phys;
+	int num;
+	unsigned int head;
+	unsigned int tail;
+};
+
+struct hisi_femac_tx_desc_ring {
+	struct tx_desc *desc;
+	dma_addr_t dma_phys;
+};
+
+struct hisi_femac_priv {
+	void __iomem *port_base;
+	void __iomem *glb_base;
+	struct clk *clk;
+	struct reset_control *mac_rst;
+	struct reset_control *phy_rst;
+	u32 phy_reset_delays[DELAYS_NUM];
+	u32 link_status;
+
+	struct device *dev;
+	struct net_device *ndev;
+
+	u32 hw_cap;
+	struct hisi_femac_queue txq;
+	struct hisi_femac_queue rxq;
+	struct hisi_femac_tx_desc_ring tx_ring;
+	u32 tx_fifo_used_cnt;
+	struct napi_struct napi;
+
+	/* 802.3x flow control */
+	bool tx_pause_en;
+	u32 tx_pause_active_thresh;
+	u32 tx_pause_deactive_thresh;
+};
+
+struct frags_info {
+	/* Word(2*i+2) */
+	u32 addr;
+	/* Word(2*i+3) */
+	u32 size:16;
+	u32 reserved:16;
+};
+
+struct tx_desc {
+	/* Word0 */
+	u32 total_len:17;
+	u32 reserv:15;
+	/* Word1 */
+	u32 ipv6_id;
+	/* Word2 */
+	u32 linear_addr;
+	/* Word3 */
+	u32 linear_len:16;
+	u32 reserv3:16;
+	/* MAX_SKB_FRAGS = 17 */
+	struct frags_info frags[30];
+	/* struct frags_info frags[MAX_SKB_FRAGS]; */
+};
+
+static void hisi_femac_irq_enable(struct hisi_femac_priv *priv, u32 irqs)
+{
+	u32 val;
+
+	val = readl(priv->glb_base + GLB_IRQ_ENA);
+	writel(val | irqs, priv->glb_base + GLB_IRQ_ENA);
+}
+
+static void hisi_femac_irq_disable(struct hisi_femac_priv *priv, u32 irqs)
+{
+	u32 val;
+
+	val = readl(priv->glb_base + GLB_IRQ_ENA);
+	writel(val & (~irqs), priv->glb_base + GLB_IRQ_ENA);
+}
+
+static void hisi_femac_set_flow_ctrl(struct hisi_femac_priv *priv)
+{
+	unsigned int pause_en;
+	unsigned int tx_flow_ctrl;
+
+	tx_flow_ctrl = readl(priv->port_base + FC_LEVEL);
+	tx_flow_ctrl &= ~FC_DEACTIVE_THR_MASK;
+	tx_flow_ctrl |= priv->tx_pause_deactive_thresh;
+	tx_flow_ctrl &= ~FC_ACTIVE_THR_MASK;
+	tx_flow_ctrl |= priv->tx_pause_active_thresh <<
+				BITS_FC_ACTIVE_THR_OFFSET;
+
+	pause_en = readl(priv->port_base + MAC_SET);
+
+	if (priv->tx_pause_en) {
+		tx_flow_ctrl |= BIT_FC_EN;
+		pause_en |= BIT_PAUSE_EN;
+	} else {
+		tx_flow_ctrl &= ~BIT_FC_EN;
+		pause_en &= ~BIT_PAUSE_EN;
+	}
+
+	writel(tx_flow_ctrl, priv->port_base + FC_LEVEL);
+
+	writel(pause_en, priv->port_base + MAC_SET);
+}
+
+static void hisi_femac_tx_sg_dma_unmap(struct hisi_femac_priv *priv,
+				       struct sk_buff *skb, unsigned int pos)
+{
+	struct tx_desc *desc_cur;
+	dma_addr_t addr;
+	u32 len;
+	int i;
+
+	desc_cur = priv->tx_ring.desc + pos;
+
+	addr = desc_cur->linear_addr;
+	len = desc_cur->linear_len;
+	dma_unmap_single(priv->dev, addr, len, DMA_TO_DEVICE);
+
+	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
+		addr = desc_cur->frags[i].addr;
+		len = desc_cur->frags[i].size;
+		dma_unmap_page(priv->dev, addr, len, DMA_TO_DEVICE);
+	}
+}
+
+static void hisi_femac_tx_dma_unmap(struct hisi_femac_priv *priv,
+				    struct sk_buff *skb, unsigned int pos)
+{
+	if (!(skb_is_gso(skb) || skb_shinfo(skb)->nr_frags)) {
+		dma_addr_t dma_addr;
+
+		dma_addr = priv->txq.dma_phys[pos];
+		dma_unmap_single(priv->dev, dma_addr, skb->len, DMA_TO_DEVICE);
+	} else {
+		hisi_femac_tx_sg_dma_unmap(priv, skb, pos);
+	}
+}
+
+static void hisi_femac_xmit_reclaim(struct net_device *dev)
+{
+	struct sk_buff *skb;
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+	struct hisi_femac_queue *txq = &priv->txq;
+	unsigned int bytes_compl = 0, pkts_compl = 0;
+	u32 val;
+
+	netif_tx_lock(dev);
+
+	val = readl(priv->port_base + ADDRQ_STAT) & TX_CNT_INUSE_MASK;
+	while (val < priv->tx_fifo_used_cnt) {
+		skb = txq->skb[txq->tail];
+		if (unlikely(!skb)) {
+			netdev_err(dev, "xmitq_cnt_inuse=%d, tx_fifo_used=%d\n",
+				   val, priv->tx_fifo_used_cnt);
+			break;
+		}
+		hisi_femac_tx_dma_unmap(priv, skb, txq->tail);
+		pkts_compl++;
+		bytes_compl += skb->len;
+		dev_kfree_skb_any(skb);
+
+		priv->tx_fifo_used_cnt--;
+
+		val = readl(priv->port_base + ADDRQ_STAT) & TX_CNT_INUSE_MASK;
+		txq->skb[txq->tail] = NULL;
+		txq->tail = (txq->tail + 1) % txq->num;
+	}
+
+	netdev_completed_queue(dev, pkts_compl, bytes_compl);
+
+	if (unlikely(netif_queue_stopped(dev)) && pkts_compl)
+		netif_wake_queue(dev);
+
+	netif_tx_unlock(dev);
+}
+
+static void hisi_femac_get_tso_err_info(struct hisi_femac_priv *priv)
+{
+	unsigned int reg_addr, reg_tx_info, reg_tx_err;
+	unsigned int sg_index;
+	struct tx_desc *sg_desc;
+	int *sg_word;
+	int i;
+
+	reg_addr = readl(priv->port_base + TSO_DBG_ADDR);
+	reg_tx_info = readl(priv->port_base + TSO_DBG_TX_INFO);
+	reg_tx_err = readl(priv->port_base + TSO_DBG_TX_ERR);
+
+	WARN(1, "tx err=0x%x, tx_info=0x%x, addr=0x%x\n",
+	     reg_tx_err, reg_tx_info, reg_addr);
+
+	sg_index = (reg_addr - priv->tx_ring.dma_phys) / sizeof(struct tx_desc);
+	sg_desc = priv->tx_ring.desc + sg_index;
+	sg_word = (int *)sg_desc;
+	for (i = 0; i < sizeof(struct tx_desc) / sizeof(int); i++)
+		pr_err("%s,%d: sg_desc word[%d]=0x%x\n",
+		       __func__, __LINE__, i, sg_word[i]);
+
+	/* restart MAC to transmit next packet */
+	hisi_femac_irq_disable(priv, INT_TX_ERR);
+	/* The following is recovery code,
+	 * allow netcard transmit packet again.
+	 * But now we disable it for error debug.
+	 *
+	 * readl(priv->port_base + TSO_DBG_STATE));
+	 * hisi_femac_irq_enable(priv, INT_TX_ERR);
+	 */
+}
+
+static netdev_tx_t hisi_femac_net_xmit(struct sk_buff *skb,
+				       struct net_device *dev);
+
+static netdev_tx_t hisi_femac_sw_gso(struct sk_buff *skb,
+				     struct net_device *dev)
+{
+	struct sk_buff *segs, *curr_skb;
+	netdev_features_t features = dev->features;
+
+	features &= ~(NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
+			NETIF_F_TSO | NETIF_F_TSO6 | NETIF_F_UFO);
+	segs = skb_gso_segment(skb, features);
+	if (IS_ERR_OR_NULL(segs))
+		goto drop;
+
+	do {
+		curr_skb = segs;
+		segs = segs->next;
+		curr_skb->next = NULL;
+		if (hisi_femac_net_xmit(curr_skb, dev)) {
+			dev_kfree_skb(curr_skb);
+			while (segs) {
+				curr_skb = segs;
+				segs = segs->next;
+				curr_skb->next = NULL;
+				dev_kfree_skb_any(curr_skb);
+			}
+			goto drop;
+		}
+	} while (segs);
+
+	dev_kfree_skb_any(skb);
+	return NETDEV_TX_OK;
+
+drop:
+	dev_kfree_skb_any(skb);
+	dev->stats.tx_dropped++;
+	return NETDEV_TX_OK;
+}
+
+static void hisi_femac_do_udp_checksum(struct sk_buff *skb)
+{
+	int offset;
+	__wsum csum;
+	__sum16 udp_csum;
+
+	offset = skb_checksum_start_offset(skb);
+	WARN_ON(offset >= skb_headlen(skb));
+	csum = skb_checksum(skb, offset, skb->len - offset, 0);
+
+	offset += skb->csum_offset;
+	WARN_ON(offset + sizeof(__sum16) > skb_headlen(skb));
+
+	udp_csum = csum_fold(csum);
+	if (udp_csum == 0)
+		udp_csum = CSUM_MANGLED_0;
+
+	*(__sum16 *)(skb->data + offset) = udp_csum;
+
+	skb->ip_summed = CHECKSUM_NONE;
+}
+
+static inline __be16 hisi_femac_get_l3_proto(struct sk_buff *skb)
+{
+	__be16 l3_proto;
+
+	l3_proto = skb->protocol;
+	if (skb->protocol == htons(ETH_P_8021Q))
+		l3_proto = vlan_get_protocol(skb);
+
+	return l3_proto;
+}
+
+static inline bool hisi_femac_skb_is_ipv6(struct sk_buff *skb)
+{
+	return (hisi_femac_get_l3_proto(skb) == htons(ETH_P_IPV6));
+}
+
+static int hisi_femac_check_hw_capability_for_ipv6(struct sk_buff *skb)
+{
+	unsigned int l4_proto = IPPROTO_MAX;
+
+	l4_proto = ipv6_hdr(skb)->nexthdr;
+
+	if ((l4_proto != IPPROTO_TCP) && (l4_proto != IPPROTO_UDP)) {
+		/* when IPv6 next header is not tcp or udp,
+		 * it means that IPv6 next header is extension header.
+		 * Hardware can't deal with this case,
+		 * so do checksumming by software or do GSO by software.
+		 */
+		if (skb_is_gso(skb))
+			return -ENOTSUPP;
+
+		if (skb->ip_summed == CHECKSUM_PARTIAL &&
+		    skb_checksum_help(skb))
+			return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int hisi_femac_check_hw_capability(struct sk_buff *skb)
+{
+	/* if tcp_mtu_probe() use (2 * tp->mss_cache) as probe_size,
+	 * the linear data length will be larger than 2048,
+	 * the MAC can't handle it, so let the software do it.
+	 */
+	if (skb_is_gso(skb) && (skb_headlen(skb) > 2048))
+		return -ENOTSUPP;
+
+	if (hisi_femac_skb_is_ipv6(skb))
+		return hisi_femac_check_hw_capability_for_ipv6(skb);
+
+	return 0;
+}
+
+static u32 hisi_femac_get_pkt_info(struct sk_buff *skb)
+{
+	__be16 l3_proto;
+	unsigned int l4_proto = IPPROTO_MAX;
+	bool do_txcsum = false;
+	int max_data_len = skb->len - ETH_HLEN;
+	unsigned int max_mss = ETH_DATA_LEN;
+	u32 pkt_info = 0;
+
+	if (skb->ip_summed == CHECKSUM_PARTIAL)
+		do_txcsum = true;
+
+	l3_proto = skb->protocol;
+	if (skb->protocol == htons(ETH_P_8021Q)) {
+		l3_proto = vlan_get_protocol(skb);
+		max_data_len -= VLAN_HLEN;
+		pkt_info |= BIT_FLAG_VLAN;
+	}
+
+	if (l3_proto == htons(ETH_P_IP)) {
+		struct iphdr *iph = ip_hdr(skb);
+
+		if ((max_data_len >= GSO_MAX_SIZE) &&
+		    (ntohs(iph->tot_len) <= (iph->ihl << 2)))
+			iph->tot_len = htons(GSO_MAX_SIZE - 1);
+
+		max_mss -= iph->ihl * WORD_TO_BYTE;
+		pkt_info |= (iph->ihl << BIT_OFFSET_IP_HEADER_LEN);
+		l4_proto = iph->protocol;
+	} else if (l3_proto == htons(ETH_P_IPV6)) {
+		max_mss -= IPV6_HDR_LEN * WORD_TO_BYTE;
+		pkt_info |= BIT_FLAG_IPV6;
+		pkt_info |= (IPV6_HDR_LEN << BIT_OFFSET_IP_HEADER_LEN);
+		l4_proto = ipv6_hdr(skb)->nexthdr;
+	} else {
+		do_txcsum = false;
+	}
+
+	if (l4_proto == IPPROTO_TCP) {
+		max_mss -= tcp_hdr(skb)->doff * WORD_TO_BYTE;
+		pkt_info |= (tcp_hdr(skb)->doff << BIT_OFFSET_PROT_HEADER_LEN);
+	} else if (l4_proto == IPPROTO_UDP) {
+		if (l3_proto == htons(ETH_P_IPV6))
+			max_mss -= sizeof(struct frag_hdr);
+		pkt_info |= (BIT_FLAG_UDP |
+				(UDP_HDR_LEN << BIT_OFFSET_PROT_HEADER_LEN));
+	} else {
+		do_txcsum = false;
+	}
+
+	/* Although netcard support UFO feature, it can't deal with
+	 * UDP header checksum.
+	 * So the driver will do UDP header checksum and netcard will just
+	 * fragment the packet.
+	 */
+	if (do_txcsum && skb_is_gso(skb) && (l4_proto == IPPROTO_UDP)) {
+		hisi_femac_do_udp_checksum(skb);
+		do_txcsum = false;
+	}
+
+	if (do_txcsum)
+		pkt_info |= BIT_FLAG_TXCSUM;
+
+	if (skb_is_gso(skb))
+		pkt_info |= (BIT_FLAG_SG | BIT_FLAG_TSO);
+	else if (skb_shinfo(skb)->nr_frags)
+		pkt_info |= BIT_FLAG_SG;
+
+	pkt_info |= (skb_shinfo(skb)->nr_frags << BIT_OFFSET_NFRAGS_NUM);
+	pkt_info |= (skb_is_gso(skb) ?
+		((skb_shinfo(skb)->gso_size > max_mss) ? max_mss :
+		skb_shinfo(skb)->gso_size) : (skb->len + ETH_FCS_LEN));
+
+	return pkt_info;
+}
+
+static int hisi_femac_fill_sg_desc(struct hisi_femac_priv *priv,
+				   struct sk_buff *skb, unsigned int pos)
+{
+	struct tx_desc *desc_cur;
+	dma_addr_t addr;
+	int ret;
+	int i;
+
+	desc_cur = priv->tx_ring.desc + pos;
+
+	desc_cur->ipv6_id = ntohl(skb_shinfo(skb)->ip6_frag_id);
+
+	desc_cur->total_len = skb->len;
+	addr = dma_map_single(priv->dev, skb->data, skb_headlen(skb),
+			      DMA_TO_DEVICE);
+	if (unlikely(dma_mapping_error(priv->dev, addr)))
+		return -EINVAL;
+	desc_cur->linear_addr = addr;
+	desc_cur->linear_len = skb_headlen(skb);
+
+	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
+		skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+		int len = frag->size;
+
+		addr = skb_frag_dma_map(priv->dev, frag, 0, len, DMA_TO_DEVICE);
+		ret = dma_mapping_error(priv->dev, addr);
+		if (unlikely(ret))
+			return -EINVAL;
+		desc_cur->frags[i].addr = addr;
+		desc_cur->frags[i].size = len;
+	}
+
+	return 0;
+}
+
+static void hisi_femac_adjust_link(struct net_device *dev)
+{
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+	struct phy_device *phy = dev->phydev;
+	u32 status = 0;
+
+	if (phy->link)
+		status |= MAC_PORTSET_LINKED;
+	if (phy->duplex == DUPLEX_FULL)
+		status |= MAC_PORTSET_DUPLEX_FULL;
+	if (phy->speed == SPEED_100)
+		status |= MAC_PORTSET_SPEED_100M;
+
+	if ((status != priv->link_status) &&
+	    ((status | priv->link_status) & MAC_PORTSET_LINKED)) {
+		writel(status, priv->port_base + MAC_PORTSET);
+		priv->link_status = status;
+		phy_print_status(phy);
+
+		priv->tx_pause_en = phy->pause;
+		hisi_femac_set_flow_ctrl(priv);
+	}
+}
+
+static void hisi_femac_rx_refill(struct hisi_femac_priv *priv)
+{
+	struct hisi_femac_queue *rxq = &priv->rxq;
+	struct sk_buff *skb;
+	u32 pos;
+	u32 len = MAX_FRAME_SIZE;
+	dma_addr_t addr;
+	u32 alloc_rxbuf_align = 0;
+	int reserve_room = 0;
+
+	pos = rxq->head;
+	while (readl(priv->port_base + ADDRQ_STAT) & BIT_RX_READY) {
+		if (!CIRC_SPACE(pos, rxq->tail, rxq->num))
+			break;
+		if (unlikely(rxq->skb[pos])) {
+			netdev_err(priv->ndev, "err skb[%d]=%p\n",
+				   pos, rxq->skb[pos]);
+			break;
+		}
+		len = MAX_FRAME_SIZE + RXBUF_ADDR_ALIGN_SIZE;
+		skb = netdev_alloc_skb_ip_align(priv->ndev, len);
+		if (unlikely(!skb))
+			break;
+
+		alloc_rxbuf_align = ((unsigned long)skb->data - NET_IP_ALIGN) &
+						(RXBUF_ADDR_ALIGN_SIZE - 1);
+		if (alloc_rxbuf_align) {
+			reserve_room = RXBUF_ADDR_ALIGN_SIZE -
+							alloc_rxbuf_align;
+			len -= reserve_room;
+			skb_reserve(skb, reserve_room);
+		}
+
+		addr = dma_map_single(priv->dev, skb->data, len,
+				      DMA_FROM_DEVICE);
+		if (dma_mapping_error(priv->dev, addr)) {
+			dev_kfree_skb_any(skb);
+			break;
+		}
+		rxq->dma_phys[pos] = addr;
+		rxq->skb[pos] = skb;
+		writel(addr, priv->port_base + IQ_ADDR);
+		pos = (pos + 1) % rxq->num;
+	}
+	rxq->head = pos;
+}
+
+static u32 hisi_femac_rx(struct net_device *dev, int limit)
+{
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+	struct hisi_femac_queue *rxq = &priv->rxq;
+	struct sk_buff *skb;
+	dma_addr_t addr;
+	u32 rx_pkt_info, pos, len, rx_pkts_num = 0;
+	int hdr_csum_done, hdr_csum_err;
+	int payload_csum_done, payload_csum_err;
+
+	pos = rxq->tail;
+	while (readl(priv->glb_base + GLB_IRQ_RAW) & IRQ_INT_RX_RDY) {
+		rx_pkt_info = readl(priv->port_base + IQFRM_DES);
+		len = rx_pkt_info & RX_FRAME_LEN_MASK;
+		len -= ETH_FCS_LEN;
+
+		/* tell hardware we will deal with this packet */
+		writel(IRQ_INT_RX_RDY, priv->glb_base + GLB_IRQ_RAW);
+
+		rx_pkts_num++;
+
+		skb = rxq->skb[pos];
+		if (unlikely(!skb)) {
+			netdev_err(dev, "rx skb NULL. pos=%d\n", pos);
+			break;
+		}
+		rxq->skb[pos] = NULL;
+
+		addr = rxq->dma_phys[pos];
+		dma_unmap_single(priv->dev, addr, MAX_FRAME_SIZE,
+				 DMA_FROM_DEVICE);
+		skb_put(skb, len);
+		if (unlikely(skb->len > MAX_FRAME_SIZE)) {
+			netdev_err(dev, "rcv len err, len = %d\n", skb->len);
+			dev->stats.rx_errors++;
+			dev->stats.rx_length_errors++;
+			dev_kfree_skb_any(skb);
+			goto next;
+		}
+
+		skb->ip_summed = CHECKSUM_NONE;
+		if (dev->features & NETIF_F_RXCSUM) {
+			hdr_csum_done =
+				(rx_pkt_info >> BITS_HEADER_DONE_OFFSET) &
+				BITS_HEADER_DONE_MASK;
+			payload_csum_done =
+				(rx_pkt_info >> BITS_PAYLOAD_DONE_OFFSET) &
+				BITS_PAYLOAD_DONE_MASK;
+			hdr_csum_err =
+				(rx_pkt_info >> BITS_HEADER_ERR_OFFSET) &
+				BITS_HEADER_ERR_MASK;
+			payload_csum_err =
+				(rx_pkt_info >> BITS_PAYLOAD_ERR_OFFSET) &
+				BITS_PAYLOAD_ERR_MASK;
+
+			if (hdr_csum_done && payload_csum_done) {
+				if (unlikely(hdr_csum_err)) {
+					dev->stats.rx_errors++;
+					dev->stats.rx_crc_errors++;
+					dev_kfree_skb_any(skb);
+					goto next;
+				} else if (!payload_csum_err) {
+					skb->ip_summed = CHECKSUM_UNNECESSARY;
+				}
+			}
+		}
+
+		skb->protocol = eth_type_trans(skb, dev);
+		napi_gro_receive(&priv->napi, skb);
+		dev->stats.rx_packets++;
+		dev->stats.rx_bytes += skb->len;
+next:
+		pos = (pos + 1) % rxq->num;
+		if (rx_pkts_num >= limit)
+			break;
+	}
+	rxq->tail = pos;
+
+	hisi_femac_rx_refill(priv);
+
+	return rx_pkts_num;
+}
+
+static int hisi_femac_poll(struct napi_struct *napi, int budget)
+{
+	struct hisi_femac_priv *priv = container_of(napi,
+					struct hisi_femac_priv, napi);
+	struct net_device *dev = priv->ndev;
+	int work_done = 0, task = budget;
+	u32 ints, num;
+
+	do {
+		hisi_femac_xmit_reclaim(dev);
+		num = hisi_femac_rx(dev, task);
+		work_done += num;
+		task -= num;
+		if (work_done >= budget)
+			break;
+
+		ints = readl(priv->glb_base + GLB_IRQ_RAW);
+		writel(ints & DEF_INT_MASK,
+		       priv->glb_base + GLB_IRQ_RAW);
+	} while (ints & DEF_INT_MASK);
+
+	if (work_done < budget) {
+		napi_complete(napi);
+		hisi_femac_irq_enable(priv, DEF_INT_MASK &
+					(~IRQ_INT_TX_PER_PACKET));
+	}
+
+	return work_done;
+}
+
+static irqreturn_t hisi_femac_interrupt(int irq, void *dev_id)
+{
+	u32 ints;
+	struct net_device *dev = (struct net_device *)dev_id;
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+
+	ints = readl(priv->glb_base + GLB_IRQ_RAW);
+
+	if (likely(ints & DEF_INT_MASK)) {
+		writel(ints & DEF_INT_MASK,
+		       priv->glb_base + GLB_IRQ_RAW);
+		hisi_femac_irq_disable(priv, DEF_INT_MASK);
+		napi_schedule(&priv->napi);
+	}
+
+	if (HAS_TSO_CAP(priv->hw_cap) &&
+	    unlikely(ints & INT_TX_ERR))
+		hisi_femac_get_tso_err_info(priv);
+
+	return IRQ_HANDLED;
+}
+
+static int hisi_femac_init_tx_descriptor_ring(struct hisi_femac_priv *priv)
+{
+	priv->tx_ring.desc = (struct tx_desc *)dma_zalloc_coherent(priv->dev,
+			TXQ_NUM * sizeof(struct tx_desc),
+			&priv->tx_ring.dma_phys,
+			GFP_KERNEL);
+	if (!priv->tx_ring.desc)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static void hisi_femac_destroy_tx_descriptor_ring(struct hisi_femac_priv *priv)
+{
+	if (priv->tx_ring.desc)
+		dma_free_coherent(priv->dev,
+				  TXQ_NUM * sizeof(struct tx_desc),
+				  priv->tx_ring.desc, priv->tx_ring.dma_phys);
+	priv->tx_ring.desc = NULL;
+}
+
+static int hisi_femac_init_queue(struct device *dev,
+				 struct hisi_femac_queue *queue,
+				 unsigned int num)
+{
+	queue->skb = devm_kcalloc(dev, num, sizeof(struct sk_buff *),
+				  GFP_KERNEL);
+	if (!queue->skb)
+		return -ENOMEM;
+
+	queue->dma_phys = devm_kcalloc(dev, num, sizeof(dma_addr_t),
+				       GFP_KERNEL);
+	if (!queue->dma_phys)
+		return -ENOMEM;
+
+	queue->num = num;
+	queue->head = 0;
+	queue->tail = 0;
+
+	return 0;
+}
+
+static int hisi_femac_init_tx_and_rx_queues(struct hisi_femac_priv *priv)
+{
+	int ret;
+
+	ret = hisi_femac_init_queue(priv->dev, &priv->txq, TXQ_NUM);
+	if (ret)
+		return ret;
+
+	ret = hisi_femac_init_queue(priv->dev, &priv->rxq, RXQ_NUM);
+	if (ret)
+		return ret;
+
+	priv->tx_fifo_used_cnt = 0;
+
+	return 0;
+}
+
+static void hisi_femac_free_skb_rings(struct hisi_femac_priv *priv)
+{
+	struct hisi_femac_queue *txq = &priv->txq;
+	struct hisi_femac_queue *rxq = &priv->rxq;
+	struct sk_buff *skb;
+	dma_addr_t dma_addr;
+	u32 pos;
+
+	pos = rxq->tail;
+	while (pos != rxq->head) {
+		skb = rxq->skb[pos];
+		if (unlikely(!skb)) {
+			netdev_err(priv->ndev, "NULL rx skb. pos=%d, head=%d\n",
+				   pos, rxq->head);
+			continue;
+		}
+
+		dma_addr = rxq->dma_phys[pos];
+		dma_unmap_single(priv->dev, dma_addr, MAX_FRAME_SIZE,
+				 DMA_FROM_DEVICE);
+
+		dev_kfree_skb_any(skb);
+		rxq->skb[pos] = NULL;
+		pos = (pos + 1) % rxq->num;
+	}
+	rxq->tail = pos;
+
+	pos = txq->tail;
+	while (pos != txq->head) {
+		skb = txq->skb[pos];
+		if (unlikely(!skb)) {
+			netdev_err(priv->ndev, "NULL tx skb. pos=%d, head=%d\n",
+				   pos, txq->head);
+			continue;
+		}
+		hisi_femac_tx_dma_unmap(priv, skb, pos);
+		dev_kfree_skb_any(skb);
+		txq->skb[pos] = NULL;
+		pos = (pos + 1) % txq->num;
+	}
+	txq->tail = pos;
+	priv->tx_fifo_used_cnt = 0;
+}
+
+static int hisi_femac_set_hw_mac_addr(struct hisi_femac_priv *priv,
+				      unsigned char *mac)
+{
+	u32 reg;
+
+	reg = mac[1] | (mac[0] << 8);
+	writel(reg, priv->glb_base + GLB_HOSTMAC_H16);
+
+	reg = mac[5] | (mac[4] << 8) | (mac[3] << 16) | (mac[2] << 24);
+	writel(reg, priv->glb_base + GLB_HOSTMAC_L32);
+
+	return 0;
+}
+
+static int hisi_femac_port_reset(struct hisi_femac_priv *priv)
+{
+	u32 val;
+
+	val = readl(priv->glb_base + GLB_SOFT_RESET);
+	val |= SOFT_RESET_ALL;
+	writel(val, priv->glb_base + GLB_SOFT_RESET);
+
+	usleep_range(500, 800);
+
+	val &= ~SOFT_RESET_ALL;
+	writel(val, priv->glb_base + GLB_SOFT_RESET);
+
+	return 0;
+}
+
+static int hisi_femac_net_open(struct net_device *dev)
+{
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+
+	hisi_femac_set_hw_mac_addr(priv, dev->dev_addr);
+	/* clear interrupts will drop the first packet MAC have received,
+	 * so do it before refill the rx free skbs.
+	 */
+	writel(IRQ_ENA_PORT0_MASK, priv->glb_base + GLB_IRQ_RAW);
+	hisi_femac_rx_refill(priv);
+
+	netif_carrier_off(dev);
+	netdev_reset_queue(dev);
+	netif_start_queue(dev);
+	napi_enable(&priv->napi);
+
+	priv->link_status = 0;
+	if (dev->phydev)
+		phy_start(dev->phydev);
+
+	hisi_femac_irq_enable(priv, IRQ_ENA_ALL | IRQ_ENA_PORT0 | DEF_INT_MASK);
+	if (HAS_TSO_CAP(priv->hw_cap))
+		hisi_femac_irq_enable(priv, INT_TX_ERR);
+
+	return 0;
+}
+
+static void hisi_femac_port_init(struct hisi_femac_priv *priv);
+
+static int hisi_femac_net_close(struct net_device *dev)
+{
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+
+	hisi_femac_irq_disable(priv, IRQ_ENA_PORT0);
+
+	if (dev->phydev)
+		phy_stop(dev->phydev);
+
+	netif_stop_queue(dev);
+	napi_disable(&priv->napi);
+
+	/* reset MAC port first before free skb rings
+	 * to prevent potential risk of use-after-free.
+	 */
+	hisi_femac_port_reset(priv);
+	hisi_femac_port_init(priv);
+
+	priv->tx_pause_en = false;
+	hisi_femac_set_flow_ctrl(priv);
+	hisi_femac_free_skb_rings(priv);
+
+	return 0;
+}
+
+static netdev_tx_t hisi_femac_net_xmit(struct sk_buff *skb,
+				       struct net_device *dev)
+{
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+	struct hisi_femac_queue *txq = &priv->txq;
+	dma_addr_t addr;
+	int ret;
+	u32 pkt_info;
+	u32 val;
+
+	val = readl(priv->port_base + ADDRQ_STAT);
+	val &= BIT_TX_READY;
+	if (!val) {
+		hisi_femac_irq_enable(priv, IRQ_INT_TX_PER_PACKET);
+		dev->stats.tx_dropped++;
+		dev->stats.tx_fifo_errors++;
+		netif_stop_queue(dev);
+		return NETDEV_TX_BUSY;
+	}
+
+	if (unlikely(!CIRC_SPACE(txq->head, txq->tail,
+				 txq->num))) {
+		hisi_femac_irq_enable(priv, IRQ_INT_TX_PER_PACKET);
+		dev->stats.tx_dropped++;
+		dev->stats.tx_fifo_errors++;
+		netif_stop_queue(dev);
+		return NETDEV_TX_BUSY;
+	}
+
+	ret = hisi_femac_check_hw_capability(skb);
+	if (unlikely(ret)) {
+		if (ret == -ENOTSUPP)
+			return hisi_femac_sw_gso(skb, dev);
+
+		dev_kfree_skb_any(skb);
+		dev->stats.tx_dropped++;
+		return NETDEV_TX_OK;
+	}
+
+	pkt_info = hisi_femac_get_pkt_info(skb);
+
+	if (!(skb_is_gso(skb) || skb_shinfo(skb)->nr_frags)) {
+		addr = dma_map_single(priv->dev, skb->data,
+				      skb->len, DMA_TO_DEVICE);
+		if (unlikely(dma_mapping_error(priv->dev, addr))) {
+			dev_kfree_skb_any(skb);
+			dev->stats.tx_dropped++;
+			return NETDEV_TX_OK;
+		}
+	} else {
+		ret = hisi_femac_fill_sg_desc(priv, skb, txq->head);
+		if (unlikely(ret)) {
+			dev_kfree_skb_any(skb);
+			dev->stats.tx_dropped++;
+			return NETDEV_TX_OK;
+		}
+
+		addr = priv->tx_ring.dma_phys +
+			txq->head * sizeof(struct tx_desc);
+
+		/* Ensure desc info writen to memory before config hardware */
+		wmb();
+	}
+	txq->dma_phys[txq->head] = addr;
+
+	txq->skb[txq->head] = skb;
+	txq->head = (txq->head + 1) % txq->num;
+
+	writel(addr, priv->port_base + EQ_ADDR);
+	writel(pkt_info, priv->port_base + EQFRM_LEN);
+
+	priv->tx_fifo_used_cnt++;
+
+	dev->stats.tx_packets++;
+	dev->stats.tx_bytes += skb->len;
+	netdev_sent_queue(dev, skb->len);
+
+	return NETDEV_TX_OK;
+}
+
+static int hisi_femac_set_mac_address(struct net_device *dev, void *p)
+{
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+	struct sockaddr *skaddr = p;
+
+	if (!is_valid_ether_addr(skaddr->sa_data))
+		return -EADDRNOTAVAIL;
+
+	memcpy(dev->dev_addr, skaddr->sa_data, dev->addr_len);
+	dev->addr_assign_type &= ~NET_ADDR_RANDOM;
+
+	hisi_femac_set_hw_mac_addr(priv, dev->dev_addr);
+
+	return 0;
+}
+
+static void hisi_femac_enable_hw_addr_filter(struct hisi_femac_priv *priv,
+					     unsigned int reg_n, bool enable)
+{
+	u32 val;
+
+	val = readl(priv->glb_base + GLB_MAC_H16(reg_n));
+	if (enable)
+		val |= BIT_MACFLT_ENA;
+	else
+		val &= ~BIT_MACFLT_ENA;
+	writel(val, priv->glb_base + GLB_MAC_H16(reg_n));
+}
+
+static void hisi_femac_set_hw_addr_filter(struct hisi_femac_priv *priv,
+					  unsigned char *addr,
+					  unsigned int reg_n)
+{
+	unsigned int high, low;
+	u32 val;
+
+	high = GLB_MAC_H16(reg_n);
+	low = GLB_MAC_L32(reg_n);
+
+	val = (addr[2] << 24) | (addr[3] << 16) | (addr[4] << 8) | addr[5];
+	writel(val, priv->glb_base + low);
+
+	val = readl(priv->glb_base + high);
+	val &= ~MACFLT_HI16_MASK;
+	val |= ((addr[0] << 8) | addr[1]);
+	val |= (BIT_MACFLT_ENA | BIT_MACFLT_FW2CPU);
+	writel(val, priv->glb_base + high);
+}
+
+static void hisi_femac_set_promisc_mode(struct hisi_femac_priv *priv,
+					bool promisc_mode)
+{
+	u32 val;
+
+	val = readl(priv->glb_base + GLB_FWCTRL);
+	if (promisc_mode)
+		val |= FWCTRL_FWALL2CPU;
+	else
+		val &= ~FWCTRL_FWALL2CPU;
+	writel(val, priv->glb_base + GLB_FWCTRL);
+}
+
+/* Handle multiple multicast addresses (perfect filtering)*/
+static void hisi_femac_set_mc_addr_filter(struct hisi_femac_priv *priv)
+{
+	struct net_device *dev = priv->ndev;
+	u32 val;
+
+	val = readl(priv->glb_base + GLB_MACTCTRL);
+	if ((netdev_mc_count(dev) > MAX_MULTICAST_ADDRESSES) ||
+	    (dev->flags & IFF_ALLMULTI)) {
+		val |= MACTCTRL_MULTI2CPU;
+	} else {
+		int reg = MAX_UNICAST_ADDRESSES;
+		int i;
+		struct netdev_hw_addr *ha;
+
+		for (i = reg; i < MAX_MAC_FILTER_NUM; i++)
+			hisi_femac_enable_hw_addr_filter(priv, i, false);
+
+		netdev_for_each_mc_addr(ha, dev) {
+			hisi_femac_set_hw_addr_filter(priv, ha->addr, reg);
+			reg++;
+		}
+		val &= ~MACTCTRL_MULTI2CPU;
+	}
+	writel(val, priv->glb_base + GLB_MACTCTRL);
+}
+
+/* Handle multiple unicast addresses (perfect filtering)*/
+static void hisi_femac_set_uc_addr_filter(struct hisi_femac_priv *priv)
+{
+	struct net_device *dev = priv->ndev;
+	u32 val;
+
+	val = readl(priv->glb_base + GLB_MACTCTRL);
+	if (netdev_uc_count(dev) > MAX_UNICAST_ADDRESSES) {
+		val |= MACTCTRL_UNI2CPU;
+	} else {
+		int reg = 0;
+		int i;
+		struct netdev_hw_addr *ha;
+
+		for (i = reg; i < MAX_UNICAST_ADDRESSES; i++)
+			hisi_femac_enable_hw_addr_filter(priv, i, false);
+
+		netdev_for_each_uc_addr(ha, dev) {
+			hisi_femac_set_hw_addr_filter(priv, ha->addr, reg);
+			reg++;
+		}
+		val &= ~MACTCTRL_UNI2CPU;
+	}
+	writel(val, priv->glb_base + GLB_MACTCTRL);
+}
+
+static void hisi_femac_net_set_rx_mode(struct net_device *dev)
+{
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+
+	if (dev->flags & IFF_PROMISC) {
+		hisi_femac_set_promisc_mode(priv, true);
+	} else {
+		hisi_femac_set_promisc_mode(priv, false);
+		hisi_femac_set_mc_addr_filter(priv);
+		hisi_femac_set_uc_addr_filter(priv);
+	}
+}
+
+static int hisi_femac_net_ioctl(struct net_device *dev,
+				struct ifreq *ifreq, int cmd)
+{
+	if (!netif_running(dev))
+		return -EINVAL;
+
+	if (!dev->phydev)
+		return -EINVAL;
+
+	return phy_mii_ioctl(dev->phydev, ifreq, cmd);
+}
+
+static void hisi_femac_get_pauseparam(struct net_device *dev,
+				      struct ethtool_pauseparam *pause)
+{
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+
+	pause->autoneg = dev->phydev->autoneg;
+	pause->rx_pause = 1;
+	if (priv->tx_pause_en)
+		pause->tx_pause = 1;
+}
+
+static int hisi_femac_set_pauseparam(struct net_device *dev,
+				     struct ethtool_pauseparam *pause)
+{
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+	struct phy_device *phy = dev->phydev;
+	int ret = 0;
+
+	if (pause->rx_pause == 0)
+		return -EINVAL;
+
+	if (pause->tx_pause != priv->tx_pause_en) {
+		priv->tx_pause_en = pause->tx_pause;
+		hisi_femac_set_flow_ctrl(priv);
+	}
+
+	if (phy->autoneg) {
+		if (netif_running(dev)) {
+			struct ethtool_cmd cmd;
+			/* auto-negotiation automatically restarted */
+			cmd.cmd = ETHTOOL_NWAY_RST;
+			cmd.supported = phy->supported;
+			cmd.advertising = phy->advertising;
+			cmd.autoneg = phy->autoneg;
+			cmd.speed = phy->speed;
+			cmd.duplex = phy->duplex;
+			cmd.phy_address = phy->mdio.addr;
+			ret = phy_ethtool_sset(phy, &cmd);
+		}
+	}
+
+	return ret;
+}
+
+static void hisi_femac_enable_rxcsum_drop(struct hisi_femac_priv *priv,
+					  bool drop)
+{
+	unsigned int val;
+
+	val = readl(priv->port_base + RX_COE_CTRL);
+	val &= ~COE_ERR_DROP;
+	if (drop)
+		val |= (BIT_COE_IPHDR_DROP | BIT_COE_IPV6_UDP_ZERO_DROP);
+	writel(val, priv->port_base + RX_COE_CTRL);
+}
+
+static int hisi_femac_set_features(struct net_device *dev,
+				   netdev_features_t features)
+{
+	struct hisi_femac_priv *priv = netdev_priv(dev);
+	netdev_features_t changed = dev->features ^ features;
+
+	if (changed & NETIF_F_RXCSUM) {
+		if (features & NETIF_F_RXCSUM)
+			hisi_femac_enable_rxcsum_drop(priv, true);
+		else
+			hisi_femac_enable_rxcsum_drop(priv, false);
+	}
+
+	return 0;
+}
+
+static const struct ethtool_ops hisi_femac_ethtools_ops = {
+	.get_link		= ethtool_op_get_link,
+	.get_link_ksettings	= phy_ethtool_get_link_ksettings,
+	.set_link_ksettings	= phy_ethtool_set_link_ksettings,
+	.get_pauseparam		= hisi_femac_get_pauseparam,
+	.set_pauseparam		= hisi_femac_set_pauseparam,
+};
+
+static const struct net_device_ops hisi_femac_netdev_ops = {
+	.ndo_open		= hisi_femac_net_open,
+	.ndo_stop		= hisi_femac_net_close,
+	.ndo_start_xmit		= hisi_femac_net_xmit,
+	.ndo_do_ioctl		= hisi_femac_net_ioctl,
+	.ndo_set_mac_address	= hisi_femac_set_mac_address,
+	.ndo_set_rx_mode	= hisi_femac_net_set_rx_mode,
+	.ndo_change_mtu		= eth_change_mtu,
+	.ndo_set_features	= hisi_femac_set_features,
+};
+
+static void hisi_femac_verify_flow_ctrl_args(struct hisi_femac_priv *priv)
+{
+	if (priv->tx_pause_active_thresh < FC_ACTIVE_MIN ||
+	    priv->tx_pause_active_thresh > FC_ACTIVE_MAX)
+		priv->tx_pause_active_thresh = FC_ACTIVE_DEFAULT;
+
+	if (priv->tx_pause_deactive_thresh < FC_DEACTIVE_MIN ||
+	    priv->tx_pause_deactive_thresh > FC_DEACTIVE_MAX)
+		priv->tx_pause_deactive_thresh = FC_DEACTIVE_DEFAULT;
+
+	if (priv->tx_pause_active_thresh >= priv->tx_pause_deactive_thresh) {
+		priv->tx_pause_active_thresh = FC_ACTIVE_DEFAULT;
+		priv->tx_pause_deactive_thresh = FC_DEACTIVE_DEFAULT;
+	}
+}
+
+static void hisi_femac_core_reset(struct hisi_femac_priv *priv)
+{
+	reset_control_assert(priv->mac_rst);
+	reset_control_deassert(priv->mac_rst);
+}
+
+static void hisi_femac_sleep_us(u32 time_us)
+{
+	u32 time_ms;
+
+	if (!time_us)
+		return;
+
+	time_ms = DIV_ROUND_UP(time_us, 1000);
+	if (time_ms < 20)
+		usleep_range(time_us, time_us + 500);
+	else
+		msleep(time_ms);
+}
+
+static void hisi_femac_phy_reset(struct hisi_femac_priv *priv)
+{
+	/* To make sure PHY hardware reset success,
+	 * we must keep PHY in deassert state first and
+	 * then complete the hardware reset operation
+	 */
+	reset_control_deassert(priv->phy_rst);
+	hisi_femac_sleep_us(priv->phy_reset_delays[PRE_DELAY]);
+
+	reset_control_assert(priv->phy_rst);
+	/* delay some time to ensure reset ok,
+	 * this depends on PHY hardware feature
+	 */
+	hisi_femac_sleep_us(priv->phy_reset_delays[PULSE]);
+	reset_control_deassert(priv->phy_rst);
+	/* delay some time to ensure later MDIO access */
+	hisi_femac_sleep_us(priv->phy_reset_delays[POST_DELAY]);
+}
+
+static void hisi_femac_port_init(struct hisi_femac_priv *priv)
+{
+	u32 val;
+
+	/* MAC gets link status info and phy mode by software config */
+	val = MAC_PORTSEL_STAT_CPU;
+	if (priv->ndev->phydev->interface == PHY_INTERFACE_MODE_RMII)
+		val |= MAC_PORTSEL_RMII;
+	writel(val, priv->port_base + MAC_PORTSEL);
+
+	/*clear all interrupt status */
+	writel(IRQ_ENA_PORT0_MASK, priv->glb_base + GLB_IRQ_RAW);
+	hisi_femac_irq_disable(priv, IRQ_ENA_PORT0_MASK | IRQ_ENA_PORT0);
+
+	if (HAS_TSO_CAP(priv->hw_cap)) {
+		/* enable TSO debug for error handle */
+		val = readl(priv->port_base + TSO_DBG_EN);
+		val |= BITS_TSO_DBG_EN;
+		writel(val, priv->port_base + TSO_DBG_EN);
+	}
+
+	val = readl(priv->glb_base + GLB_FWCTRL);
+	val &= ~(FWCTRL_VLAN_ENABLE | FWCTRL_FWALL2CPU);
+	val |= FWCTRL_FW2CPU_ENA;
+	writel(val, priv->glb_base + GLB_FWCTRL);
+
+	val = readl(priv->glb_base + GLB_MACTCTRL);
+	val |= (MACTCTRL_BROAD2CPU | MACTCTRL_MACT_ENA);
+	writel(val, priv->glb_base + GLB_MACTCTRL);
+
+	val = readl(priv->port_base + MAC_SET);
+	val &= ~MAX_FRAME_SIZE_MASK;
+	val |= MAX_FRAME_SIZE;
+	writel(val, priv->port_base + MAC_SET);
+
+	val = RX_COALESCED_TIMER |
+		(RX_COALESCED_FRAMES << RX_COALESCED_FRAME_OFFSET);
+	writel(val, priv->port_base + RX_COALESCE_SET);
+
+	val = (HW_RX_FIFO_DEPTH << RX_DEPTH_OFFSET) | HW_TX_FIFO_DEPTH;
+	writel(val, priv->port_base + QLEN_SET);
+
+	hisi_femac_set_flow_ctrl(priv);
+}
+
+static int hisi_femac_drv_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *node = dev->of_node;
+	struct resource *res;
+	struct net_device *ndev;
+	struct hisi_femac_priv *priv;
+	struct phy_device *phy;
+	const char *mac_addr;
+	int ret;
+
+	ndev = alloc_etherdev(sizeof(*priv));
+	if (!ndev)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ndev);
+	SET_NETDEV_DEV(ndev, &pdev->dev);
+
+	priv = netdev_priv(ndev);
+	priv->dev = dev;
+	priv->ndev = ndev;
+
+	if (of_device_is_compatible(node, "hisilicon,hisi-femac-v2"))
+		priv->hw_cap |= HW_CAP_TSO | HW_CAP_RXCSUM;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	priv->port_base = devm_ioremap_resource(dev, res);
+	if (IS_ERR(priv->port_base)) {
+		ret = PTR_ERR(priv->port_base);
+		goto out_free_netdev;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+	priv->glb_base = devm_ioremap_resource(dev, res);
+	if (IS_ERR(priv->glb_base)) {
+		ret = PTR_ERR(priv->glb_base);
+		goto out_free_netdev;
+	}
+
+	priv->clk = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(priv->clk)) {
+		dev_err(dev, "failed to get clk\n");
+		ret = -ENODEV;
+		goto out_free_netdev;
+	}
+
+	ret = clk_prepare_enable(priv->clk);
+	if (ret) {
+		dev_err(dev, "failed to enable clk %d\n", ret);
+		goto out_free_netdev;
+	}
+
+	priv->mac_rst = devm_reset_control_get(dev, "mac");
+	if (IS_ERR(priv->mac_rst)) {
+		ret = PTR_ERR(priv->mac_rst);
+		goto out_disable_clk;
+	}
+	hisi_femac_core_reset(priv);
+
+	priv->phy_rst = devm_reset_control_get(dev, "phy");
+	if (IS_ERR(priv->phy_rst)) {
+		priv->phy_rst = NULL;
+	} else {
+		ret = of_property_read_u32_array(node,
+						 PHY_RESET_DELAYS_PROPERTY,
+						 priv->phy_reset_delays,
+						 DELAYS_NUM);
+		if (ret)
+			goto out_disable_clk;
+		hisi_femac_phy_reset(priv);
+	}
+
+	phy_register_fixups();
+
+	phy = of_phy_get_and_connect(ndev, node, hisi_femac_adjust_link);
+	if (!phy) {
+		dev_err(dev, "connect to PHY failed!\n");
+		ret = -ENODEV;
+		goto out_disable_clk;
+	}
+
+	phy->advertising |= ADVERTISED_Pause;
+	phy->supported |= ADVERTISED_Pause;
+
+	phy->advertising &= ~(ADVERTISED_1000baseT_Full |
+			      ADVERTISED_1000baseT_Half);
+
+	phy_attached_print(phy, "phy_id=0x%.8lx, phy_mode=%s\n",
+			   (unsigned long)phy->phy_id,
+			   phy_modes(phy->interface));
+
+	mac_addr = of_get_mac_address(node);
+	if (mac_addr)
+		ether_addr_copy(ndev->dev_addr, mac_addr);
+	if (!is_valid_ether_addr(ndev->dev_addr)) {
+		eth_hw_addr_random(ndev);
+		dev_warn(dev, "using random MAC address %pM\n",
+			 ndev->dev_addr);
+	}
+
+	ndev->watchdog_timeo = 6 * HZ;
+	ndev->priv_flags |= IFF_UNICAST_FLT;
+	ndev->netdev_ops = &hisi_femac_netdev_ops;
+	ndev->ethtool_ops = &hisi_femac_ethtools_ops;
+	netif_napi_add(ndev, &priv->napi, hisi_femac_poll, FEMAC_POLL_WEIGHT);
+
+	if (HAS_TSO_CAP(priv->hw_cap))
+		ndev->hw_features |= NETIF_F_SG |
+			NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
+			NETIF_F_TSO | NETIF_F_TSO6 | NETIF_F_UFO;
+
+	if (HAS_RXCSUM_CAP(priv->hw_cap))
+		ndev->hw_features |= NETIF_F_RXCSUM;
+	ndev->features |= ndev->hw_features;
+	ndev->vlan_features |= ndev->features;
+
+	device_set_wakeup_capable(priv->dev, true);
+	device_set_wakeup_enable(priv->dev, true);
+
+	priv->tx_pause_en = true;
+	priv->tx_pause_active_thresh = TX_FLOW_CTRL_ACTIVE_THRESHOLD;
+	priv->tx_pause_deactive_thresh = TX_FLOW_CTRL_DEACTIVE_THRESHOLD;
+
+	hisi_femac_verify_flow_ctrl_args(priv);
+
+	hisi_femac_port_init(priv);
+
+	if (HAS_RXCSUM_CAP(priv->hw_cap))
+		hisi_femac_enable_rxcsum_drop(priv, true);
+
+	ret = hisi_femac_init_tx_and_rx_queues(priv);
+	if (ret)
+		goto out_disconnect_phy;
+
+	if (HAS_TSO_CAP(priv->hw_cap)) {
+		ret = hisi_femac_init_tx_descriptor_ring(priv);
+		if (ret)
+			goto out_disconnect_phy;
+	}
+
+	ndev->irq = platform_get_irq(pdev, 0);
+	if (ndev->irq <= 0) {
+		dev_err(dev, "No irq resource\n");
+		ret = -ENODEV;
+		goto out_destroy_descriptor;
+	}
+
+	ret = devm_request_irq(dev, ndev->irq, hisi_femac_interrupt,
+			       IRQF_SHARED, pdev->name, ndev);
+	if (ret) {
+		dev_err(dev, "devm_request_irq %d failed!\n", ndev->irq);
+		goto out_destroy_descriptor;
+	}
+
+	ret = register_netdev(ndev);
+	if (ret) {
+		dev_err(dev, "register_netdev failed!\n");
+		goto out_destroy_descriptor;
+	}
+
+	return ret;
+
+out_destroy_descriptor:
+	if (HAS_TSO_CAP(priv->hw_cap))
+		hisi_femac_destroy_tx_descriptor_ring(priv);
+out_disconnect_phy:
+	netif_napi_del(&priv->napi);
+	phy_disconnect(phy);
+out_disable_clk:
+	clk_disable_unprepare(priv->clk);
+out_free_netdev:
+	free_netdev(ndev);
+
+	return ret;
+}
+
+static int hisi_femac_drv_remove(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct hisi_femac_priv *priv = netdev_priv(ndev);
+
+	netif_napi_del(&priv->napi);
+	unregister_netdev(ndev);
+	if (HAS_TSO_CAP(priv->hw_cap))
+		hisi_femac_destroy_tx_descriptor_ring(priv);
+
+	phy_disconnect(ndev->phydev);
+	clk_disable_unprepare(priv->clk);
+	free_netdev(ndev);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM
+static int hisi_femac_drv_suspend(struct platform_device *pdev,
+				  pm_message_t state)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct hisi_femac_priv *priv = netdev_priv(ndev);
+
+	disable_irq(ndev->irq);
+	if (netif_running(ndev)) {
+		hisi_femac_net_close(ndev);
+		netif_device_detach(ndev);
+	}
+
+	clk_disable_unprepare(priv->clk);
+
+	return 0;
+}
+
+static int hisi_femac_drv_resume(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct hisi_femac_priv *priv = netdev_priv(ndev);
+
+	clk_prepare_enable(priv->clk);
+	if (priv->phy_rst)
+		hisi_femac_phy_reset(priv);
+
+	if (netif_running(ndev)) {
+		hisi_femac_port_init(priv);
+		hisi_femac_net_open(ndev);
+		netif_device_attach(ndev);
+	}
+	enable_irq(ndev->irq);
+
+	return 0;
+}
+#endif
+
+static const struct of_device_id hisi_femac_match[] = {
+	{.compatible = "hisilicon,hisi-femac-v1",},
+	{.compatible = "hisilicon,hisi-femac-v2",},
+	{.compatible = "hisilicon,hi3516cv300-femac",},
+	{.compatible = "hisilicon,hi3536dv100-femac",},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, hisi_femac_match);
+
+static struct platform_driver hisi_femac_driver = {
+	.driver = {
+		.name = "hisi-femac",
+		.of_match_table = hisi_femac_match,
+	},
+	.probe = hisi_femac_drv_probe,
+	.remove = hisi_femac_drv_remove,
+#ifdef CONFIG_PM
+	.suspend = hisi_femac_drv_suspend,
+	.resume = hisi_femac_drv_resume,
+#endif
+};
+
+module_platform_driver(hisi_femac_driver);
+
+MODULE_DESCRIPTION("Hisilicon Fast Ethernet MAC driver");
+MODULE_AUTHOR("Dongpo Li <lidongpo@hisilicon.com>");
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("platform:hisi-femac");
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/hisi-femac/Makefile linux-4.9.37_modified/drivers/net/ethernet/hisilicon/hisi-femac/Makefile
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/hisi-femac/Makefile	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/hisi-femac/Makefile	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,5 @@
+#
+# Makefile for the HISILICON Fast Ethernet network device drivers.
+#
+
+obj-$(CONFIG_HISI_FEMAC) += hisi_femac.o phy_fix.o
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/hisi-femac/phy_fix.c linux-4.9.37_modified/drivers/net/ethernet/hisilicon/hisi-femac/phy_fix.c
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/hisi-femac/phy_fix.c	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/hisi-femac/phy_fix.c	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,52 @@
+#include <linux/phy.h>
+#include "phy_fix.h"
+
+static const u32 phy_v272_fix_param[] = {
+#include "festa_v272_2723.h"
+};
+
+static int phy_expanded_write_bulk(struct phy_device *phy_dev,
+				   const u32 reg_and_val[], int count)
+{
+	int i, v, ret = 0;
+	u32 reg_addr;
+	u16 val;
+
+	v = phy_read(phy_dev, MII_BMCR);
+	v |= BMCR_PDOWN;
+	phy_write(phy_dev, MII_BMCR, v);
+
+	for (i = 0; i < (2 * count); i += 2) {
+		reg_addr = reg_and_val[i];
+		val = (u16)reg_and_val[i + 1];
+		phy_write(phy_dev, MII_EXPMA, reg_addr);
+		ret = phy_write(phy_dev, MII_EXPMD, val);
+	}
+
+	v = phy_read(phy_dev, MII_BMCR);
+	v &= (~BMCR_PDOWN);
+	phy_write(phy_dev, MII_BMCR, v);
+
+	return ret;
+}
+
+static int hisilicon_fephy_v272_fix(struct phy_device *phy_dev)
+{
+	int count;
+
+	count = ARRAY_SIZE(phy_v272_fix_param);
+	if (count % 2)
+		pr_warn("internal FEPHY fix register count is not right.\n");
+	count /= 2;
+
+	phy_expanded_write_bulk(phy_dev, phy_v272_fix_param, count);
+
+	return 0;
+}
+
+void phy_register_fixups(void)
+{
+	phy_register_fixup_for_uid(HISILICON_PHY_ID_FESTAV272,
+				   HISILICON_PHY_MASK,
+				   hisilicon_fephy_v272_fix);
+}
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/hisi-femac/phy_fix.h linux-4.9.37_modified/drivers/net/ethernet/hisilicon/hisi-femac/phy_fix.h
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/hisi-femac/phy_fix.h	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/hisi-femac/phy_fix.h	2018-07-04 07:50:03.091724541 +0300
@@ -0,0 +1,7 @@
+#define HISILICON_PHY_ID_FESTAV272	0x20669901
+#define HISILICON_PHY_MASK		0xfffffff0
+
+#define MII_EXPMD		0x1d
+#define MII_EXPMA		0x1e
+
+void phy_register_fixups(void);
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/Kconfig linux-4.9.37_modified/drivers/net/ethernet/hisilicon/Kconfig
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/Kconfig	2017-07-12 16:42:41.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/Kconfig	2018-07-04 07:50:03.091724541 +0300
@@ -76,4 +76,6 @@
 	  This selects the general ethernet driver for HNS.  This module make
 	  use of any HNS AE driver, such as HNS_DSAF
 
+source "drivers/net/ethernet/hisilicon/higmac/Kconfig"
+
 endif # NET_VENDOR_HISILICON
diff -ruN linux-4.9.37_original/drivers/net/ethernet/hisilicon/Makefile linux-4.9.37_modified/drivers/net/ethernet/hisilicon/Makefile
--- linux-4.9.37_original/drivers/net/ethernet/hisilicon/Makefile	2017-07-12 16:42:41.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/ethernet/hisilicon/Makefile	2018-07-04 07:50:03.091724541 +0300
@@ -6,4 +6,5 @@
 obj-$(CONFIG_HIP04_ETH) += hip04_eth.o
 obj-$(CONFIG_HNS_MDIO) += hns_mdio.o
 obj-$(CONFIG_HNS) += hns/
-obj-$(CONFIG_HISI_FEMAC) += hisi_femac.o
+obj-$(CONFIG_HISI_FEMAC) += hisi-femac/
+obj-$(CONFIG_HIETH_GMAC) += higmac/
diff -ruN linux-4.9.37_original/drivers/net/phy/Kconfig linux-4.9.37_modified/drivers/net/phy/Kconfig
--- linux-4.9.37_original/drivers/net/phy/Kconfig	2017-07-12 16:42:41.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/phy/Kconfig	2018-07-04 07:50:03.095724580 +0300
@@ -105,6 +105,13 @@
 	  This module provides a driver for the MDIO busses found in the
 	  Hisilicon SoC that have an Fast Ethernet MAC.
 
+config MDIO_HISI_GEMAC
+	tristate "Hisilicon GEMAC MDIO bus controller"
+	depends on HAS_IOMEM && OF_MDIO
+	help
+	  This module provides a driver for the MDIO busses found in the
+	  Hisilicon SoC that have an Gigabit Ethernet MAC.
+
 config MDIO_MOXART
         tristate "MOXA ART MDIO interface support"
         depends on ARCH_MOXART
diff -ruN linux-4.9.37_original/drivers/net/phy/Makefile linux-4.9.37_modified/drivers/net/phy/Makefile
--- linux-4.9.37_original/drivers/net/phy/Makefile	2017-07-12 16:42:41.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/phy/Makefile	2018-07-04 07:50:03.095724580 +0300
@@ -15,6 +15,7 @@
 obj-$(CONFIG_MDIO_CAVIUM)	+= mdio-cavium.o
 obj-$(CONFIG_MDIO_GPIO)		+= mdio-gpio.o
 obj-$(CONFIG_MDIO_HISI_FEMAC)	+= mdio-hisi-femac.o
+obj-$(CONFIG_MDIO_HISI_GEMAC)	+= mdio-hisi-gemac.o
 obj-$(CONFIG_MDIO_MOXART)	+= mdio-moxart.o
 obj-$(CONFIG_MDIO_OCTEON)	+= mdio-octeon.o
 obj-$(CONFIG_MDIO_SUN4I)	+= mdio-sun4i.o
diff -ruN linux-4.9.37_original/drivers/net/phy/mdio-hisi-femac.c linux-4.9.37_modified/drivers/net/phy/mdio-hisi-femac.c
--- linux-4.9.37_original/drivers/net/phy/mdio-hisi-femac.c	2017-07-12 16:42:41.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/phy/mdio-hisi-femac.c	2018-07-04 07:50:03.095724580 +0300
@@ -24,6 +24,7 @@
 #include <linux/of_address.h>
 #include <linux/of_mdio.h>
 #include <linux/platform_device.h>
+#include <linux/reset.h>
 
 #define MDIO_RWCTRL		0x00
 #define MDIO_RO_DATA		0x04
@@ -32,16 +33,55 @@
 #define BIT_PHY_ADDR_OFFSET	8
 #define BIT_WR_DATA_OFFSET	16
 
+#define BIT_MASK_FEPHY_ADDR	GENMASK(4, 0)
+#define BIT_FEPHY_SEL		BIT(5)
+
+#define BIT_OFFSET_LD_SET	0
+#define BIT_OFFSET_LDO_SET	5
+#define BIT_OFFSET_R_TUNING	8
+
+#define MII_EXPMD		0x1d
+#define MII_EXPMA		0x1e
+
+#define REG_LD_AM		0x3050
+#define BIT_MASK_LD_SET		GENMASK(4, 0)
+#define REG_LDO_AM		0x3051
+#define BIT_MASK_LDO_SET	GENMASK(2, 0)
+#define REG_R_TUNING		0x3052
+#define BIT_MASK_R_TUNING	GENMASK(5, 0)
+#define REG_WR_DONE		0x3053
+#define BIT_CFG_DONE		BIT(0)
+#define BIT_CFG_ACK		BIT(1)
+#define REG_DEF_ATE		0x3057
+#define BIT_AUTOTRIM_DONE	BIT(0)
+
+#define PHY_RESET_DELAYS_PROPERTY	"hisilicon,phy-reset-delays-us"
+
+enum phy_reset_delays {
+	PRE_DELAY,
+	PULSE,
+	POST_DELAY,
+	DELAYS_NUM,
+};
+
 struct hisi_femac_mdio_data {
 	struct clk *clk;
+	struct clk *fephy_clk;
+	struct reset_control *phy_rst;
+	struct reset_control *fephy_rst;
+	u32 phy_reset_delays[DELAYS_NUM];
 	void __iomem *membase;
+	void __iomem *fephy_iobase;
+	void __iomem *fephy_trim_iobase;
+	struct mii_bus *bus;
+	u32 phy_addr;
 };
 
 static int hisi_femac_mdio_wait_ready(struct hisi_femac_mdio_data *data)
 {
 	u32 val;
 
-	return readl_poll_timeout(data->membase + MDIO_RWCTRL,
+	return readl_poll_timeout_atomic(data->membase + MDIO_RWCTRL,
 				  val, val & MDIO_RW_FINISH, 20, 10000);
 }
 
@@ -54,8 +94,8 @@
 	if (ret)
 		return ret;
 
-	writel((mii_id << BIT_PHY_ADDR_OFFSET) | regnum,
-	       data->membase + MDIO_RWCTRL);
+	writel((mii_id << BIT_PHY_ADDR_OFFSET) | ((u32)regnum),
+		  data->membase + MDIO_RWCTRL);
 
 	ret = hisi_femac_mdio_wait_ready(data);
 	if (ret)
@@ -75,12 +115,215 @@
 		return ret;
 
 	writel(MDIO_WRITE | (value << BIT_WR_DATA_OFFSET) |
-	       (mii_id << BIT_PHY_ADDR_OFFSET) | regnum,
+	       (mii_id << BIT_PHY_ADDR_OFFSET) | ((u32)regnum),
 	       data->membase + MDIO_RWCTRL);
 
 	return hisi_femac_mdio_wait_ready(data);
 }
 
+static void hisi_femac_sleep_us(u32 time_us)
+{
+	u32 time_ms;
+
+	if (!time_us)
+		return;
+
+	time_ms = DIV_ROUND_UP(time_us, 1000);
+	if (time_ms < 20)
+		usleep_range(time_us, time_us + 500);
+	else
+		msleep(time_ms);
+}
+
+static void hisi_femac_phy_reset(struct hisi_femac_mdio_data *data)
+{
+	/* To make sure PHY hardware reset success,
+	 * we must keep PHY in deassert state first and
+	 * then complete the hardware reset operation
+	 */
+	reset_control_deassert(data->phy_rst);
+	hisi_femac_sleep_us(data->phy_reset_delays[PRE_DELAY]);
+
+	reset_control_assert(data->phy_rst);
+	/* delay some time to ensure reset ok,
+	 * this depends on PHY hardware feature
+	 */
+	hisi_femac_sleep_us(data->phy_reset_delays[PULSE]);
+	reset_control_deassert(data->phy_rst);
+	/* delay some time to ensure later MDIO access */
+	hisi_femac_sleep_us(data->phy_reset_delays[POST_DELAY]);
+}
+
+static void hisi_femac_get_phy_addr(struct hisi_femac_mdio_data *data,
+				    struct device_node *np)
+{
+	struct device_node *child = NULL;
+	int addr;
+
+	child = of_get_next_available_child(np, NULL);
+	if (!child) {
+		pr_err("%s: No valid PHY device node!\n", __func__);
+		return;
+	}
+
+	addr = of_mdio_parse_addr(&data->bus->dev, child);
+	if (addr < 0) {
+		pr_err("%s: get PHY address failed!\n", __func__);
+		return;
+	}
+
+	data->phy_addr = addr;
+}
+
+static inline bool hisi_femac_use_fephy(struct hisi_femac_mdio_data *data)
+{
+	/*return false;*/
+	return (data->fephy_iobase ?
+			!(readl(data->fephy_iobase) & BIT_FEPHY_SEL) : false);
+}
+
+static void hisi_femac_fephy_reset(struct hisi_femac_mdio_data *data)
+{
+	u32 val;
+
+	/* disable MDCK clock to make sure FEPHY reset success */
+	clk_disable_unprepare(data->clk);
+
+	val = readl(data->fephy_iobase);
+	val &= ~BIT_MASK_FEPHY_ADDR;
+	val |= data->phy_addr;
+	writel(val, data->fephy_iobase);
+
+	clk_prepare_enable(data->fephy_clk);
+	udelay(10);
+
+	reset_control_assert(data->fephy_rst);
+	udelay(10);
+	reset_control_deassert(data->fephy_rst);
+	/* delay at least 15ms for MDIO operation */
+	msleep(20);
+
+	clk_prepare_enable(data->clk);
+	/* delay 5ms after enable MDCK to make sure FEPHY trim safe */
+	mdelay(5);
+}
+
+static inline int fephy_expanded_read(struct mii_bus *bus, int phy_addr,
+				      u32 reg_addr)
+{
+	int ret;
+
+	hisi_femac_mdio_write(bus, phy_addr, MII_EXPMA, reg_addr);
+	ret = hisi_femac_mdio_read(bus, phy_addr, MII_EXPMD);
+
+	return ret;
+}
+
+static inline int fephy_expanded_write(struct mii_bus *bus, int phy_addr,
+				       u32 reg_addr, u16 val)
+{
+	int ret;
+
+	hisi_femac_mdio_write(bus, phy_addr, MII_EXPMA, reg_addr);
+	ret = hisi_femac_mdio_write(bus, phy_addr, MII_EXPMD, val);
+
+	return ret;
+}
+
+void hisi_femac_fephy_use_default_trim(struct hisi_femac_mdio_data *data)
+{
+	unsigned short val;
+	int timeout = 3;
+
+	pr_info("No OTP data, festa PHY use default ATE parameters!\n");
+
+	do {
+		msleep(250);
+		val = fephy_expanded_read(data->bus, data->phy_addr,
+					  REG_DEF_ATE);
+		val &= BIT_AUTOTRIM_DONE;
+	} while (!val && --timeout);
+
+	if (!timeout)
+		pr_err("festa PHY wait autotrim done timeout!\n");
+
+	mdelay(5);
+}
+
+static void hisi_femac_fephy_trim(struct hisi_femac_mdio_data *data)
+{
+	struct mii_bus *bus = data->bus;
+	u32 phy_addr = data->phy_addr;
+	int timeout = 3000;
+	u32 val;
+	u8 ld_set;
+	u8 ldo_set;
+	u8 r_tuning;
+
+	val = readl(data->fephy_trim_iobase);
+	ld_set = (val >> BIT_OFFSET_LD_SET) & BIT_MASK_LD_SET;
+	ldo_set = (val >> BIT_OFFSET_LDO_SET) & BIT_MASK_LDO_SET;
+	r_tuning = (val >> BIT_OFFSET_R_TUNING) & BIT_MASK_R_TUNING;
+
+	if (!ld_set && !ldo_set && !r_tuning) {
+		hisi_femac_fephy_use_default_trim(data);
+		return;
+	}
+
+	val = fephy_expanded_read(bus, phy_addr, REG_LD_AM);
+	val = (val & ~BIT_MASK_LD_SET) | (ld_set & BIT_MASK_LD_SET);
+	fephy_expanded_write(bus, phy_addr, REG_LD_AM, val);
+
+	val = fephy_expanded_read(bus, phy_addr, REG_LDO_AM);
+	val = (val & ~BIT_MASK_LDO_SET) | (ldo_set & BIT_MASK_LDO_SET);
+	fephy_expanded_write(bus, phy_addr, REG_LDO_AM, val);
+
+	val = fephy_expanded_read(bus, phy_addr, REG_R_TUNING);
+	val = (val & ~BIT_MASK_R_TUNING) | (r_tuning & BIT_MASK_R_TUNING);
+	fephy_expanded_write(bus, phy_addr, REG_R_TUNING, val);
+
+	val = fephy_expanded_read(bus, phy_addr, REG_WR_DONE);
+	if (val & BIT_CFG_ACK)
+		pr_err("festa PHY 0x3053 bit CFG_ACK value: 1\n");
+	val = val | BIT_CFG_DONE;
+	fephy_expanded_write(bus, phy_addr, REG_WR_DONE, val);
+
+	do {
+		usleep_range(100, 150);
+		val = fephy_expanded_read(bus, phy_addr, REG_WR_DONE);
+		val &= BIT_CFG_ACK;
+	} while (!val && --timeout);
+	if (!timeout)
+		pr_err("festa PHY 0x3053 wait bit CFG_ACK timeout!\n");
+
+	mdelay(5);
+
+	pr_info("FEPHY:addr=%d, la_am=0x%x, ldo_am=0x%x, r_tuning=0x%x\n",
+		phy_addr,
+		fephy_expanded_read(bus, phy_addr, REG_LD_AM),
+		fephy_expanded_read(bus, phy_addr, REG_LDO_AM),
+		fephy_expanded_read(bus, phy_addr, REG_R_TUNING));
+}
+
+static void hisi_femac_fephy_reset_and_trim(struct hisi_femac_mdio_data *data)
+{
+	hisi_femac_fephy_reset(data);
+	hisi_femac_fephy_trim(data);
+}
+
+static void hisi_femac_fephy_set_phy_addr(struct hisi_femac_mdio_data *data)
+{
+	u32 val;
+
+	if (!data->fephy_iobase)
+		return;
+
+	val = readl(data->fephy_iobase);
+	val &= ~BIT_MASK_FEPHY_ADDR;
+	val |= (data->phy_addr + 1);
+	writel(val, data->fephy_iobase);
+}
+
 static int hisi_femac_mdio_probe(struct platform_device *pdev)
 {
 	struct device_node *np = pdev->dev.of_node;
@@ -100,6 +343,7 @@
 	bus->parent = &pdev->dev;
 
 	data = bus->priv;
+	data->bus = bus;
 	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 	data->membase = devm_ioremap_resource(&pdev->dev, res);
 	if (IS_ERR(data->membase)) {
@@ -107,16 +351,66 @@
 		goto err_out_free_mdiobus;
 	}
 
-	data->clk = devm_clk_get(&pdev->dev, NULL);
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+	if (res) {
+		data->fephy_iobase = devm_ioremap_resource(&pdev->dev, res);
+		if (IS_ERR(data->fephy_iobase)) {
+			ret = PTR_ERR(data->fephy_iobase);
+			goto err_out_free_mdiobus;
+		}
+	} else {
+		data->fephy_iobase = NULL;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 2);
+	if (res) {
+		data->fephy_trim_iobase = devm_ioremap_resource(&pdev->dev,
+								res);
+		if (IS_ERR(data->fephy_trim_iobase)) {
+			ret = PTR_ERR(data->fephy_trim_iobase);
+			goto err_out_free_mdiobus;
+		}
+	} else {
+		data->fephy_trim_iobase = NULL;
+	}
+
+	data->clk = devm_clk_get(&pdev->dev, "mdio");
 	if (IS_ERR(data->clk)) {
 		ret = PTR_ERR(data->clk);
 		goto err_out_free_mdiobus;
 	}
 
+	data->fephy_clk = devm_clk_get(&pdev->dev, "phy");
+	if (IS_ERR(data->fephy_clk))
+		data->fephy_clk = NULL;
+
 	ret = clk_prepare_enable(data->clk);
 	if (ret)
 		goto err_out_free_mdiobus;
 
+	data->phy_rst = devm_reset_control_get(&pdev->dev, "external-phy");
+	if (IS_ERR(data->phy_rst)) {
+		data->phy_rst = NULL;
+	} else {
+		ret = of_property_read_u32_array(np,
+						 PHY_RESET_DELAYS_PROPERTY,
+						 data->phy_reset_delays,
+						 DELAYS_NUM);
+		if (ret)
+			goto err_out_disable_clk;
+		hisi_femac_phy_reset(data);
+	}
+
+	data->fephy_rst = devm_reset_control_get(&pdev->dev, "internal-phy");
+	if (IS_ERR(data->fephy_rst))
+		data->fephy_rst = NULL;
+
+	hisi_femac_get_phy_addr(data, np);
+	if (hisi_femac_use_fephy(data))
+		hisi_femac_fephy_reset_and_trim(data);
+	else
+		hisi_femac_fephy_set_phy_addr(data);
+
 	ret = of_mdiobus_register(bus, np);
 	if (ret)
 		goto err_out_disable_clk;
@@ -126,6 +420,7 @@
 	return 0;
 
 err_out_disable_clk:
+	clk_disable_unprepare(data->fephy_clk);
 	clk_disable_unprepare(data->clk);
 err_out_free_mdiobus:
 	mdiobus_free(bus);
diff -ruN linux-4.9.37_original/drivers/net/phy/mdio-hisi-gemac.c linux-4.9.37_modified/drivers/net/phy/mdio-hisi-gemac.c
--- linux-4.9.37_original/drivers/net/phy/mdio-hisi-gemac.c	1970-01-01 03:00:00.000000000 +0300
+++ linux-4.9.37_modified/drivers/net/phy/mdio-hisi-gemac.c	2018-07-04 07:50:03.095724580 +0300
@@ -0,0 +1,223 @@
+/*
+ * Hisilicon Gigabit Ethernet MDIO Bus Driver
+ *
+ * Copyright (c) 2016 HiSilicon Technologies Co., Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program. If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/clk.h>
+#include <linux/iopoll.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of_address.h>
+#include <linux/of_mdio.h>
+#include <linux/platform_device.h>
+#include <linux/reset.h>
+
+#if defined(CONFIG_ARCH_HI3519) || defined(CONFIG_ARCH_HI3519V101) || \
+	defined(CONFIG_ARCH_HI3516AV200)
+#ifdef readl
+#undef readl
+#undef writel
+#define readl		hi_readl
+#define writel		hi_writel
+#endif
+#endif
+
+#define MDIO_SINGLE_CMD		0x00
+#define MDIO_SINGLE_DATA	0x04
+#define MDIO_RDATA_STATUS	0x10
+#define BIT_PHY_ADDR_OFFSET	8
+#define MDIO_WRITE		BIT(16)
+#define MDIO_READ		BIT(17)
+#define MDIO_START		BIT(20)
+#define MDIO_START_READ		(MDIO_START | MDIO_READ)
+#define MDIO_START_WRITE	(MDIO_START | MDIO_WRITE)
+
+struct hisi_gemac_mdio_data {
+	struct clk *clk;
+	struct reset_control *phy_rst;
+	void __iomem *membase;
+};
+
+static int hisi_gemac_mdio_wait_ready(struct hisi_gemac_mdio_data *data)
+{
+	u32 val;
+
+	return readl_poll_timeout(data->membase + MDIO_SINGLE_CMD,
+				  val, !(val & MDIO_START), 20, 10000);
+}
+
+static int hisi_gemac_mdio_read(struct mii_bus *bus, int mii_id, int regnum)
+{
+	struct hisi_gemac_mdio_data *data = bus->priv;
+	int ret;
+
+	ret = hisi_gemac_mdio_wait_ready(data);
+	if (ret)
+		return ret;
+
+	writel(MDIO_START_READ | ((u32)mii_id << BIT_PHY_ADDR_OFFSET) |
+		((u32)regnum),
+	       data->membase + MDIO_SINGLE_CMD);
+
+	ret = hisi_gemac_mdio_wait_ready(data);
+	if (ret)
+		return ret;
+
+	/* if read data is invalid, we just return 0 instead of -EAGAIN.
+	 * This can make MDIO more robust when reading PHY status.
+	 */
+	if (readl(data->membase + MDIO_RDATA_STATUS))
+		return 0;
+
+	return readl(data->membase + MDIO_SINGLE_DATA) >> 16;
+}
+
+static int hisi_gemac_mdio_write(struct mii_bus *bus, int mii_id, int regnum,
+				 u16 value)
+{
+	struct hisi_gemac_mdio_data *data = bus->priv;
+	int ret;
+
+	ret = hisi_gemac_mdio_wait_ready(data);
+	if (ret)
+		return ret;
+
+	writel(value, data->membase + MDIO_SINGLE_DATA);
+	writel(MDIO_START_WRITE | ((u32)mii_id << BIT_PHY_ADDR_OFFSET) |
+		((u32)regnum),
+	       data->membase + MDIO_SINGLE_CMD);
+
+	return hisi_gemac_mdio_wait_ready(data);
+}
+
+static void hisi_gemac_external_phy_reset(struct hisi_gemac_mdio_data *data)
+{
+	if (data->phy_rst) {
+		/* write 0 to cancel reset */
+		reset_control_deassert(data->phy_rst);
+		msleep(50);
+
+		/* HIFONE or 98cv200 use CRG register to reset phy */
+		/* RST_BIT, write 0 to reset phy, write 1 to cancel reset */
+		reset_control_assert(data->phy_rst);
+
+		/* delay some time to ensure reset ok,
+		 * this depends on PHY hardware feature
+		 */
+		msleep(50);
+
+		/* write 0 to cancel reset */
+		reset_control_deassert(data->phy_rst);
+		/* delay some time to ensure later MDIO access */
+		msleep(50);
+	} else {
+	}
+}
+
+static int hisi_gemac_mdio_probe(struct platform_device *pdev)
+{
+	struct device_node *np = pdev->dev.of_node;
+	struct mii_bus *bus;
+	struct hisi_gemac_mdio_data *data;
+	struct resource *res;
+	int ret;
+
+	bus = mdiobus_alloc_size(sizeof(*data));
+	if (!bus)
+		return -ENOMEM;
+
+	bus->name = "hisi_gemac_mii_bus";
+	bus->read = &hisi_gemac_mdio_read;
+	bus->write = &hisi_gemac_mdio_write;
+	snprintf(bus->id, MII_BUS_ID_SIZE, "%s", pdev->name);
+	bus->parent = &pdev->dev;
+
+	data = bus->priv;
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		ret = -ENXIO;
+		goto err_out_free_mdiobus;
+	}
+	data->membase = devm_ioremap(&pdev->dev, res->start,
+				     resource_size(res));
+	if (!data->membase) {
+		ret = -ENOMEM;
+		goto err_out_free_mdiobus;
+	}
+
+	data->clk = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(data->clk)) {
+		ret = PTR_ERR(data->clk);
+		goto err_out_free_mdiobus;
+	}
+
+	ret = clk_prepare_enable(data->clk);
+	if (ret)
+		goto err_out_free_mdiobus;
+
+	data->phy_rst = devm_reset_control_get(&pdev->dev, "phy_reset");
+	if (IS_ERR(data->phy_rst))
+		data->phy_rst = NULL;
+	hisi_gemac_external_phy_reset(data);
+
+	ret = of_mdiobus_register(bus, np);
+	if (ret)
+		goto err_out_disable_clk;
+
+	platform_set_drvdata(pdev, bus);
+
+	return 0;
+
+err_out_disable_clk:
+	clk_disable_unprepare(data->clk);
+err_out_free_mdiobus:
+	mdiobus_free(bus);
+	return ret;
+}
+
+static int hisi_gemac_mdio_remove(struct platform_device *pdev)
+{
+	struct mii_bus *bus = platform_get_drvdata(pdev);
+	struct hisi_gemac_mdio_data *data = bus->priv;
+
+	mdiobus_unregister(bus);
+	clk_disable_unprepare(data->clk);
+	mdiobus_free(bus);
+
+	return 0;
+}
+
+static const struct of_device_id hisi_gemac_mdio_dt_ids[] = {
+	{ .compatible = "hisilicon,hisi-gemac-mdio" },
+	{ }
+};
+MODULE_DEVICE_TABLE(of, hisi_gemac_mdio_dt_ids);
+
+static struct platform_driver hisi_gemac_mdio_driver = {
+	.probe = hisi_gemac_mdio_probe,
+	.remove = hisi_gemac_mdio_remove,
+	.driver = {
+		.name = "hisi-gemac-mdio",
+		.of_match_table = hisi_gemac_mdio_dt_ids,
+	},
+};
+
+module_platform_driver(hisi_gemac_mdio_driver);
+
+MODULE_DESCRIPTION("Hisilicon Gigabit Ethernet MAC MDIO interface driver");
+MODULE_AUTHOR("Dongpo Li <lidongpo@hisilicon.com>");
+MODULE_LICENSE("GPL v2");
